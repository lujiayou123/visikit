Logging to /root/code/data/maml-kate-def

 ---------------- Iteration 0 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 0              |
| ItrTime                 | 79.5           |
| LossAfter               | -0.029458333   |
| LossBefore              | 2.8590702e-10  |
| MeanKL                  | 0.0073749833   |
| MeanKLBefore            | -1.5013836e-09 |
| Step_0-AverageDiscou... | -1.08          |
| Step_0-AveragePolicyStd | 1.0            |
| Step_0-AverageReturn    | -1.17          |
| Step_0-EnvExecTime      | 19.2           |
| Step_0-MaxReturn        | 43.2           |
| Step_0-MinReturn        | -16.1          |
| Step_0-NumTrajs         | 20921          |
| Step_0-PolicyExecTime   | 1.59           |
| Step_0-StdReturn        | 3.66           |
| Step_1-AverageDiscou... | 0.0349         |
| Step_1-AveragePolicyStd | 0.99873775     |
| Step_1-AverageReturn    | 0.00453        |
| Step_1-EnvExecTime      | 19.2           |
| Step_1-MaxReturn        | 50             |
| Step_1-MinReturn        | -15.5          |
| Step_1-NumTrajs         | 18337          |
| Step_1-PolicyExecTime   | 3.55           |
| Step_1-StdReturn        | 4.1            |
| Time                    | 79.5           |
| Time-InnerStep          | 1.17           |
| Time-MAMLSteps          | 23.2           |
| Time-OuterStep          | 23.2           |
| Time-SampleProc         | 7.48           |
| Time-Sampling           | 47             |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.029458333    |
| n_timesteps             | 320000         |
--------------------------------------------

 ---------------- Iteration 1 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 1              |
| ItrTime                 | 67.7           |
| LossAfter               | -0.02914608    |
| LossBefore              | -8.0428675e-10 |
| MeanKL                  | 0.00727198     |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | -1.55          |
| Step_0-AveragePolicyStd | 0.99014217     |
| Step_0-AverageReturn    | -1.67          |
| Step_0-EnvExecTime      | 19.1           |
| Step_0-MaxReturn        | 20.2           |
| Step_0-MinReturn        | -18.3          |
| Step_0-NumTrajs         | 21097          |
| Step_0-PolicyExecTime   | 1.01           |
| Step_0-StdReturn        | 4.05           |
| Step_1-AverageDiscou... | -0.55          |
| Step_1-AveragePolicyStd | 0.9888608      |
| Step_1-AverageReturn    | -0.609         |
| Step_1-EnvExecTime      | 18.9           |
| Step_1-MaxReturn        | 32.6           |
| Step_1-MinReturn        | -16.5          |
| Step_1-NumTrajs         | 18878          |
| Step_1-PolicyExecTime   | 3.11           |
| Step_1-StdReturn        | 4.4            |
| Time                    | 147            |
| Time-InnerStep          | 0.152          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 7.87           |
| Time-Sampling           | 45.7           |
| Time-TotalInner         | 53.8           |
| dLoss                   | 0.02914608     |
| n_timesteps             | 640000         |
--------------------------------------------

 ---------------- Iteration 2 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 2              |
| ItrTime                 | 66.5           |
| LossAfter               | -0.026888993   |
| LossBefore              | -1.08087e-09   |
| MeanKL                  | 0.0069356537   |
| MeanKLBefore            | -3.9204195e-13 |
| Step_0-AverageDiscou... | -1.3           |
| Step_0-AveragePolicyStd | 0.9822293      |
| Step_0-AverageReturn    | -1.41          |
| Step_0-EnvExecTime      | 18.9           |
| Step_0-MaxReturn        | 30.7           |
| Step_0-MinReturn        | -14.1          |
| Step_0-NumTrajs         | 19785          |
| Step_0-PolicyExecTime   | 0.995          |
| Step_0-StdReturn        | 3.83           |
| Step_1-AverageDiscou... | -0.0506        |
| Step_1-AveragePolicyStd | 0.9808485      |
| Step_1-AverageReturn    | -0.0979        |
| Step_1-EnvExecTime      | 18.4           |
| Step_1-MaxReturn        | 41.4           |
| Step_1-MinReturn        | -17.3          |
| Step_1-NumTrajs         | 17023          |
| Step_1-PolicyExecTime   | 3.36           |
| Step_1-StdReturn        | 4.37           |
| Time                    | 214            |
| Time-InnerStep          | 0.153          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 7.23           |
| Time-Sampling           | 45.2           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.02688899     |
| n_timesteps             | 960000         |
--------------------------------------------

 ---------------- Iteration 3 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 3             |
| ItrTime                 | 65.2          |
| LossAfter               | -0.024643077  |
| LossBefore              | 1.8515198e-09 |
| MeanKL                  | 0.00737796    |
| MeanKLBefore            | 1.6759596e-09 |
| Step_0-AverageDiscou... | -0.0192       |
| Step_0-AveragePolicyStd | 0.9738699     |
| Step_0-AverageReturn    | -0.072        |
| Step_0-EnvExecTime      | 18.7          |
| Step_0-MaxReturn        | 36.3          |
| Step_0-MinReturn        | -18.2         |
| Step_0-NumTrajs         | 18582         |
| Step_0-PolicyExecTime   | 1.03          |
| Step_0-StdReturn        | 4.14          |
| Step_1-AverageDiscou... | 1.38          |
| Step_1-AveragePolicyStd | 0.9724825     |
| Step_1-AverageReturn    | 1.43          |
| Step_1-EnvExecTime      | 18.3          |
| Step_1-MaxReturn        | 57.1          |
| Step_1-MinReturn        | -18.2         |
| Step_1-NumTrajs         | 15250         |
| Step_1-PolicyExecTime   | 2.97          |
| Step_1-StdReturn        | 5.25          |
| Time                    | 279           |
| Time-InnerStep          | 0.152         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 6.74          |
| Time-Sampling           | 44.4          |
| Time-TotalInner         | 51.4          |
| dLoss                   | 0.024643078   |
| n_timesteps             | 1280000       |
-------------------------------------------

 ---------------- Iteration 4 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 4             |
| ItrTime                 | 64.3          |
| LossAfter               | -0.023602631  |
| LossBefore              | 2.2621225e-09 |
| MeanKL                  | 0.0073573217  |
| MeanKLBefore            | 1.8156637e-09 |
| Step_0-AverageDiscou... | 0.284         |
| Step_0-AveragePolicyStd | 0.9642685     |
| Step_0-AverageReturn    | 0.248         |
| Step_0-EnvExecTime      | 18.6          |
| Step_0-MaxReturn        | 50.8          |
| Step_0-MinReturn        | -15.2         |
| Step_0-NumTrajs         | 17813         |
| Step_0-PolicyExecTime   | 0.986         |
| Step_0-StdReturn        | 4.59          |
| Step_1-AverageDiscou... | 1.91          |
| Step_1-AveragePolicyStd | 0.96245456    |
| Step_1-AverageReturn    | 2             |
| Step_1-EnvExecTime      | 18.1          |
| Step_1-MaxReturn        | 63.7          |
| Step_1-MinReturn        | -14.4         |
| Step_1-NumTrajs         | 14266         |
| Step_1-PolicyExecTime   | 2.96          |
| Step_1-StdReturn        | 5.87          |
| Time                    | 343           |
| Time-InnerStep          | 0.153         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 6.3           |
| Time-Sampling           | 44            |
| Time-TotalInner         | 50.5          |
| dLoss                   | 0.023602633   |
| n_timesteps             | 1600000       |
-------------------------------------------

 ---------------- Iteration 5 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 5              |
| ItrTime                 | 63.3           |
| LossAfter               | -0.020799497   |
| LossBefore              | -5.4830174e-10 |
| MeanKL                  | 0.0072080395   |
| MeanKLBefore            | 6.145848e-09   |
| Step_0-AverageDiscou... | 1.99           |
| Step_0-AveragePolicyStd | 0.95482606     |
| Step_0-AverageReturn    | 2.05           |
| Step_0-EnvExecTime      | 18.4           |
| Step_0-MaxReturn        | 72.5           |
| Step_0-MinReturn        | -18            |
| Step_0-NumTrajs         | 15806          |
| Step_0-PolicyExecTime   | 1.41           |
| Step_0-StdReturn        | 4.81           |
| Step_1-AverageDiscou... | 4.22           |
| Step_1-AveragePolicyStd | 0.95331544     |
| Step_1-AverageReturn    | 4.52           |
| Step_1-EnvExecTime      | 17.8           |
| Step_1-MaxReturn        | 82.6           |
| Step_1-MinReturn        | -15.8          |
| Step_1-NumTrajs         | 11953          |
| Step_1-PolicyExecTime   | 2.97           |
| Step_1-StdReturn        | 6.85           |
| Time                    | 406            |
| Time-InnerStep          | 0.152          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 5.47           |
| Time-Sampling           | 43.8           |
| Time-TotalInner         | 49.5           |
| dLoss                   | 0.020799497    |
| n_timesteps             | 1920000        |
--------------------------------------------

 ---------------- Iteration 6 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 6              |
| ItrTime                 | 62.7           |
| LossAfter               | -0.019044928   |
| LossBefore              | -3.4747716e-10 |
| MeanKL                  | 0.0074014515   |
| MeanKLBefore            | -1.6755048e-09 |
| Step_0-AverageDiscou... | 1.34           |
| Step_0-AveragePolicyStd | 0.9451719      |
| Step_0-AverageReturn    | 1.36           |
| Step_0-EnvExecTime      | 18.2           |
| Step_0-MaxReturn        | 55.6           |
| Step_0-MinReturn        | -23.5          |
| Step_0-NumTrajs         | 15426          |
| Step_0-PolicyExecTime   | 1.01           |
| Step_0-StdReturn        | 5.31           |
| Step_1-AverageDiscou... | 3.36           |
| Step_1-AveragePolicyStd | 0.9436383      |
| Step_1-AverageReturn    | 3.6            |
| Step_1-EnvExecTime      | 17.8           |
| Step_1-MaxReturn        | 113            |
| Step_1-MinReturn        | -27.9          |
| Step_1-NumTrajs         | 11973          |
| Step_1-PolicyExecTime   | 2.99           |
| Step_1-StdReturn        | 7.61           |
| Time                    | 469            |
| Time-InnerStep          | 0.152          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 5.45           |
| Time-Sampling           | 43.3           |
| Time-TotalInner         | 49             |
| dLoss                   | 0.019044928    |
| n_timesteps             | 2240000        |
--------------------------------------------

 ---------------- Iteration 7 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 7             |
| ItrTime                 | 61.7          |
| LossAfter               | -0.017772203  |
| LossBefore              | 1.9167818e-09 |
| MeanKL                  | 0.0080961995  |
| MeanKLBefore            | -4.283026e-09 |
| Step_0-AverageDiscou... | 1.92          |
| Step_0-AveragePolicyStd | 0.9360401     |
| Step_0-AverageReturn    | 2.02          |
| Step_0-EnvExecTime      | 17.9          |
| Step_0-MaxReturn        | 72.2          |
| Step_0-MinReturn        | -25.1         |
| Step_0-NumTrajs         | 13725         |
| Step_0-PolicyExecTime   | 1.02          |
| Step_0-StdReturn        | 6.45          |
| Step_1-AverageDiscou... | 4.36          |
| Step_1-AveragePolicyStd | 0.93472654    |
| Step_1-AverageReturn    | 4.79          |
| Step_1-EnvExecTime      | 17.6          |
| Step_1-MaxReturn        | 88            |
| Step_1-MinReturn        | -28.1         |
| Step_1-NumTrajs         | 10646         |
| Step_1-PolicyExecTime   | 3.04          |
| Step_1-StdReturn        | 9.41          |
| Time                    | 531           |
| Time-InnerStep          | 0.156         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 4.89          |
| Time-Sampling           | 42.9          |
| Time-TotalInner         | 48            |
| dLoss                   | 0.017772205   |
| n_timesteps             | 2560000       |
-------------------------------------------

 ---------------- Iteration 8 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 8              |
| ItrTime                 | 61.2           |
| LossAfter               | -0.016744139   |
| LossBefore              | -2.8614804e-09 |
| MeanKL                  | 0.007890506    |
| MeanKLBefore            | 4.561684e-13   |
| Step_0-AverageDiscou... | 3.24           |
| Step_0-AveragePolicyStd | 0.92052984     |
| Step_0-AverageReturn    | 3.47           |
| Step_0-EnvExecTime      | 18.2           |
| Step_0-MaxReturn        | 83.1           |
| Step_0-MinReturn        | -27.2          |
| Step_0-NumTrajs         | 12220          |
| Step_0-PolicyExecTime   | 0.996          |
| Step_0-StdReturn        | 7.1            |
| Step_1-AverageDiscou... | 5.77           |
| Step_1-AveragePolicyStd | 0.9189956      |
| Step_1-AverageReturn    | 6.4            |
| Step_1-EnvExecTime      | 17.4           |
| Step_1-MaxReturn        | 149            |
| Step_1-MinReturn        | -39.8          |
| Step_1-NumTrajs         | 9598           |
| Step_1-PolicyExecTime   | 3.05           |
| Step_1-StdReturn        | 10.3           |
| Time                    | 592            |
| Time-InnerStep          | 0.154          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 4.42           |
| Time-Sampling           | 42.9           |
| Time-TotalInner         | 47.5           |
| dLoss                   | 0.016744135    |
| n_timesteps             | 2880000        |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 9             |
| ItrTime                 | 60.3          |
| LossAfter               | -0.015500884  |
| LossBefore              | 3.8045203e-09 |
| MeanKL                  | 0.00828741    |
| MeanKLBefore            | 1.3026079e-09 |
| Step_0-AverageDiscou... | 3.4           |
| Step_0-AveragePolicyStd | 0.90778524    |
| Step_0-AverageReturn    | 3.67          |
| Step_0-EnvExecTime      | 17.6          |
| Step_0-MaxReturn        | 96.5          |
| Step_0-MinReturn        | -16.3         |
| Step_0-NumTrajs         | 11582         |
| Step_0-PolicyExecTime   | 1             |
| Step_0-StdReturn        | 8.46          |
| Step_1-AverageDiscou... | 5.99          |
| Step_1-AveragePolicyStd | 0.90648764    |
| Step_1-AverageReturn    | 6.72          |
| Step_1-EnvExecTime      | 17.4          |
| Step_1-MaxReturn        | 145           |
| Step_1-MinReturn        | -26.4         |
| Step_1-NumTrajs         | 9024          |
| Step_1-PolicyExecTime   | 3.06          |
| Step_1-StdReturn        | 12.1          |
| Time                    | 652           |
| Time-InnerStep          | 0.155         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 4.2           |
| Time-Sampling           | 42.2          |
| Time-TotalInner         | 46.7          |
| dLoss                   | 0.015500887   |
| n_timesteps             | 3200000       |
-------------------------------------------

 ---------------- Iteration 10 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 10             |
| ItrTime                 | 59.5           |
| LossAfter               | -0.014443112   |
| LossBefore              | -3.9883954e-10 |
| MeanKL                  | 0.0065264017   |
| MeanKLBefore            | -2.0493705e-09 |
| Step_0-AverageDiscou... | 5.55           |
| Step_0-AveragePolicyStd | 0.8964488      |
| Step_0-AverageReturn    | 6.12           |
| Step_0-EnvExecTime      | 17.4           |
| Step_0-MaxReturn        | 98.7           |
| Step_0-MinReturn        | -20.6          |
| Step_0-NumTrajs         | 10314          |
| Step_0-PolicyExecTime   | 1.02           |
| Step_0-StdReturn        | 10.1           |
| Step_1-AverageDiscou... | 8.86           |
| Step_1-AveragePolicyStd | 0.8950974      |
| Step_1-AverageReturn    | 10.1           |
| Step_1-EnvExecTime      | 17.2           |
| Step_1-MaxReturn        | 124            |
| Step_1-MinReturn        | -22.7          |
| Step_1-NumTrajs         | 7867           |
| Step_1-PolicyExecTime   | 3.08           |
| Step_1-StdReturn        | 14             |
| Time                    | 712            |
| Time-InnerStep          | 0.154          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 3.79           |
| Time-Sampling           | 41.8           |
| Time-TotalInner         | 45.8           |
| dLoss                   | 0.014443112    |
| n_timesteps             | 3520000        |
--------------------------------------------

 ---------------- Iteration 11 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 11            |
| ItrTime                 | 59.4          |
| LossAfter               | -0.014049512  |
| LossBefore              | 3.8377186e-09 |
| MeanKL                  | 0.008986273   |
| MeanKLBefore            | 1.4901325e-09 |
| Step_0-AverageDiscou... | 5.91          |
| Step_0-AveragePolicyStd | 0.8900761     |
| Step_0-AverageReturn    | 6.57          |
| Step_0-EnvExecTime      | 17.3          |
| Step_0-MaxReturn        | 108           |
| Step_0-MinReturn        | -31.1         |
| Step_0-NumTrajs         | 9440          |
| Step_0-PolicyExecTime   | 1.01          |
| Step_0-StdReturn        | 11.2          |
| Step_1-AverageDiscou... | 9.11          |
| Step_1-AveragePolicyStd | 0.88884115    |
| Step_1-AverageReturn    | 10.5          |
| Step_1-EnvExecTime      | 17.2          |
| Step_1-MaxReturn        | 123           |
| Step_1-MinReturn        | -23           |
| Step_1-NumTrajs         | 7418          |
| Step_1-PolicyExecTime   | 3.49          |
| Step_1-StdReturn        | 15.4          |
| Time                    | 771           |
| Time-InnerStep          | 0.155         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 3.52          |
| Time-Sampling           | 42            |
| Time-TotalInner         | 45.8          |
| dLoss                   | 0.014049516   |
| n_timesteps             | 3840000       |
-------------------------------------------

 ---------------- Iteration 12 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 12            |
| ItrTime                 | 58.5          |
| LossAfter               | -0.013213232  |
| LossBefore              | 1.2646698e-09 |
| MeanKL                  | 0.008382094   |
| MeanKLBefore            | 8.939486e-09  |
| Step_0-AverageDiscou... | 8.02          |
| Step_0-AveragePolicyStd | 0.87451094    |
| Step_0-AverageReturn    | 9.13          |
| Step_0-EnvExecTime      | 17.2          |
| Step_0-MaxReturn        | 115           |
| Step_0-MinReturn        | -30.4         |
| Step_0-NumTrajs         | 8417          |
| Step_0-PolicyExecTime   | 1             |
| Step_0-StdReturn        | 14.3          |
| Step_1-AverageDiscou... | 12            |
| Step_1-AveragePolicyStd | 0.8733777     |
| Step_1-AverageReturn    | 14.2          |
| Step_1-EnvExecTime      | 17.1          |
| Step_1-MaxReturn        | 156           |
| Step_1-MinReturn        | -28.6         |
| Step_1-NumTrajs         | 6617          |
| Step_1-PolicyExecTime   | 3.09          |
| Step_1-StdReturn        | 19.7          |
| Time                    | 830           |
| Time-InnerStep          | 0.154         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 3.24          |
| Time-Sampling           | 41.5          |
| Time-TotalInner         | 44.9          |
| dLoss                   | 0.013213233   |
| n_timesteps             | 4160000       |
-------------------------------------------

 ---------------- Iteration 13 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 13            |
| ItrTime                 | 58.2          |
| LossAfter               | -0.0127700595 |
| LossBefore              | 5.057428e-09  |
| MeanKL                  | 0.00844157    |
| MeanKLBefore            | 5.959348e-09  |
| Step_0-AverageDiscou... | 10.5          |
| Step_0-AveragePolicyStd | 0.86065906    |
| Step_0-AverageReturn    | 12.3          |
| Step_0-EnvExecTime      | 17.3          |
| Step_0-MaxReturn        | 138           |
| Step_0-MinReturn        | -36.8         |
| Step_0-NumTrajs         | 7348          |
| Step_0-PolicyExecTime   | 1.01          |
| Step_0-StdReturn        | 17.8          |
| Step_1-AverageDiscou... | 14.7          |
| Step_1-AveragePolicyStd | 0.8592994     |
| Step_1-AverageReturn    | 17.8          |
| Step_1-EnvExecTime      | 17.1          |
| Step_1-MaxReturn        | 147           |
| Step_1-MinReturn        | -26.7         |
| Step_1-NumTrajs         | 5854          |
| Step_1-PolicyExecTime   | 3.1           |
| Step_1-StdReturn        | 22.2          |
| Time                    | 888           |
| Time-InnerStep          | 0.15          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 2.85          |
| Time-Sampling           | 41.5          |
| Time-TotalInner         | 44.5          |
| dLoss                   | 0.012770064   |
| n_timesteps             | 4480000       |
-------------------------------------------

 ---------------- Iteration 14 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 14            |
| ItrTime                 | 57.6          |
| LossAfter               | -0.011431383  |
| LossBefore              | 7.7997536e-10 |
| MeanKL                  | 0.009087064   |
| MeanKLBefore            | -2.33058e-13  |
| Step_0-AverageDiscou... | 13.1          |
| Step_0-AveragePolicyStd | 0.84742165    |
| Step_0-AverageReturn    | 15.5          |
| Step_0-EnvExecTime      | 17.2          |
| Step_0-MaxReturn        | 160           |
| Step_0-MinReturn        | -40.8         |
| Step_0-NumTrajs         | 6303          |
| Step_0-PolicyExecTime   | 1.02          |
| Step_0-StdReturn        | 19.8          |
| Step_1-AverageDiscou... | 17.3          |
| Step_1-AveragePolicyStd | 0.8465089     |
| Step_1-AverageReturn    | 21            |
| Step_1-EnvExecTime      | 17.2          |
| Step_1-MaxReturn        | 189           |
| Step_1-MinReturn        | -81.2         |
| Step_1-NumTrajs         | 5167          |
| Step_1-PolicyExecTime   | 3.12          |
| Step_1-StdReturn        | 24            |
| Time                    | 946           |
| Time-InnerStep          | 0.151         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 2.56          |
| Time-Sampling           | 41.3          |
| Time-TotalInner         | 44.1          |
| dLoss                   | 0.011431384   |
| n_timesteps             | 4800000       |
-------------------------------------------

 ---------------- Iteration 15 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 15             |
| ItrTime                 | 57.3           |
| LossAfter               | -0.01067109    |
| LossBefore              | -2.9156224e-09 |
| MeanKL                  | 0.009367781    |
| MeanKLBefore            | -5.9593135e-09 |
| Step_0-AverageDiscou... | 15.3           |
| Step_0-AveragePolicyStd | 0.83432454     |
| Step_0-AverageReturn    | 18.6           |
| Step_0-EnvExecTime      | 17.1           |
| Step_0-MaxReturn        | 162            |
| Step_0-MinReturn        | -22.9          |
| Step_0-NumTrajs         | 5600           |
| Step_0-PolicyExecTime   | 1.02           |
| Step_0-StdReturn        | 24             |
| Step_1-AverageDiscou... | 18.7           |
| Step_1-AveragePolicyStd | 0.83338916     |
| Step_1-AverageReturn    | 23.1           |
| Step_1-EnvExecTime      | 17             |
| Step_1-MaxReturn        | 154            |
| Step_1-MinReturn        | -38.5          |
| Step_1-NumTrajs         | 4786           |
| Step_1-PolicyExecTime   | 3.16           |
| Step_1-StdReturn        | 27             |
| Time                    | 1e+03          |
| Time-InnerStep          | 0.151          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 2.34           |
| Time-Sampling           | 41.1           |
| Time-TotalInner         | 43.7           |
| dLoss                   | 0.010671088    |
| n_timesteps             | 5120000        |
--------------------------------------------

 ---------------- Iteration 16 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 16             |
| ItrTime                 | 57.9           |
| LossAfter               | -0.008552777   |
| LossBefore              | -1.3018728e-09 |
| MeanKL                  | 0.0064048083   |
| MeanKLBefore            | 1.4891348e-09  |
| Step_0-AverageDiscou... | 19.2           |
| Step_0-AveragePolicyStd | 0.82325476     |
| Step_0-AverageReturn    | 23.9           |
| Step_0-EnvExecTime      | 17.2           |
| Step_0-MaxReturn        | 197            |
| Step_0-MinReturn        | -40.5          |
| Step_0-NumTrajs         | 4906           |
| Step_0-PolicyExecTime   | 1.02           |
| Step_0-StdReturn        | 27.6           |
| Step_1-AverageDiscou... | 23.8           |
| Step_1-AveragePolicyStd | 0.82229596     |
| Step_1-AverageReturn    | 30.3           |
| Step_1-EnvExecTime      | 17.4           |
| Step_1-MaxReturn        | 198            |
| Step_1-MinReturn        | -43.1          |
| Step_1-NumTrajs         | 4254           |
| Step_1-PolicyExecTime   | 3.18           |
| Step_1-StdReturn        | 31.1           |
| Time                    | 1.06e+03       |
| Time-InnerStep          | 0.152          |
| Time-MAMLSteps          | 13.9           |
| Time-OuterStep          | 13.9           |
| Time-SampleProc         | 2.15           |
| Time-Sampling           | 41.6           |
| Time-TotalInner         | 44             |
| dLoss                   | 0.008552776    |
| n_timesteps             | 5440000        |
--------------------------------------------

 ---------------- Iteration 17 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 17            |
| ItrTime                 | 57.9          |
| LossAfter               | -0.009935443  |
| LossBefore              | 8.566483e-09  |
| MeanKL                  | 0.008610673   |
| MeanKLBefore            | 1.4884687e-09 |
| Step_0-AverageDiscou... | 19.5          |
| Step_0-AveragePolicyStd | 0.8103022     |
| Step_0-AverageReturn    | 24.3          |
| Step_0-EnvExecTime      | 17.7          |
| Step_0-MaxReturn        | 215           |
| Step_0-MinReturn        | -47.2         |
| Step_0-NumTrajs         | 4778          |
| Step_0-PolicyExecTime   | 1.03          |
| Step_0-StdReturn        | 28.2          |
| Step_1-AverageDiscou... | 23.9          |
| Step_1-AveragePolicyStd | 0.80947196    |
| Step_1-AverageReturn    | 30.4          |
| Step_1-EnvExecTime      | 17.2          |
| Step_1-MaxReturn        | 204           |
| Step_1-MinReturn        | -41.6         |
| Step_1-NumTrajs         | 4196          |
| Step_1-PolicyExecTime   | 3.19          |
| Step_1-StdReturn        | 32.5          |
| Time                    | 1.12e+03      |
| Time-InnerStep          | 0.15          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 2.08          |
| Time-Sampling           | 42            |
| Time-TotalInner         | 44.3          |
| dLoss                   | 0.009935452   |
| n_timesteps             | 5760000       |
-------------------------------------------

 ---------------- Iteration 18 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 18            |
| ItrTime                 | 57            |
| LossAfter               | -0.009511951  |
| LossBefore              | 5.978967e-11  |
| MeanKL                  | 0.006418444   |
| MeanKLBefore            | 2.9791187e-09 |
| Step_0-AverageDiscou... | 22.7          |
| Step_0-AveragePolicyStd | 0.79643995    |
| Step_0-AverageReturn    | 28.8          |
| Step_0-EnvExecTime      | 17            |
| Step_0-MaxReturn        | 186           |
| Step_0-MinReturn        | -47.5         |
| Step_0-NumTrajs         | 4377          |
| Step_0-PolicyExecTime   | 1.02          |
| Step_0-StdReturn        | 31.2          |
| Step_1-AverageDiscou... | 26.9          |
| Step_1-AveragePolicyStd | 0.79564005    |
| Step_1-AverageReturn    | 34.9          |
| Step_1-EnvExecTime      | 17.2          |
| Step_1-MaxReturn        | 242           |
| Step_1-MinReturn        | -43.8         |
| Step_1-NumTrajs         | 3873          |
| Step_1-PolicyExecTime   | 3.2           |
| Step_1-StdReturn        | 34.5          |
| Time                    | 1.18e+03      |
| Time-InnerStep          | 0.15          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.97          |
| Time-Sampling           | 41.2          |
| Time-TotalInner         | 43.4          |
| dLoss                   | 0.009511951   |
| n_timesteps             | 6080000       |
-------------------------------------------

 ---------------- Iteration 19 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 19            |
| ItrTime                 | 57.3          |
| LossAfter               | -0.009736454  |
| LossBefore              | 2.3158464e-10 |
| MeanKL                  | 0.008588948   |
| MeanKLBefore            | 1.8616287e-09 |
| Step_0-AverageDiscou... | 26.4          |
| Step_0-AveragePolicyStd | 0.79201543    |
| Step_0-AverageReturn    | 33.9          |
| Step_0-EnvExecTime      | 17.2          |
| Step_0-MaxReturn        | 219           |
| Step_0-MinReturn        | -41.8         |
| Step_0-NumTrajs         | 3903          |
| Step_0-PolicyExecTime   | 1.04          |
| Step_0-StdReturn        | 33.6          |
| Step_1-AverageDiscou... | 30.1          |
| Step_1-AveragePolicyStd | 0.79129267    |
| Step_1-AverageReturn    | 39.3          |
| Step_1-EnvExecTime      | 17.4          |
| Step_1-MaxReturn        | 214           |
| Step_1-MinReturn        | -63           |
| Step_1-NumTrajs         | 3620          |
| Step_1-PolicyExecTime   | 3.24          |
| Step_1-StdReturn        | 35.7          |
| Time                    | 1.23e+03      |
| Time-InnerStep          | 0.155         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.85          |
| Time-Sampling           | 41.7          |
| Time-TotalInner         | 43.7          |
| dLoss                   | 0.009736454   |
| n_timesteps             | 6400000       |
-------------------------------------------

 ---------------- Iteration 20 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 20            |
| ItrTime                 | 57.4          |
| LossAfter               | -0.0093842205 |
| LossBefore              | 1.1170698e-09 |
| MeanKL                  | 0.007447979   |
| MeanKLBefore            | 6.7045827e-09 |
| Step_0-AverageDiscou... | 31.2          |
| Step_0-AveragePolicyStd | 0.7795809     |
| Step_0-AverageReturn    | 40.9          |
| Step_0-EnvExecTime      | 17.3          |
| Step_0-MaxReturn        | 188           |
| Step_0-MinReturn        | -44.6         |
| Step_0-NumTrajs         | 3607          |
| Step_0-PolicyExecTime   | 1.04          |
| Step_0-StdReturn        | 35.9          |
| Step_1-AverageDiscou... | 35.8          |
| Step_1-AveragePolicyStd | 0.7788435     |
| Step_1-AverageReturn    | 48            |
| Step_1-EnvExecTime      | 17.5          |
| Step_1-MaxReturn        | 203           |
| Step_1-MinReturn        | -32.2         |
| Step_1-NumTrajs         | 3224          |
| Step_1-PolicyExecTime   | 3.26          |
| Step_1-StdReturn        | 38.2          |
| Time                    | 1.29e+03      |
| Time-InnerStep          | 0.153         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.69          |
| Time-Sampling           | 41.9          |
| Time-TotalInner         | 43.8          |
| dLoss                   | 0.009384221   |
| n_timesteps             | 6720000       |
-------------------------------------------

 ---------------- Iteration 21 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 21            |
| ItrTime                 | 57.9          |
| LossAfter               | -0.008771976  |
| LossBefore              | 3.8371213e-09 |
| MeanKL                  | 0.008857472   |
| MeanKLBefore            | -2.980577e-09 |
| Step_0-AverageDiscou... | 33            |
| Step_0-AveragePolicyStd | 0.7693089     |
| Step_0-AverageReturn    | 43.8          |
| Step_0-EnvExecTime      | 17.4          |
| Step_0-MaxReturn        | 217           |
| Step_0-MinReturn        | -41.1         |
| Step_0-NumTrajs         | 3314          |
| Step_0-PolicyExecTime   | 1.05          |
| Step_0-StdReturn        | 37.2          |
| Step_1-AverageDiscou... | 36.8          |
| Step_1-AveragePolicyStd | 0.7688668     |
| Step_1-AverageReturn    | 49.3          |
| Step_1-EnvExecTime      | 17.9          |
| Step_1-MaxReturn        | 223           |
| Step_1-MinReturn        | -69.5         |
| Step_1-NumTrajs         | 3086          |
| Step_1-PolicyExecTime   | 3.28          |
| Step_1-StdReturn        | 38.4          |
| Time                    | 1.35e+03      |
| Time-InnerStep          | 0.153         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.62          |
| Time-Sampling           | 42.5          |
| Time-TotalInner         | 44.3          |
| dLoss                   | 0.00877198    |
| n_timesteps             | 7040000       |
-------------------------------------------

 ---------------- Iteration 22 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 22              |
| ItrTime                 | 57.8            |
| LossAfter               | -0.008460417    |
| LossBefore              | -2.6964513e-09  |
| MeanKL                  | 0.008155324     |
| MeanKLBefore            | -1.34103555e-08 |
| Step_0-AverageDiscou... | 37.1            |
| Step_0-AveragePolicyStd | 0.75738615      |
| Step_0-AverageReturn    | 49.9            |
| Step_0-EnvExecTime      | 17.6            |
| Step_0-MaxReturn        | 204             |
| Step_0-MinReturn        | -48.6           |
| Step_0-NumTrajs         | 3065            |
| Step_0-PolicyExecTime   | 1.05            |
| Step_0-StdReturn        | 38.3            |
| Step_1-AverageDiscou... | 40.4            |
| Step_1-AveragePolicyStd | 0.7568345       |
| Step_1-AverageReturn    | 55.1            |
| Step_1-EnvExecTime      | 17.7            |
| Step_1-MaxReturn        | 229             |
| Step_1-MinReturn        | -54.7           |
| Step_1-NumTrajs         | 2929            |
| Step_1-PolicyExecTime   | 3.29            |
| Step_1-StdReturn        | 39.6            |
| Time                    | 1.41e+03        |
| Time-InnerStep          | 0.153           |
| Time-MAMLSteps          | 13.5            |
| Time-OuterStep          | 13.5            |
| Time-SampleProc         | 1.52            |
| Time-Sampling           | 42.5            |
| Time-TotalInner         | 44.2            |
| dLoss                   | 0.008460415     |
| n_timesteps             | 7360000         |
---------------------------------------------

 ---------------- Iteration 23 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 23             |
| ItrTime                 | 58.4           |
| LossAfter               | -0.008843119   |
| LossBefore              | 4.87042e-09    |
| MeanKL                  | 0.007587263    |
| MeanKLBefore            | -5.9596643e-09 |
| Step_0-AverageDiscou... | 37.9           |
| Step_0-AveragePolicyStd | 0.7483052      |
| Step_0-AverageReturn    | 51.1           |
| Step_0-EnvExecTime      | 17.8           |
| Step_0-MaxReturn        | 212            |
| Step_0-MinReturn        | -71.3          |
| Step_0-NumTrajs         | 3071           |
| Step_0-PolicyExecTime   | 1.06           |
| Step_0-StdReturn        | 39             |
| Step_1-AverageDiscou... | 39.9           |
| Step_1-AveragePolicyStd | 0.7478822      |
| Step_1-AverageReturn    | 54.3           |
| Step_1-EnvExecTime      | 17.9           |
| Step_1-MaxReturn        | 206            |
| Step_1-MinReturn        | -69.4          |
| Step_1-NumTrajs         | 2955           |
| Step_1-PolicyExecTime   | 3.3            |
| Step_1-StdReturn        | 39.9           |
| Time                    | 1.46e+03       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.61           |
| Time-Sampling           | 43             |
| Time-TotalInner         | 44.8           |
| dLoss                   | 0.008843124    |
| n_timesteps             | 7680000        |
--------------------------------------------

 ---------------- Iteration 24 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 24            |
| ItrTime                 | 57.9          |
| LossAfter               | -0.0082052015 |
| LossBefore              | 3.7304138e-10 |
| MeanKL                  | 0.007507344   |
| MeanKLBefore            | -5.960648e-09 |
| Step_0-AverageDiscou... | 39.4          |
| Step_0-AveragePolicyStd | 0.7414441     |
| Step_0-AverageReturn    | 53.5          |
| Step_0-EnvExecTime      | 17.7          |
| Step_0-MaxReturn        | 210           |
| Step_0-MinReturn        | -36.9         |
| Step_0-NumTrajs         | 3025          |
| Step_0-PolicyExecTime   | 1.07          |
| Step_0-StdReturn        | 40.5          |
| Step_1-AverageDiscou... | 41.9          |
| Step_1-AveragePolicyStd | 0.7409549     |
| Step_1-AverageReturn    | 57.5          |
| Step_1-EnvExecTime      | 17.7          |
| Step_1-MaxReturn        | 213           |
| Step_1-MinReturn        | -61.4         |
| Step_1-NumTrajs         | 2905          |
| Step_1-PolicyExecTime   | 3.28          |
| Step_1-StdReturn        | 41.7          |
| Time                    | 1.52e+03      |
| Time-InnerStep          | 0.155         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.53          |
| Time-Sampling           | 42.6          |
| Time-TotalInner         | 44.3          |
| dLoss                   | 0.0082052015  |
| n_timesteps             | 8000000       |
-------------------------------------------

 ---------------- Iteration 25 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 25             |
| ItrTime                 | 58.8           |
| LossAfter               | -0.008883398   |
| LossBefore              | -1.6661819e-09 |
| MeanKL                  | 0.007863542    |
| MeanKLBefore            | -1.9369107e-08 |
| Step_0-AverageDiscou... | 43.7           |
| Step_0-AveragePolicyStd | 0.73728776     |
| Step_0-AverageReturn    | 59.8           |
| Step_0-EnvExecTime      | 17.9           |
| Step_0-MaxReturn        | 217            |
| Step_0-MinReturn        | -99.2          |
| Step_0-NumTrajs         | 2856           |
| Step_0-PolicyExecTime   | 1.07           |
| Step_0-StdReturn        | 40.3           |
| Step_1-AverageDiscou... | 47             |
| Step_1-AveragePolicyStd | 0.73669714     |
| Step_1-AverageReturn    | 65.3           |
| Step_1-EnvExecTime      | 18.3           |
| Step_1-MaxReturn        | 226            |
| Step_1-MinReturn        | -45.8          |
| Step_1-NumTrajs         | 2737           |
| Step_1-PolicyExecTime   | 3.33           |
| Step_1-StdReturn        | 41             |
| Time                    | 1.58e+03       |
| Time-InnerStep          | 0.156          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.51           |
| Time-Sampling           | 43.5           |
| Time-TotalInner         | 45.2           |
| dLoss                   | 0.008883396    |
| n_timesteps             | 8320000        |
--------------------------------------------

 ---------------- Iteration 26 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 26            |
| ItrTime                 | 58.2          |
| LossAfter               | -0.008364355  |
| LossBefore              | 1.8287603e-09 |
| MeanKL                  | 0.007410863   |
| MeanKLBefore            | 3.28626e-14   |
| Step_0-AverageDiscou... | 44.9          |
| Step_0-AveragePolicyStd | 0.7272596     |
| Step_0-AverageReturn    | 61.4          |
| Step_0-EnvExecTime      | 17.8          |
| Step_0-MaxReturn        | 197           |
| Step_0-MinReturn        | -42.2         |
| Step_0-NumTrajs         | 2760          |
| Step_0-PolicyExecTime   | 1.07          |
| Step_0-StdReturn        | 39.5          |
| Step_1-AverageDiscou... | 47.9          |
| Step_1-AveragePolicyStd | 0.72699493    |
| Step_1-AverageReturn    | 66.8          |
| Step_1-EnvExecTime      | 17.9          |
| Step_1-MaxReturn        | 217           |
| Step_1-MinReturn        | -118          |
| Step_1-NumTrajs         | 2639          |
| Step_1-PolicyExecTime   | 3.31          |
| Step_1-StdReturn        | 41.2          |
| Time                    | 1.64e+03      |
| Time-InnerStep          | 0.156         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.42          |
| Time-Sampling           | 43            |
| Time-TotalInner         | 44.7          |
| dLoss                   | 0.008364357   |
| n_timesteps             | 8640000       |
-------------------------------------------

 ---------------- Iteration 27 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 27            |
| ItrTime                 | 58.9          |
| LossAfter               | -0.0076729767 |
| LossBefore              | 3.677323e-09  |
| MeanKL                  | 0.0071975915  |
| MeanKLBefore            | 7.4498443e-09 |
| Step_0-AverageDiscou... | 49.5          |
| Step_0-AveragePolicyStd | 0.7186966     |
| Step_0-AverageReturn    | 68.7          |
| Step_0-EnvExecTime      | 18.1          |
| Step_0-MaxReturn        | 213           |
| Step_0-MinReturn        | -50.1         |
| Step_0-NumTrajs         | 2627          |
| Step_0-PolicyExecTime   | 1.08          |
| Step_0-StdReturn        | 41.7          |
| Step_1-AverageDiscou... | 51.8          |
| Step_1-AveragePolicyStd | 0.7182149     |
| Step_1-AverageReturn    | 73.4          |
| Step_1-EnvExecTime      | 18.3          |
| Step_1-MaxReturn        | 232           |
| Step_1-MinReturn        | -47.4         |
| Step_1-NumTrajs         | 2523          |
| Step_1-PolicyExecTime   | 3.35          |
| Step_1-StdReturn        | 43.3          |
| Time                    | 1.7e+03       |
| Time-InnerStep          | 0.156         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.4           |
| Time-Sampling           | 43.7          |
| Time-TotalInner         | 45.3          |
| dLoss                   | 0.0076729804  |
| n_timesteps             | 8960000       |
-------------------------------------------

 ---------------- Iteration 28 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 28            |
| ItrTime                 | 59.1          |
| LossAfter               | -0.008200617  |
| LossBefore              | 5.332407e-10  |
| MeanKL                  | 0.0071342164  |
| MeanKLBefore            | 1.1254997e-12 |
| Step_0-AverageDiscou... | 49.8          |
| Step_0-AveragePolicyStd | 0.7141866     |
| Step_0-AverageReturn    | 69.4          |
| Step_0-EnvExecTime      | 18.3          |
| Step_0-MaxReturn        | 239           |
| Step_0-MinReturn        | -46.3         |
| Step_0-NumTrajs         | 2629          |
| Step_0-PolicyExecTime   | 1.09          |
| Step_0-StdReturn        | 41.8          |
| Step_1-AverageDiscou... | 51.4          |
| Step_1-AveragePolicyStd | 0.713743      |
| Step_1-AverageReturn    | 71.9          |
| Step_1-EnvExecTime      | 18.3          |
| Step_1-MaxReturn        | 247           |
| Step_1-MinReturn        | -50.5         |
| Step_1-NumTrajs         | 2573          |
| Step_1-PolicyExecTime   | 3.38          |
| Step_1-StdReturn        | 42.1          |
| Time                    | 1.76e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.39          |
| Time-Sampling           | 43.9          |
| Time-TotalInner         | 45.5          |
| dLoss                   | 0.0082006175  |
| n_timesteps             | 9280000       |
-------------------------------------------

 ---------------- Iteration 29 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 29            |
| ItrTime                 | 59.3          |
| LossAfter               | -0.0081129195 |
| LossBefore              | -5.679127e-10 |
| MeanKL                  | 0.007873689   |
| MeanKLBefore            | 3.7260657e-09 |
| Step_0-AverageDiscou... | 50.3          |
| Step_0-AveragePolicyStd | 0.7088768     |
| Step_0-AverageReturn    | 69.7          |
| Step_0-EnvExecTime      | 18.3          |
| Step_0-MaxReturn        | 228           |
| Step_0-MinReturn        | -50.1         |
| Step_0-NumTrajs         | 2642          |
| Step_0-PolicyExecTime   | 1.08          |
| Step_0-StdReturn        | 39.8          |
| Step_1-AverageDiscou... | 52.6          |
| Step_1-AveragePolicyStd | 0.7084561     |
| Step_1-AverageReturn    | 73.6          |
| Step_1-EnvExecTime      | 18.4          |
| Step_1-MaxReturn        | 237           |
| Step_1-MinReturn        | -40.7         |
| Step_1-NumTrajs         | 2579          |
| Step_1-PolicyExecTime   | 3.38          |
| Step_1-StdReturn        | 41.4          |
| Time                    | 1.82e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.41          |
| Time-Sampling           | 44.1          |
| Time-TotalInner         | 45.7          |
| dLoss                   | 0.008112919   |
| n_timesteps             | 9600000       |
-------------------------------------------

 ---------------- Iteration 30 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 30             |
| ItrTime                 | 59.7           |
| LossAfter               | -0.007992731   |
| LossBefore              | -6.4196726e-10 |
| MeanKL                  | 0.0073638437   |
| MeanKLBefore            | -1.490663e-09  |
| Step_0-AverageDiscou... | 53.5           |
| Step_0-AveragePolicyStd | 0.7003268      |
| Step_0-AverageReturn    | 75.2           |
| Step_0-EnvExecTime      | 18.4           |
| Step_0-MaxReturn        | 233            |
| Step_0-MinReturn        | -46            |
| Step_0-NumTrajs         | 2597           |
| Step_0-PolicyExecTime   | 1.09           |
| Step_0-StdReturn        | 41.7           |
| Step_1-AverageDiscou... | 55.8           |
| Step_1-AveragePolicyStd | 0.7000237      |
| Step_1-AverageReturn    | 79.4           |
| Step_1-EnvExecTime      | 18.7           |
| Step_1-MaxReturn        | 259            |
| Step_1-MinReturn        | -57.3          |
| Step_1-NumTrajs         | 2474           |
| Step_1-PolicyExecTime   | 3.41           |
| Step_1-StdReturn        | 42.1           |
| Time                    | 1.88e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.36           |
| Time-Sampling           | 44.5           |
| Time-TotalInner         | 46.1           |
| dLoss                   | 0.0079927305   |
| n_timesteps             | 9920000        |
--------------------------------------------

 ---------------- Iteration 31 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 31             |
| ItrTime                 | 59.9           |
| LossAfter               | -0.007922001   |
| LossBefore              | -2.6853915e-09 |
| MeanKL                  | 0.0064900927   |
| MeanKLBefore            | -1.1918431e-08 |
| Step_0-AverageDiscou... | 57.1           |
| Step_0-AveragePolicyStd | 0.6946587      |
| Step_0-AverageReturn    | 81.7           |
| Step_0-EnvExecTime      | 18.6           |
| Step_0-MaxReturn        | 249            |
| Step_0-MinReturn        | -72.1          |
| Step_0-NumTrajs         | 2415           |
| Step_0-PolicyExecTime   | 1.09           |
| Step_0-StdReturn        | 40.7           |
| Step_1-AverageDiscou... | 58.4           |
| Step_1-AveragePolicyStd | 0.6944112      |
| Step_1-AverageReturn    | 84             |
| Step_1-EnvExecTime      | 18.8           |
| Step_1-MaxReturn        | 236            |
| Step_1-MinReturn        | -41.4          |
| Step_1-NumTrajs         | 2385           |
| Step_1-PolicyExecTime   | 3.42           |
| Step_1-StdReturn        | 41.7           |
| Time                    | 1.94e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.33           |
| Time-Sampling           | 44.8           |
| Time-TotalInner         | 46.3           |
| dLoss                   | 0.007921998    |
| n_timesteps             | 10240000       |
--------------------------------------------

 ---------------- Iteration 32 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 32           |
| ItrTime                 | 60.9         |
| LossAfter               | -0.008105626 |
| LossBefore              | 4.543499e-09 |
| MeanKL                  | 0.0071345074 |
| MeanKLBefore            | -7.46212e-10 |
| Step_0-AverageDiscou... | 58.3         |
| Step_0-AveragePolicyStd | 0.6924197    |
| Step_0-AverageReturn    | 83.4         |
| Step_0-EnvExecTime      | 19.1         |
| Step_0-MaxReturn        | 238          |
| Step_0-MinReturn        | -56.2        |
| Step_0-NumTrajs         | 2386         |
| Step_0-PolicyExecTime   | 1.11         |
| Step_0-StdReturn        | 41.9         |
| Step_1-AverageDiscou... | 60.5         |
| Step_1-AveragePolicyStd | 0.6920156    |
| Step_1-AverageReturn    | 87.3         |
| Step_1-EnvExecTime      | 19.2         |
| Step_1-MaxReturn        | 233          |
| Step_1-MinReturn        | -44.8        |
| Step_1-NumTrajs         | 2332         |
| Step_1-PolicyExecTime   | 3.44         |
| Step_1-StdReturn        | 42.4         |
| Time                    | 2e+03        |
| Time-InnerStep          | 0.159        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 1.32         |
| Time-Sampling           | 45.8         |
| Time-TotalInner         | 47.3         |
| dLoss                   | 0.008105631  |
| n_timesteps             | 10560000     |
------------------------------------------

 ---------------- Iteration 33 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 33            |
| ItrTime                 | 60.3          |
| LossAfter               | -0.008285912  |
| LossBefore              | -1.369778e-09 |
| MeanKL                  | 0.007840104   |
| MeanKLBefore            | 3.7249301e-09 |
| Step_0-AverageDiscou... | 61            |
| Step_0-AveragePolicyStd | 0.6857807     |
| Step_0-AverageReturn    | 87.8          |
| Step_0-EnvExecTime      | 18.8          |
| Step_0-MaxReturn        | 231           |
| Step_0-MinReturn        | -51.6         |
| Step_0-NumTrajs         | 2412          |
| Step_0-PolicyExecTime   | 1.1           |
| Step_0-StdReturn        | 40.5          |
| Step_1-AverageDiscou... | 63            |
| Step_1-AveragePolicyStd | 0.68550575    |
| Step_1-AverageReturn    | 91.5          |
| Step_1-EnvExecTime      | 19            |
| Step_1-MaxReturn        | 242           |
| Step_1-MinReturn        | -48           |
| Step_1-NumTrajs         | 2321          |
| Step_1-PolicyExecTime   | 3.41          |
| Step_1-StdReturn        | 41.3          |
| Time                    | 2.06e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.31          |
| Time-Sampling           | 45.2          |
| Time-TotalInner         | 46.7          |
| dLoss                   | 0.008285911   |
| n_timesteps             | 10880000      |
-------------------------------------------

 ---------------- Iteration 34 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 34             |
| ItrTime                 | 61.2           |
| LossAfter               | -0.008006264   |
| LossBefore              | 5.544579e-10   |
| MeanKL                  | 0.007361668    |
| MeanKLBefore            | -5.2135314e-09 |
| Step_0-AverageDiscou... | 61.6           |
| Step_0-AveragePolicyStd | 0.6762702      |
| Step_0-AverageReturn    | 89.4           |
| Step_0-EnvExecTime      | 19.2           |
| Step_0-MaxReturn        | 260            |
| Step_0-MinReturn        | -47.6          |
| Step_0-NumTrajs         | 2233           |
| Step_0-PolicyExecTime   | 1.12           |
| Step_0-StdReturn        | 41.4           |
| Step_1-AverageDiscou... | 64.9           |
| Step_1-AveragePolicyStd | 0.6760048      |
| Step_1-AverageReturn    | 95.6           |
| Step_1-EnvExecTime      | 19.5           |
| Step_1-MaxReturn        | 234            |
| Step_1-MinReturn        | -47.7          |
| Step_1-NumTrajs         | 2206           |
| Step_1-PolicyExecTime   | 3.48           |
| Step_1-StdReturn        | 40.8           |
| Time                    | 2.12e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.27           |
| Time-Sampling           | 46.2           |
| Time-TotalInner         | 47.7           |
| dLoss                   | 0.008006265    |
| n_timesteps             | 11200000       |
--------------------------------------------

 ---------------- Iteration 35 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 35            |
| ItrTime                 | 62.4          |
| LossAfter               | -0.007847508  |
| LossBefore              | -5.913451e-10 |
| MeanKL                  | 0.006644789   |
| MeanKLBefore            | 6.7051835e-09 |
| Step_0-AverageDiscou... | 64.3          |
| Step_0-AveragePolicyStd | 0.6718754     |
| Step_0-AverageReturn    | 94.1          |
| Step_0-EnvExecTime      | 19.6          |
| Step_0-MaxReturn        | 249           |
| Step_0-MinReturn        | -49.7         |
| Step_0-NumTrajs         | 2210          |
| Step_0-PolicyExecTime   | 1.12          |
| Step_0-StdReturn        | 41.1          |
| Step_1-AverageDiscou... | 67.1          |
| Step_1-AveragePolicyStd | 0.67155844    |
| Step_1-AverageReturn    | 99.5          |
| Step_1-EnvExecTime      | 19.8          |
| Step_1-MaxReturn        | 244           |
| Step_1-MinReturn        | -48.8         |
| Step_1-NumTrajs         | 2183          |
| Step_1-PolicyExecTime   | 3.89          |
| Step_1-StdReturn        | 41.4          |
| Time                    | 2.18e+03      |
| Time-InnerStep          | 0.157         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.26          |
| Time-Sampling           | 47.4          |
| Time-TotalInner         | 48.9          |
| dLoss                   | 0.0078475075  |
| n_timesteps             | 11520000      |
-------------------------------------------

 ---------------- Iteration 36 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 36             |
| ItrTime                 | 61.8           |
| LossAfter               | -0.008148598   |
| LossBefore              | -8.6101365e-10 |
| MeanKL                  | 0.0077260425   |
| MeanKLBefore            | -2.9791427e-09 |
| Step_0-AverageDiscou... | 63.4           |
| Step_0-AveragePolicyStd | 0.6652333      |
| Step_0-AverageReturn    | 93.3           |
| Step_0-EnvExecTime      | 19.4           |
| Step_0-MaxReturn        | 231            |
| Step_0-MinReturn        | -44.7          |
| Step_0-NumTrajs         | 2235           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 41.6           |
| Step_1-AverageDiscou... | 64.8           |
| Step_1-AveragePolicyStd | 0.66498923     |
| Step_1-AverageReturn    | 96.1           |
| Step_1-EnvExecTime      | 19.7           |
| Step_1-MaxReturn        | 258            |
| Step_1-MinReturn        | -24.3          |
| Step_1-NumTrajs         | 2171           |
| Step_1-PolicyExecTime   | 3.52           |
| Step_1-StdReturn        | 41.8           |
| Time                    | 2.24e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.26           |
| Time-Sampling           | 46.7           |
| Time-TotalInner         | 48.2           |
| dLoss                   | 0.008148597    |
| n_timesteps             | 11840000       |
--------------------------------------------

 ---------------- Iteration 37 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 37             |
| ItrTime                 | 62.1           |
| LossAfter               | -0.0081069125  |
| LossBefore              | -1.0348655e-11 |
| MeanKL                  | 0.008042772    |
| MeanKLBefore            | 2.2353057e-09  |
| Step_0-AverageDiscou... | 68             |
| Step_0-AveragePolicyStd | 0.6603411      |
| Step_0-AverageReturn    | 101            |
| Step_0-EnvExecTime      | 19.6           |
| Step_0-MaxReturn        | 270            |
| Step_0-MinReturn        | -27.6          |
| Step_0-NumTrajs         | 2224           |
| Step_0-PolicyExecTime   | 1.12           |
| Step_0-StdReturn        | 40             |
| Step_1-AverageDiscou... | 70.1           |
| Step_1-AveragePolicyStd | 0.66016597     |
| Step_1-AverageReturn    | 105            |
| Step_1-EnvExecTime      | 19.8           |
| Step_1-MaxReturn        | 245            |
| Step_1-MinReturn        | -16.3          |
| Step_1-NumTrajs         | 2207           |
| Step_1-PolicyExecTime   | 3.49           |
| Step_1-StdReturn        | 39.8           |
| Time                    | 2.31e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.27           |
| Time-Sampling           | 47             |
| Time-TotalInner         | 48.5           |
| dLoss                   | 0.0081069125   |
| n_timesteps             | 12160000       |
--------------------------------------------

 ---------------- Iteration 38 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 38             |
| ItrTime                 | 61.5           |
| LossAfter               | -0.008610186   |
| LossBefore              | -5.6217795e-09 |
| MeanKL                  | 0.0074651376   |
| MeanKLBefore            | -4.4703428e-09 |
| Step_0-AverageDiscou... | 69.1           |
| Step_0-AveragePolicyStd | 0.6542293      |
| Step_0-AverageReturn    | 102            |
| Step_0-EnvExecTime      | 19.2           |
| Step_0-MaxReturn        | 254            |
| Step_0-MinReturn        | -16.7          |
| Step_0-NumTrajs         | 2214           |
| Step_0-PolicyExecTime   | 1.12           |
| Step_0-StdReturn        | 39.3           |
| Step_1-AverageDiscou... | 70.2           |
| Step_1-AveragePolicyStd | 0.6538698      |
| Step_1-AverageReturn    | 104            |
| Step_1-EnvExecTime      | 19.7           |
| Step_1-MaxReturn        | 259            |
| Step_1-MinReturn        | -47.6          |
| Step_1-NumTrajs         | 2201           |
| Step_1-PolicyExecTime   | 3.5            |
| Step_1-StdReturn        | 38.9           |
| Time                    | 2.37e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.26           |
| Time-Sampling           | 46.5           |
| Time-TotalInner         | 48             |
| dLoss                   | 0.008610181    |
| n_timesteps             | 12480000       |
--------------------------------------------

 ---------------- Iteration 39 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 39            |
| ItrTime                 | 63            |
| LossAfter               | -0.0076683364 |
| LossBefore              | 5.50866e-10   |
| MeanKL                  | 0.0072681373  |
| MeanKLBefore            | 1.4894095e-09 |
| Step_0-AverageDiscou... | 69            |
| Step_0-AveragePolicyStd | 0.64945173    |
| Step_0-AverageReturn    | 103           |
| Step_0-EnvExecTime      | 20            |
| Step_0-MaxReturn        | 244           |
| Step_0-MinReturn        | -32.7         |
| Step_0-NumTrajs         | 2158          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 41.2          |
| Step_1-AverageDiscou... | 70.3          |
| Step_1-AveragePolicyStd | 0.64926064    |
| Step_1-AverageReturn    | 105           |
| Step_1-EnvExecTime      | 20.2          |
| Step_1-MaxReturn        | 264           |
| Step_1-MinReturn        | -48.7         |
| Step_1-NumTrajs         | 2112          |
| Step_1-PolicyExecTime   | 3.5           |
| Step_1-StdReturn        | 40.8          |
| Time                    | 2.43e+03      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.23          |
| Time-Sampling           | 47.9          |
| Time-TotalInner         | 49.3          |
| dLoss                   | 0.007668337   |
| n_timesteps             | 12800000      |
-------------------------------------------

 ---------------- Iteration 40 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 40             |
| ItrTime                 | 62.2           |
| LossAfter               | -0.007861057   |
| LossBefore              | -1.2571431e-09 |
| MeanKL                  | 0.0075340928   |
| MeanKLBefore            | 4.4695647e-09  |
| Step_0-AverageDiscou... | 71.2           |
| Step_0-AveragePolicyStd | 0.64425087     |
| Step_0-AverageReturn    | 105            |
| Step_0-EnvExecTime      | 19.7           |
| Step_0-MaxReturn        | 267            |
| Step_0-MinReturn        | -38.7          |
| Step_0-NumTrajs         | 2232           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 38.4           |
| Step_1-AverageDiscou... | 72.5           |
| Step_1-AveragePolicyStd | 0.644007       |
| Step_1-AverageReturn    | 109            |
| Step_1-EnvExecTime      | 19.8           |
| Step_1-MaxReturn        | 266            |
| Step_1-MinReturn        | -7.18          |
| Step_1-NumTrajs         | 2134           |
| Step_1-PolicyExecTime   | 3.48           |
| Step_1-StdReturn        | 39.6           |
| Time                    | 2.49e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.25           |
| Time-Sampling           | 47.1           |
| Time-TotalInner         | 48.6           |
| dLoss                   | 0.007861056    |
| n_timesteps             | 13120000       |
--------------------------------------------

 ---------------- Iteration 41 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 41             |
| ItrTime                 | 63             |
| LossAfter               | -0.007619093   |
| LossBefore              | -1.2129798e-09 |
| MeanKL                  | 0.0065881684   |
| MeanKLBefore            | 5.9589853e-09  |
| Step_0-AverageDiscou... | 72.3           |
| Step_0-AveragePolicyStd | 0.6372539      |
| Step_0-AverageReturn    | 108            |
| Step_0-EnvExecTime      | 20.1           |
| Step_0-MaxReturn        | 267            |
| Step_0-MinReturn        | -7.03          |
| Step_0-NumTrajs         | 2154           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 38.5           |
| Step_1-AverageDiscou... | 74.3           |
| Step_1-AveragePolicyStd | 0.63689697     |
| Step_1-AverageReturn    | 112            |
| Step_1-EnvExecTime      | 20.3           |
| Step_1-MaxReturn        | 249            |
| Step_1-MinReturn        | -10.5          |
| Step_1-NumTrajs         | 2117           |
| Step_1-PolicyExecTime   | 3.49           |
| Step_1-StdReturn        | 39.1           |
| Time                    | 2.56e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.24           |
| Time-Sampling           | 48             |
| Time-TotalInner         | 49.4           |
| dLoss                   | 0.007619092    |
| n_timesteps             | 13440000       |
--------------------------------------------

 ---------------- Iteration 42 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 42            |
| ItrTime                 | 64            |
| LossAfter               | -0.007970696  |
| LossBefore              | -2.728438e-10 |
| MeanKL                  | 0.008395076   |
| MeanKLBefore            | 2.9805833e-09 |
| Step_0-AverageDiscou... | 74.3          |
| Step_0-AveragePolicyStd | 0.63664854    |
| Step_0-AverageReturn    | 111           |
| Step_0-EnvExecTime      | 20.7          |
| Step_0-MaxReturn        | 268           |
| Step_0-MinReturn        | -6.52         |
| Step_0-NumTrajs         | 2194          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 38.5          |
| Step_1-AverageDiscou... | 75.7          |
| Step_1-AveragePolicyStd | 0.6362961     |
| Step_1-AverageReturn    | 114           |
| Step_1-EnvExecTime      | 20.6          |
| Step_1-MaxReturn        | 265           |
| Step_1-MinReturn        | -16.1         |
| Step_1-NumTrajs         | 2136          |
| Step_1-PolicyExecTime   | 3.49          |
| Step_1-StdReturn        | 38.9          |
| Time                    | 2.62e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.26          |
| Time-Sampling           | 49            |
| Time-TotalInner         | 50.4          |
| dLoss                   | 0.007970696   |
| n_timesteps             | 13760000      |
-------------------------------------------

 ---------------- Iteration 43 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 43            |
| ItrTime                 | 64.2          |
| LossAfter               | -0.007300862  |
| LossBefore              | 1.9517237e-09 |
| MeanKL                  | 0.0068160417  |
| MeanKLBefore            | 5.9608043e-09 |
| Step_0-AverageDiscou... | 77.7          |
| Step_0-AveragePolicyStd | 0.63128084    |
| Step_0-AverageReturn    | 117           |
| Step_0-EnvExecTime      | 20.8          |
| Step_0-MaxReturn        | 261           |
| Step_0-MinReturn        | -5.4          |
| Step_0-NumTrajs         | 2164          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 36.9          |
| Step_1-AverageDiscou... | 78.2          |
| Step_1-AveragePolicyStd | 0.63115215    |
| Step_1-AverageReturn    | 118           |
| Step_1-EnvExecTime      | 20.7          |
| Step_1-MaxReturn        | 260           |
| Step_1-MinReturn        | -21.4         |
| Step_1-NumTrajs         | 2135          |
| Step_1-PolicyExecTime   | 3.45          |
| Step_1-StdReturn        | 37.4          |
| Time                    | 2.68e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.29          |
| Time-Sampling           | 49.1          |
| Time-TotalInner         | 50.6          |
| dLoss                   | 0.007300864   |
| n_timesteps             | 14080000      |
-------------------------------------------

 ---------------- Iteration 44 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 44             |
| ItrTime                 | 64.2           |
| LossAfter               | -0.0073610023  |
| LossBefore              | -1.4307435e-09 |
| MeanKL                  | 0.0075291707   |
| MeanKLBefore            | 1.6388991e-08  |
| Step_0-AverageDiscou... | 77.6           |
| Step_0-AveragePolicyStd | 0.6286486      |
| Step_0-AverageReturn    | 117            |
| Step_0-EnvExecTime      | 20.6           |
| Step_0-MaxReturn        | 255            |
| Step_0-MinReturn        | -13.2          |
| Step_0-NumTrajs         | 2143           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 37.6           |
| Step_1-AverageDiscou... | 78.7           |
| Step_1-AveragePolicyStd | 0.62846994     |
| Step_1-AverageReturn    | 120            |
| Step_1-EnvExecTime      | 20.9           |
| Step_1-MaxReturn        | 262            |
| Step_1-MinReturn        | -5.64          |
| Step_1-NumTrajs         | 2087           |
| Step_1-PolicyExecTime   | 3.52           |
| Step_1-StdReturn        | 37.4           |
| Time                    | 2.75e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.21           |
| Time-Sampling           | 49.2           |
| Time-TotalInner         | 50.6           |
| dLoss                   | 0.007361001    |
| n_timesteps             | 14400000       |
--------------------------------------------

 ---------------- Iteration 45 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 45            |
| ItrTime                 | 64.2          |
| LossAfter               | -0.007404706  |
| LossBefore              | -7.66775e-09  |
| MeanKL                  | 0.007407519   |
| MeanKLBefore            | 1.4897876e-09 |
| Step_0-AverageDiscou... | 75.3          |
| Step_0-AveragePolicyStd | 0.6227261     |
| Step_0-AverageReturn    | 114           |
| Step_0-EnvExecTime      | 20.8          |
| Step_0-MaxReturn        | 262           |
| Step_0-MinReturn        | -3.67         |
| Step_0-NumTrajs         | 2105          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 39.7          |
| Step_1-AverageDiscou... | 76.6          |
| Step_1-AveragePolicyStd | 0.62259895    |
| Step_1-AverageReturn    | 117           |
| Step_1-EnvExecTime      | 20.7          |
| Step_1-MaxReturn        | 260           |
| Step_1-MinReturn        | -2.06         |
| Step_1-NumTrajs         | 2081          |
| Step_1-PolicyExecTime   | 3.52          |
| Step_1-StdReturn        | 39.5          |
| Time                    | 2.81e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.21          |
| Time-Sampling           | 49.3          |
| Time-TotalInner         | 50.7          |
| dLoss                   | 0.0074046985  |
| n_timesteps             | 14720000      |
-------------------------------------------

 ---------------- Iteration 46 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 46             |
| ItrTime                 | 64.1           |
| LossAfter               | -0.0071171643  |
| LossBefore              | -8.6030816e-10 |
| MeanKL                  | 0.008109915    |
| MeanKLBefore            | -5.9592926e-09 |
| Step_0-AverageDiscou... | 77.7           |
| Step_0-AveragePolicyStd | 0.62085974     |
| Step_0-AverageReturn    | 118            |
| Step_0-EnvExecTime      | 20.5           |
| Step_0-MaxReturn        | 257            |
| Step_0-MinReturn        | -2.26          |
| Step_0-NumTrajs         | 2132           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 36.9           |
| Step_1-AverageDiscou... | 77.8           |
| Step_1-AveragePolicyStd | 0.62054884     |
| Step_1-AverageReturn    | 119            |
| Step_1-EnvExecTime      | 20.6           |
| Step_1-MaxReturn        | 258            |
| Step_1-MinReturn        | -5.51          |
| Step_1-NumTrajs         | 2094           |
| Step_1-PolicyExecTime   | 3.84           |
| Step_1-StdReturn        | 38.1           |
| Time                    | 2.88e+03       |
| Time-InnerStep          | 0.155          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.24           |
| Time-Sampling           | 49.1           |
| Time-TotalInner         | 50.5           |
| dLoss                   | 0.0071171634   |
| n_timesteps             | 15040000       |
--------------------------------------------

 ---------------- Iteration 47 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 47            |
| ItrTime                 | 64.2          |
| LossAfter               | -0.00755051   |
| LossBefore              | 1.5113765e-09 |
| MeanKL                  | 0.0066807466  |
| MeanKLBefore            | 7.911893e-13  |
| Step_0-AverageDiscou... | 79            |
| Step_0-AveragePolicyStd | 0.6134342     |
| Step_0-AverageReturn    | 119           |
| Step_0-EnvExecTime      | 20.6          |
| Step_0-MaxReturn        | 265           |
| Step_0-MinReturn        | -11           |
| Step_0-NumTrajs         | 2150          |
| Step_0-PolicyExecTime   | 1.12          |
| Step_0-StdReturn        | 36.4          |
| Step_1-AverageDiscou... | 80.1          |
| Step_1-AveragePolicyStd | 0.61318266    |
| Step_1-AverageReturn    | 121           |
| Step_1-EnvExecTime      | 20.9          |
| Step_1-MaxReturn        | 264           |
| Step_1-MinReturn        | -1.28         |
| Step_1-NumTrajs         | 2163          |
| Step_1-PolicyExecTime   | 3.46          |
| Step_1-StdReturn        | 34.1          |
| Time                    | 2.94e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.24          |
| Time-Sampling           | 49.1          |
| Time-TotalInner         | 50.5          |
| dLoss                   | 0.0075505115  |
| n_timesteps             | 15360000      |
-------------------------------------------

 ---------------- Iteration 48 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 48            |
| ItrTime                 | 64.6          |
| LossAfter               | -0.0070479074 |
| LossBefore              | 1.3177432e-09 |
| MeanKL                  | 0.00689868    |
| MeanKLBefore            | 2.9795224e-09 |
| Step_0-AverageDiscou... | 77.7          |
| Step_0-AveragePolicyStd | 0.61417735    |
| Step_0-AverageReturn    | 118           |
| Step_0-EnvExecTime      | 20.9          |
| Step_0-MaxReturn        | 270           |
| Step_0-MinReturn        | 5.24          |
| Step_0-NumTrajs         | 2075          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 37.8          |
| Step_1-AverageDiscou... | 78.8          |
| Step_1-AveragePolicyStd | 0.6139324     |
| Step_1-AverageReturn    | 121           |
| Step_1-EnvExecTime      | 21.1          |
| Step_1-MaxReturn        | 260           |
| Step_1-MinReturn        | -16.8         |
| Step_1-NumTrajs         | 2064          |
| Step_1-PolicyExecTime   | 3.51          |
| Step_1-StdReturn        | 38.4          |
| Time                    | 3e+03         |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.21          |
| Time-Sampling           | 49.6          |
| Time-TotalInner         | 51.1          |
| dLoss                   | 0.007047909   |
| n_timesteps             | 15680000      |
-------------------------------------------

 ---------------- Iteration 49 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 49            |
| ItrTime                 | 64.8          |
| LossAfter               | -0.0070180995 |
| LossBefore              | -5.251632e-09 |
| MeanKL                  | 0.0066509023  |
| MeanKLBefore            | -8.940768e-09 |
| Step_0-AverageDiscou... | 80            |
| Step_0-AveragePolicyStd | 0.6123175     |
| Step_0-AverageReturn    | 123           |
| Step_0-EnvExecTime      | 21.1          |
| Step_0-MaxReturn        | 269           |
| Step_0-MinReturn        | -26.1         |
| Step_0-NumTrajs         | 2067          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 37.3          |
| Step_1-AverageDiscou... | 80.8          |
| Step_1-AveragePolicyStd | 0.6121682     |
| Step_1-AverageReturn    | 123           |
| Step_1-EnvExecTime      | 21.1          |
| Step_1-MaxReturn        | 252           |
| Step_1-MinReturn        | 0.241         |
| Step_1-NumTrajs         | 2100          |
| Step_1-PolicyExecTime   | 3.48          |
| Step_1-StdReturn        | 36.4          |
| Time                    | 3.07e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.2           |
| Time-Sampling           | 49.9          |
| Time-TotalInner         | 51.3          |
| dLoss                   | 0.0070180944  |
| n_timesteps             | 16000000      |
-------------------------------------------

 ---------------- Iteration 50 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 50            |
| ItrTime                 | 66.2          |
| LossAfter               | -0.0070741787 |
| LossBefore              | 7.7790324e-10 |
| MeanKL                  | 0.00722956    |
| MeanKLBefore            | 7.4784623e-13 |
| Step_0-AverageDiscou... | 79.1          |
| Step_0-AveragePolicyStd | 0.610353      |
| Step_0-AverageReturn    | 123           |
| Step_0-EnvExecTime      | 21.7          |
| Step_0-MaxReturn        | 254           |
| Step_0-MinReturn        | 0.494         |
| Step_0-NumTrajs         | 1945          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 39.7          |
| Step_1-AverageDiscou... | 80            |
| Step_1-AveragePolicyStd | 0.61028457    |
| Step_1-AverageReturn    | 125           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 261           |
| Step_1-MinReturn        | -3.3          |
| Step_1-NumTrajs         | 1946          |
| Step_1-PolicyExecTime   | 3.56          |
| Step_1-StdReturn        | 38.4          |
| Time                    | 3.14e+03      |
| Time-InnerStep          | 0.157         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 51.2          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.0070741796  |
| n_timesteps             | 16320000      |
-------------------------------------------

 ---------------- Iteration 51 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 51             |
| ItrTime                 | 65.5           |
| LossAfter               | -0.006877005   |
| LossBefore              | -2.5803029e-09 |
| MeanKL                  | 0.0070256023   |
| MeanKLBefore            | 1.4898058e-08  |
| Step_0-AverageDiscou... | 79.4           |
| Step_0-AveragePolicyStd | 0.60558283     |
| Step_0-AverageReturn    | 122            |
| Step_0-EnvExecTime      | 21.3           |
| Step_0-MaxReturn        | 265            |
| Step_0-MinReturn        | -13.5          |
| Step_0-NumTrajs         | 2027           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 37.8           |
| Step_1-AverageDiscou... | 79.3           |
| Step_1-AveragePolicyStd | 0.6055367      |
| Step_1-AverageReturn    | 121            |
| Step_1-EnvExecTime      | 21.6           |
| Step_1-MaxReturn        | 284            |
| Step_1-MinReturn        | -1.94          |
| Step_1-NumTrajs         | 2048           |
| Step_1-PolicyExecTime   | 3.54           |
| Step_1-StdReturn        | 38.2           |
| Time                    | 3.2e+03        |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.19           |
| Time-Sampling           | 50.6           |
| Time-TotalInner         | 52             |
| dLoss                   | 0.0068770023   |
| n_timesteps             | 16640000       |
--------------------------------------------

 ---------------- Iteration 52 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 52             |
| ItrTime                 | 64.9           |
| LossAfter               | -0.0070305467  |
| LossBefore              | -6.9858425e-10 |
| MeanKL                  | 0.0072487434   |
| MeanKLBefore            | 2.9793716e-09  |
| Step_0-AverageDiscou... | 78             |
| Step_0-AveragePolicyStd | 0.604919       |
| Step_0-AverageReturn    | 119            |
| Step_0-EnvExecTime      | 20.9           |
| Step_0-MaxReturn        | 271            |
| Step_0-MinReturn        | -4.33          |
| Step_0-NumTrajs         | 2073           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 41.1           |
| Step_1-AverageDiscou... | 78.4           |
| Step_1-AveragePolicyStd | 0.60470265     |
| Step_1-AverageReturn    | 120            |
| Step_1-EnvExecTime      | 21.3           |
| Step_1-MaxReturn        | 301            |
| Step_1-MinReturn        | -4.09          |
| Step_1-NumTrajs         | 2067           |
| Step_1-PolicyExecTime   | 3.56           |
| Step_1-StdReturn        | 41.9           |
| Time                    | 3.27e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.19           |
| Time-Sampling           | 50             |
| Time-TotalInner         | 51.3           |
| dLoss                   | 0.0070305457   |
| n_timesteps             | 16960000       |
--------------------------------------------

 ---------------- Iteration 53 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 53            |
| ItrTime                 | 65.3          |
| LossAfter               | -0.007005684  |
| LossBefore              | -3.550961e-09 |
| MeanKL                  | 0.007293974   |
| MeanKLBefore            | 2.9802323e-09 |
| Step_0-AverageDiscou... | 80.3          |
| Step_0-AveragePolicyStd | 0.60035545    |
| Step_0-AverageReturn    | 123           |
| Step_0-EnvExecTime      | 21.4          |
| Step_0-MaxReturn        | 268           |
| Step_0-MinReturn        | -13.5         |
| Step_0-NumTrajs         | 2048          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 37            |
| Step_1-AverageDiscou... | 80.8          |
| Step_1-AveragePolicyStd | 0.600123      |
| Step_1-AverageReturn    | 125           |
| Step_1-EnvExecTime      | 21.2          |
| Step_1-MaxReturn        | 275           |
| Step_1-MinReturn        | -0.386        |
| Step_1-NumTrajs         | 2018          |
| Step_1-PolicyExecTime   | 3.48          |
| Step_1-StdReturn        | 37.7          |
| Time                    | 3.33e+03      |
| Time-InnerStep          | 0.157         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.19          |
| Time-Sampling           | 50.3          |
| Time-TotalInner         | 51.7          |
| dLoss                   | 0.0070056804  |
| n_timesteps             | 17280000      |
-------------------------------------------

 ---------------- Iteration 54 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 54            |
| ItrTime                 | 64.7          |
| LossAfter               | -0.007043696  |
| LossBefore              | -3.738715e-09 |
| MeanKL                  | 0.007187107   |
| MeanKLBefore            | -3.375078e-14 |
| Step_0-AverageDiscou... | 80.4          |
| Step_0-AveragePolicyStd | 0.5974163     |
| Step_0-AverageReturn    | 123           |
| Step_0-EnvExecTime      | 21.1          |
| Step_0-MaxReturn        | 262           |
| Step_0-MinReturn        | 0.158         |
| Step_0-NumTrajs         | 2050          |
| Step_0-PolicyExecTime   | 1.12          |
| Step_0-StdReturn        | 38.2          |
| Step_1-AverageDiscou... | 81.7          |
| Step_1-AveragePolicyStd | 0.59740007    |
| Step_1-AverageReturn    | 126           |
| Step_1-EnvExecTime      | 21            |
| Step_1-MaxReturn        | 260           |
| Step_1-MinReturn        | -8.24         |
| Step_1-NumTrajs         | 2053          |
| Step_1-PolicyExecTime   | 3.49          |
| Step_1-StdReturn        | 36.4          |
| Time                    | 3.4e+03       |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.2           |
| Time-Sampling           | 49.7          |
| Time-TotalInner         | 51.1          |
| dLoss                   | 0.0070436923  |
| n_timesteps             | 17600000      |
-------------------------------------------

 ---------------- Iteration 55 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 55             |
| ItrTime                 | 64.6           |
| LossAfter               | -0.0069342614  |
| LossBefore              | -2.9453786e-09 |
| MeanKL                  | 0.007561496    |
| MeanKLBefore            | 8.9384375e-09  |
| Step_0-AverageDiscou... | 81.4           |
| Step_0-AveragePolicyStd | 0.5951498      |
| Step_0-AverageReturn    | 125            |
| Step_0-EnvExecTime      | 20.8           |
| Step_0-MaxReturn        | 261            |
| Step_0-MinReturn        | -3.46          |
| Step_0-NumTrajs         | 2110           |
| Step_0-PolicyExecTime   | 1.12           |
| Step_0-StdReturn        | 36.8           |
| Step_1-AverageDiscou... | 81.8           |
| Step_1-AveragePolicyStd | 0.5950055      |
| Step_1-AverageReturn    | 126            |
| Step_1-EnvExecTime      | 21.2           |
| Step_1-MaxReturn        | 259            |
| Step_1-MinReturn        | -5.98          |
| Step_1-NumTrajs         | 2084           |
| Step_1-PolicyExecTime   | 3.51           |
| Step_1-StdReturn        | 37.9           |
| Time                    | 3.46e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.22           |
| Time-Sampling           | 49.7           |
| Time-TotalInner         | 51.1           |
| dLoss                   | 0.0069342586   |
| n_timesteps             | 17920000       |
--------------------------------------------

 ---------------- Iteration 56 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 56             |
| ItrTime                 | 65.1           |
| LossAfter               | -0.006612055   |
| LossBefore              | -1.5411834e-09 |
| MeanKL                  | 0.007069981    |
| MeanKLBefore            | 2.9802323e-09  |
| Step_0-AverageDiscou... | 81.6           |
| Step_0-AveragePolicyStd | 0.5916723      |
| Step_0-AverageReturn    | 126            |
| Step_0-EnvExecTime      | 21.3           |
| Step_0-MaxReturn        | 274            |
| Step_0-MinReturn        | -0.65          |
| Step_0-NumTrajs         | 2052           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 36.5           |
| Step_1-AverageDiscou... | 83.2           |
| Step_1-AveragePolicyStd | 0.59146196     |
| Step_1-AverageReturn    | 129            |
| Step_1-EnvExecTime      | 21.2           |
| Step_1-MaxReturn        | 285            |
| Step_1-MinReturn        | -1.75          |
| Step_1-NumTrajs         | 2022           |
| Step_1-PolicyExecTime   | 3.5            |
| Step_1-StdReturn        | 35.9           |
| Time                    | 3.53e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.21           |
| Time-Sampling           | 50.2           |
| Time-TotalInner         | 51.6           |
| dLoss                   | 0.0066120536   |
| n_timesteps             | 18240000       |
--------------------------------------------

 ---------------- Iteration 57 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 57             |
| ItrTime                 | 65.6           |
| LossAfter               | -0.006864792   |
| LossBefore              | -1.9759931e-09 |
| MeanKL                  | 0.006830655    |
| MeanKLBefore            | 7.44917e-09    |
| Step_0-AverageDiscou... | 80.5           |
| Step_0-AveragePolicyStd | 0.5900047      |
| Step_0-AverageReturn    | 124            |
| Step_0-EnvExecTime      | 21.4           |
| Step_0-MaxReturn        | 262            |
| Step_0-MinReturn        | 0.846          |
| Step_0-NumTrajs         | 2017           |
| Step_0-PolicyExecTime   | 1.52           |
| Step_0-StdReturn        | 37.7           |
| Step_1-AverageDiscou... | 81.6           |
| Step_1-AveragePolicyStd | 0.5898158      |
| Step_1-AverageReturn    | 127            |
| Step_1-EnvExecTime      | 21.3           |
| Step_1-MaxReturn        | 273            |
| Step_1-MinReturn        | -3.7           |
| Step_1-NumTrajs         | 2024           |
| Step_1-PolicyExecTime   | 3.49           |
| Step_1-StdReturn        | 37.6           |
| Time                    | 3.59e+03       |
| Time-InnerStep          | 0.156          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.19           |
| Time-Sampling           | 50.6           |
| Time-TotalInner         | 52             |
| dLoss                   | 0.0068647903   |
| n_timesteps             | 18560000       |
--------------------------------------------

 ---------------- Iteration 58 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 58            |
| ItrTime                 | 66.4          |
| LossAfter               | -0.0070036845 |
| LossBefore              | 1.77904e-09   |
| MeanKL                  | 0.0076170727  |
| MeanKLBefore            | 1.0462742e-13 |
| Step_0-AverageDiscou... | 79.7          |
| Step_0-AveragePolicyStd | 0.58818805    |
| Step_0-AverageReturn    | 123           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 260           |
| Step_0-MinReturn        | 3.2           |
| Step_0-NumTrajs         | 2017          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 40.4          |
| Step_1-AverageDiscou... | 81.3          |
| Step_1-AveragePolicyStd | 0.5881968     |
| Step_1-AverageReturn    | 126           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 259           |
| Step_1-MinReturn        | -1.34         |
| Step_1-NumTrajs         | 2024          |
| Step_1-PolicyExecTime   | 3.5           |
| Step_1-StdReturn        | 37.7          |
| Time                    | 3.66e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.19          |
| Time-Sampling           | 51.5          |
| Time-TotalInner         | 52.9          |
| dLoss                   | 0.0070036864  |
| n_timesteps             | 18880000      |
-------------------------------------------

 ---------------- Iteration 59 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 59             |
| ItrTime                 | 65.9           |
| LossAfter               | -0.0069613783  |
| LossBefore              | -1.8189006e-09 |
| MeanKL                  | 0.0068564015   |
| MeanKLBefore            | -5.9590968e-09 |
| Step_0-AverageDiscou... | 79.9           |
| Step_0-AveragePolicyStd | 0.58441305     |
| Step_0-AverageReturn    | 122            |
| Step_0-EnvExecTime      | 21.7           |
| Step_0-MaxReturn        | 268            |
| Step_0-MinReturn        | 1.56           |
| Step_0-NumTrajs         | 2056           |
| Step_0-PolicyExecTime   | 1.12           |
| Step_0-StdReturn        | 40             |
| Step_1-AverageDiscou... | 80.8           |
| Step_1-AveragePolicyStd | 0.58425224     |
| Step_1-AverageReturn    | 124            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 268            |
| Step_1-MinReturn        | -3.23          |
| Step_1-NumTrajs         | 2035           |
| Step_1-PolicyExecTime   | 3.47           |
| Step_1-StdReturn        | 39.5           |
| Time                    | 3.72e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.19           |
| Time-Sampling           | 51             |
| Time-TotalInner         | 52.4           |
| dLoss                   | 0.0069613764   |
| n_timesteps             | 19200000       |
--------------------------------------------

 ---------------- Iteration 60 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 60             |
| ItrTime                 | 65.5           |
| LossAfter               | -0.0067131757  |
| LossBefore              | -9.703864e-10  |
| MeanKL                  | 0.006598802    |
| MeanKLBefore            | -5.9604646e-09 |
| Step_0-AverageDiscou... | 82.2           |
| Step_0-AveragePolicyStd | 0.58207774     |
| Step_0-AverageReturn    | 128            |
| Step_0-EnvExecTime      | 21.4           |
| Step_0-MaxReturn        | 265            |
| Step_0-MinReturn        | 3.05           |
| Step_0-NumTrajs         | 1977           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 38.6           |
| Step_1-AverageDiscou... | 83             |
| Step_1-AveragePolicyStd | 0.5818755      |
| Step_1-AverageReturn    | 130            |
| Step_1-EnvExecTime      | 21.5           |
| Step_1-MaxReturn        | 290            |
| Step_1-MinReturn        | 6.4            |
| Step_1-NumTrajs         | 1990           |
| Step_1-PolicyExecTime   | 3.53           |
| Step_1-StdReturn        | 37.2           |
| Time                    | 3.79e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.18           |
| Time-Sampling           | 50.6           |
| Time-TotalInner         | 52             |
| dLoss                   | 0.0067131747   |
| n_timesteps             | 19520000       |
--------------------------------------------

 ---------------- Iteration 61 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 61            |
| ItrTime                 | 65            |
| LossAfter               | -0.0068882965 |
| LossBefore              | 4.1705945e-09 |
| MeanKL                  | 0.0065024057  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 80.8          |
| Step_0-AveragePolicyStd | 0.5829845     |
| Step_0-AverageReturn    | 125           |
| Step_0-EnvExecTime      | 21.1          |
| Step_0-MaxReturn        | 265           |
| Step_0-MinReturn        | -2.12         |
| Step_0-NumTrajs         | 2032          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 39.3          |
| Step_1-AverageDiscou... | 81.1          |
| Step_1-AveragePolicyStd | 0.5830214     |
| Step_1-AverageReturn    | 126           |
| Step_1-EnvExecTime      | 21.3          |
| Step_1-MaxReturn        | 265           |
| Step_1-MinReturn        | 0.893         |
| Step_1-NumTrajs         | 2009          |
| Step_1-PolicyExecTime   | 3.47          |
| Step_1-StdReturn        | 39.7          |
| Time                    | 3.85e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.22          |
| Time-Sampling           | 50.1          |
| Time-TotalInner         | 51.5          |
| dLoss                   | 0.0068883006  |
| n_timesteps             | 19840000      |
-------------------------------------------

 ---------------- Iteration 62 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 62             |
| ItrTime                 | 66.3           |
| LossAfter               | -0.0065773576  |
| LossBefore              | -3.4973617e-09 |
| MeanKL                  | 0.006823711    |
| MeanKLBefore            | -2.9795373e-09 |
| Step_0-AverageDiscou... | 82.3           |
| Step_0-AveragePolicyStd | 0.5822514      |
| Step_0-AverageReturn    | 128            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 281            |
| Step_0-MinReturn        | 8.51           |
| Step_0-NumTrajs         | 2002           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 37.7           |
| Step_1-AverageDiscou... | 82.7           |
| Step_1-AveragePolicyStd | 0.5821683      |
| Step_1-AverageReturn    | 129            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 268            |
| Step_1-MinReturn        | -1.25          |
| Step_1-NumTrajs         | 1996           |
| Step_1-PolicyExecTime   | 3.49           |
| Step_1-StdReturn        | 37             |
| Time                    | 3.92e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.19           |
| Time-Sampling           | 51.3           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.006577354    |
| n_timesteps             | 20160000       |
--------------------------------------------

 ---------------- Iteration 63 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 63             |
| ItrTime                 | 65.3           |
| LossAfter               | -0.006766091   |
| LossBefore              | -2.7788256e-09 |
| MeanKL                  | 0.0074921055   |
| MeanKLBefore            | -8.939233e-09  |
| Step_0-AverageDiscou... | 80.9           |
| Step_0-AveragePolicyStd | 0.5821863      |
| Step_0-AverageReturn    | 125            |
| Step_0-EnvExecTime      | 21.3           |
| Step_0-MaxReturn        | 251            |
| Step_0-MinReturn        | -0.507         |
| Step_0-NumTrajs         | 2003           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 39.2           |
| Step_1-AverageDiscou... | 81.7           |
| Step_1-AveragePolicyStd | 0.58212596     |
| Step_1-AverageReturn    | 128            |
| Step_1-EnvExecTime      | 21.4           |
| Step_1-MaxReturn        | 263            |
| Step_1-MinReturn        | 4.53           |
| Step_1-NumTrajs         | 1993           |
| Step_1-PolicyExecTime   | 3.52           |
| Step_1-StdReturn        | 39.7           |
| Time                    | 3.99e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.17           |
| Time-Sampling           | 50.4           |
| Time-TotalInner         | 51.8           |
| dLoss                   | 0.0067660883   |
| n_timesteps             | 20480000       |
--------------------------------------------

 ---------------- Iteration 64 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 64             |
| ItrTime                 | 66.2           |
| LossAfter               | -0.006796822   |
| LossBefore              | -2.8351894e-09 |
| MeanKL                  | 0.0071407696   |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | 83.9           |
| Step_0-AveragePolicyStd | 0.5771242      |
| Step_0-AverageReturn    | 131            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 276            |
| Step_0-MinReturn        | 2.55           |
| Step_0-NumTrajs         | 1995           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 35             |
| Step_1-AverageDiscou... | 83.7           |
| Step_1-AveragePolicyStd | 0.57687163     |
| Step_1-AverageReturn    | 131            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 267            |
| Step_1-MinReturn        | -7.24          |
| Step_1-NumTrajs         | 1991           |
| Step_1-PolicyExecTime   | 3.47           |
| Step_1-StdReturn        | 35.4           |
| Time                    | 4.05e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.17           |
| Time-Sampling           | 51.3           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.006796819    |
| n_timesteps             | 20800000       |
--------------------------------------------

 ---------------- Iteration 65 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 65            |
| ItrTime                 | 65.6          |
| LossAfter               | -0.0067778737 |
| LossBefore              | 1.4679424e-09 |
| MeanKL                  | 0.0068519376  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 82.2          |
| Step_0-AveragePolicyStd | 0.5797726     |
| Step_0-AverageReturn    | 128           |
| Step_0-EnvExecTime      | 21.4          |
| Step_0-MaxReturn        | 277           |
| Step_0-MinReturn        | -2.34         |
| Step_0-NumTrajs         | 2007          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 38.6          |
| Step_1-AverageDiscou... | 82.9          |
| Step_1-AveragePolicyStd | 0.57977206    |
| Step_1-AverageReturn    | 129           |
| Step_1-EnvExecTime      | 21.6          |
| Step_1-MaxReturn        | 258           |
| Step_1-MinReturn        | -2.82         |
| Step_1-NumTrajs         | 2010          |
| Step_1-PolicyExecTime   | 3.53          |
| Step_1-StdReturn        | 37            |
| Time                    | 4.12e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.19          |
| Time-Sampling           | 50.7          |
| Time-TotalInner         | 52.1          |
| dLoss                   | 0.006777875   |
| n_timesteps             | 21120000      |
-------------------------------------------

 ---------------- Iteration 66 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 66             |
| ItrTime                 | 66.5           |
| LossAfter               | -0.0066476115  |
| LossBefore              | 3.977975e-09   |
| MeanKL                  | 0.0067950175   |
| MeanKLBefore            | -1.3408517e-08 |
| Step_0-AverageDiscou... | 81.5           |
| Step_0-AveragePolicyStd | 0.5790639      |
| Step_0-AverageReturn    | 127            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 271            |
| Step_0-MinReturn        | 0.703          |
| Step_0-NumTrajs         | 1974           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 41.8           |
| Step_1-AverageDiscou... | 82.1           |
| Step_1-AveragePolicyStd | 0.57905066     |
| Step_1-AverageReturn    | 130            |
| Step_1-EnvExecTime      | 21.9           |
| Step_1-MaxReturn        | 266            |
| Step_1-MinReturn        | 1.78           |
| Step_1-NumTrajs         | 1928           |
| Step_1-PolicyExecTime   | 3.56           |
| Step_1-StdReturn        | 41.3           |
| Time                    | 4.18e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.16           |
| Time-Sampling           | 51.6           |
| Time-TotalInner         | 52.9           |
| dLoss                   | 0.0066476157   |
| n_timesteps             | 21440000       |
--------------------------------------------

 ---------------- Iteration 67 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 67             |
| ItrTime                 | 66.5           |
| LossAfter               | -0.006632968   |
| LossBefore              | -4.0670516e-09 |
| MeanKL                  | 0.0073672966   |
| MeanKLBefore            | -7.7147175e-13 |
| Step_0-AverageDiscou... | 82.9           |
| Step_0-AveragePolicyStd | 0.57921904     |
| Step_0-AverageReturn    | 131            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 281            |
| Step_0-MinReturn        | -3.49          |
| Step_0-NumTrajs         | 1920           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 40             |
| Step_1-AverageDiscou... | 83.6           |
| Step_1-AveragePolicyStd | 0.5790543      |
| Step_1-AverageReturn    | 133            |
| Step_1-EnvExecTime      | 21.9           |
| Step_1-MaxReturn        | 266            |
| Step_1-MinReturn        | 6.58           |
| Step_1-NumTrajs         | 1923           |
| Step_1-PolicyExecTime   | 3.53           |
| Step_1-StdReturn        | 38.8           |
| Time                    | 4.25e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.15           |
| Time-Sampling           | 51.5           |
| Time-TotalInner         | 52.9           |
| dLoss                   | 0.0066329637   |
| n_timesteps             | 21760000       |
--------------------------------------------

 ---------------- Iteration 68 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 68            |
| ItrTime                 | 65.9          |
| LossAfter               | -0.006692428  |
| LossBefore              | 2.1758435e-09 |
| MeanKL                  | 0.0066252956  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 83.6          |
| Step_0-AveragePolicyStd | 0.5730357     |
| Step_0-AverageReturn    | 131           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 276           |
| Step_0-MinReturn        | 2.01          |
| Step_0-NumTrajs         | 1957          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 37.4          |
| Step_1-AverageDiscou... | 83            |
| Step_1-AveragePolicyStd | 0.573028      |
| Step_1-AverageReturn    | 131           |
| Step_1-EnvExecTime      | 21.3          |
| Step_1-MaxReturn        | 260           |
| Step_1-MinReturn        | 4.29          |
| Step_1-NumTrajs         | 1949          |
| Step_1-PolicyExecTime   | 3.49          |
| Step_1-StdReturn        | 39.6          |
| Time                    | 4.32e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 51            |
| Time-TotalInner         | 52.4          |
| dLoss                   | 0.0066924305  |
| n_timesteps             | 22080000      |
-------------------------------------------

 ---------------- Iteration 69 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 69           |
| ItrTime                 | 65.9         |
| LossAfter               | -0.006880039 |
| LossBefore              | 3.343812e-09 |
| MeanKL                  | 0.007774768  |
| MeanKLBefore            | 0.0          |
| Step_0-AverageDiscou... | 82.4         |
| Step_0-AveragePolicyStd | 0.57293534   |
| Step_0-AverageReturn    | 128          |
| Step_0-EnvExecTime      | 21.7         |
| Step_0-MaxReturn        | 285          |
| Step_0-MinReturn        | 2.38         |
| Step_0-NumTrajs         | 1996         |
| Step_0-PolicyExecTime   | 1.14         |
| Step_0-StdReturn        | 39.5         |
| Step_1-AverageDiscou... | 82.8         |
| Step_1-AveragePolicyStd | 0.57286364   |
| Step_1-AverageReturn    | 130          |
| Step_1-EnvExecTime      | 21.6         |
| Step_1-MaxReturn        | 269          |
| Step_1-MinReturn        | -5.76        |
| Step_1-NumTrajs         | 1948         |
| Step_1-PolicyExecTime   | 3.51         |
| Step_1-StdReturn        | 40           |
| Time                    | 4.38e+03     |
| Time-InnerStep          | 0.158        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 1.15         |
| Time-Sampling           | 51           |
| Time-TotalInner         | 52.4         |
| dLoss                   | 0.006880042  |
| n_timesteps             | 22400000     |
------------------------------------------

 ---------------- Iteration 70 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 70             |
| ItrTime                 | 66.6           |
| LossAfter               | -0.006760914   |
| LossBefore              | -1.7416092e-09 |
| MeanKL                  | 0.0073553124   |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | 84.2           |
| Step_0-AveragePolicyStd | 0.56949955     |
| Step_0-AverageReturn    | 133            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 281            |
| Step_0-MinReturn        | 2.08           |
| Step_0-NumTrajs         | 1935           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 38.8           |
| Step_1-AverageDiscou... | 84.3           |
| Step_1-AveragePolicyStd | 0.56947535     |
| Step_1-AverageReturn    | 133            |
| Step_1-EnvExecTime      | 22             |
| Step_1-MaxReturn        | 268            |
| Step_1-MinReturn        | -2.56          |
| Step_1-NumTrajs         | 1921           |
| Step_1-PolicyExecTime   | 3.56           |
| Step_1-StdReturn        | 39.6           |
| Time                    | 4.45e+03       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.16           |
| Time-Sampling           | 51.6           |
| Time-TotalInner         | 53             |
| dLoss                   | 0.006760912    |
| n_timesteps             | 22720000       |
--------------------------------------------

 ---------------- Iteration 71 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 71            |
| ItrTime                 | 65.6          |
| LossAfter               | -0.007063945  |
| LossBefore              | -4.095657e-11 |
| MeanKL                  | 0.0069863885  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 83.5          |
| Step_0-AveragePolicyStd | 0.56904656    |
| Step_0-AverageReturn    | 127           |
| Step_0-EnvExecTime      | 21.5          |
| Step_0-MaxReturn        | 251           |
| Step_0-MinReturn        | 6.98          |
| Step_0-NumTrajs         | 2119          |
| Step_0-PolicyExecTime   | 1.12          |
| Step_0-StdReturn        | 33.3          |
| Step_1-AverageDiscou... | 84.1          |
| Step_1-AveragePolicyStd | 0.56902987    |
| Step_1-AverageReturn    | 129           |
| Step_1-EnvExecTime      | 21.5          |
| Step_1-MaxReturn        | 273           |
| Step_1-MinReturn        | -1.96         |
| Step_1-NumTrajs         | 2075          |
| Step_1-PolicyExecTime   | 3.47          |
| Step_1-StdReturn        | 34.4          |
| Time                    | 4.51e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.24          |
| Time-Sampling           | 50.6          |
| Time-TotalInner         | 52            |
| dLoss                   | 0.007063945   |
| n_timesteps             | 23040000      |
-------------------------------------------

 ---------------- Iteration 72 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 72            |
| ItrTime                 | 66.3          |
| LossAfter               | -0.007075803  |
| LossBefore              | 1.0979502e-10 |
| MeanKL                  | 0.009221368   |
| MeanKLBefore            | 1.4897125e-09 |
| Step_0-AverageDiscou... | 83.7          |
| Step_0-AveragePolicyStd | 0.5697656     |
| Step_0-AverageReturn    | 132           |
| Step_0-EnvExecTime      | 21.6          |
| Step_0-MaxReturn        | 265           |
| Step_0-MinReturn        | -1.34         |
| Step_0-NumTrajs         | 1944          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 36.8          |
| Step_1-AverageDiscou... | 84            |
| Step_1-AveragePolicyStd | 0.56968933    |
| Step_1-AverageReturn    | 133           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 263           |
| Step_1-MinReturn        | 0.38          |
| Step_1-NumTrajs         | 1918          |
| Step_1-PolicyExecTime   | 3.57          |
| Step_1-StdReturn        | 37.3          |
| Time                    | 4.58e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.13          |
| Time-Sampling           | 51.3          |
| Time-TotalInner         | 52.7          |
| dLoss                   | 0.007075803   |
| n_timesteps             | 23360000      |
-------------------------------------------

 ---------------- Iteration 73 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 73             |
| ItrTime                 | 66             |
| LossAfter               | -0.007097131   |
| LossBefore              | -2.8446696e-09 |
| MeanKL                  | 0.0073626786   |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | 84             |
| Step_0-AveragePolicyStd | 0.5641117      |
| Step_0-AverageReturn    | 130            |
| Step_0-EnvExecTime      | 21.7           |
| Step_0-MaxReturn        | 272            |
| Step_0-MinReturn        | 2.08           |
| Step_0-NumTrajs         | 2023           |
| Step_0-PolicyExecTime   | 1.13           |
| Step_0-StdReturn        | 35.9           |
| Step_1-AverageDiscou... | 84.8           |
| Step_1-AveragePolicyStd | 0.5641291      |
| Step_1-AverageReturn    | 132            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 262            |
| Step_1-MinReturn        | 6.39           |
| Step_1-NumTrajs         | 2002           |
| Step_1-PolicyExecTime   | 3.51           |
| Step_1-StdReturn        | 35.4           |
| Time                    | 4.65e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.18           |
| Time-Sampling           | 51             |
| Time-TotalInner         | 52.4           |
| dLoss                   | 0.0070971283   |
| n_timesteps             | 23680000       |
--------------------------------------------

 ---------------- Iteration 74 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 74             |
| ItrTime                 | 66             |
| LossAfter               | -0.008254468   |
| LossBefore              | -2.5938705e-09 |
| MeanKL                  | 0.009943574    |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | 83.2           |
| Step_0-AveragePolicyStd | 0.5639722      |
| Step_0-AverageReturn    | 132            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 264            |
| Step_0-MinReturn        | -5.35          |
| Step_0-NumTrajs         | 1902           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 41.6           |
| Step_1-AverageDiscou... | 83.2           |
| Step_1-AveragePolicyStd | 0.56395704     |
| Step_1-AverageReturn    | 133            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 275            |
| Step_1-MinReturn        | -0.0436        |
| Step_1-NumTrajs         | 1878           |
| Step_1-PolicyExecTime   | 3.53           |
| Step_1-StdReturn        | 43             |
| Time                    | 4.71e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.2           |
| Time-OuterStep          | 13.2           |
| Time-SampleProc         | 1.15           |
| Time-Sampling           | 51.4           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.008254466    |
| n_timesteps             | 24000000       |
--------------------------------------------

 ---------------- Iteration 75 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 75             |
| ItrTime                 | 65.7           |
| LossAfter               | -0.0067910505  |
| LossBefore              | -1.6385242e-09 |
| MeanKL                  | 0.0065448745   |
| MeanKLBefore            | 3.887557e-13   |
| Step_0-AverageDiscou... | 83.4           |
| Step_0-AveragePolicyStd | 0.56447387     |
| Step_0-AverageReturn    | 129            |
| Step_0-EnvExecTime      | 21.5           |
| Step_0-MaxReturn        | 262            |
| Step_0-MinReturn        | -0.621         |
| Step_0-NumTrajs         | 1982           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 37.9           |
| Step_1-AverageDiscou... | 84.7           |
| Step_1-AveragePolicyStd | 0.5644356      |
| Step_1-AverageReturn    | 133            |
| Step_1-EnvExecTime      | 21.5           |
| Step_1-MaxReturn        | 302            |
| Step_1-MinReturn        | -8.32          |
| Step_1-NumTrajs         | 1958           |
| Step_1-PolicyExecTime   | 3.52           |
| Step_1-StdReturn        | 38.6           |
| Time                    | 4.78e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.17           |
| Time-Sampling           | 50.8           |
| Time-TotalInner         | 52.2           |
| dLoss                   | 0.0067910487   |
| n_timesteps             | 24320000       |
--------------------------------------------

 ---------------- Iteration 76 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 76            |
| ItrTime                 | 66            |
| LossAfter               | -0.007096041  |
| LossBefore              | 2.8784406e-09 |
| MeanKL                  | 0.0070801834  |
| MeanKLBefore            | 3.7561065e-13 |
| Step_0-AverageDiscou... | 81.7          |
| Step_0-AveragePolicyStd | 0.56638426    |
| Step_0-AverageReturn    | 127           |
| Step_0-EnvExecTime      | 21.7          |
| Step_0-MaxReturn        | 272           |
| Step_0-MinReturn        | -4.15         |
| Step_0-NumTrajs         | 1943          |
| Step_0-PolicyExecTime   | 1.13          |
| Step_0-StdReturn        | 42.5          |
| Step_1-AverageDiscou... | 82.6          |
| Step_1-AveragePolicyStd | 0.56632173    |
| Step_1-AverageReturn    | 129           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 305           |
| Step_1-MinReturn        | -2.2          |
| Step_1-NumTrajs         | 1955          |
| Step_1-PolicyExecTime   | 3.5           |
| Step_1-StdReturn        | 41.8          |
| Time                    | 4.84e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.18          |
| Time-Sampling           | 51.1          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.007096044   |
| n_timesteps             | 24640000      |
-------------------------------------------

 ---------------- Iteration 77 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 77            |
| ItrTime                 | 66.5          |
| LossAfter               | -0.006643627  |
| LossBefore              | 2.1863358e-09 |
| MeanKL                  | 0.0068452745  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 83.5          |
| Step_0-AveragePolicyStd | 0.5672194     |
| Step_0-AverageReturn    | 132           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 270           |
| Step_0-MinReturn        | -3.11         |
| Step_0-NumTrajs         | 1928          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 41.6          |
| Step_1-AverageDiscou... | 85.1          |
| Step_1-AveragePolicyStd | 0.5672752     |
| Step_1-AverageReturn    | 136           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 300           |
| Step_1-MinReturn        | 0.954         |
| Step_1-NumTrajs         | 1908          |
| Step_1-PolicyExecTime   | 3.55          |
| Step_1-StdReturn        | 41.1          |
| Time                    | 4.91e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.14          |
| Time-Sampling           | 51.5          |
| Time-TotalInner         | 52.9          |
| dLoss                   | 0.006643629   |
| n_timesteps             | 24960000      |
-------------------------------------------

 ---------------- Iteration 78 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 78             |
| ItrTime                 | 67.2           |
| LossAfter               | -0.006800416   |
| LossBefore              | 1.7951489e-09  |
| MeanKL                  | 0.006551978    |
| MeanKLBefore            | -5.9600604e-09 |
| Step_0-AverageDiscou... | 83.7           |
| Step_0-AveragePolicyStd | 0.5668791      |
| Step_0-AverageReturn    | 132            |
| Step_0-EnvExecTime      | 22.1           |
| Step_0-MaxReturn        | 273            |
| Step_0-MinReturn        | 11.8           |
| Step_0-NumTrajs         | 1945           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 39.7           |
| Step_1-AverageDiscou... | 84.7           |
| Step_1-AveragePolicyStd | 0.5669088      |
| Step_1-AverageReturn    | 135            |
| Step_1-EnvExecTime      | 22.4           |
| Step_1-MaxReturn        | 276            |
| Step_1-MinReturn        | 0.689          |
| Step_1-NumTrajs         | 1892           |
| Step_1-PolicyExecTime   | 3.55           |
| Step_1-StdReturn        | 40.4           |
| Time                    | 4.98e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.15           |
| Time-Sampling           | 52.2           |
| Time-TotalInner         | 53.6           |
| dLoss                   | 0.006800418    |
| n_timesteps             | 25280000       |
--------------------------------------------

 ---------------- Iteration 79 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 79             |
| ItrTime                 | 66.3           |
| LossAfter               | -0.0068363575  |
| LossBefore              | -7.9041806e-10 |
| MeanKL                  | 0.00749947     |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | 82.3           |
| Step_0-AveragePolicyStd | 0.56787616     |
| Step_0-AverageReturn    | 132            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 266            |
| Step_0-MinReturn        | 0.601          |
| Step_0-NumTrajs         | 1821           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 45.4           |
| Step_1-AverageDiscou... | 84.4           |
| Step_1-AveragePolicyStd | 0.5677494      |
| Step_1-AverageReturn    | 137            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 294            |
| Step_1-MinReturn        | 4.73           |
| Step_1-NumTrajs         | 1807           |
| Step_1-PolicyExecTime   | 3.58           |
| Step_1-StdReturn        | 43.4           |
| Time                    | 5.04e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.1            |
| Time-Sampling           | 51.4           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.0068363566   |
| n_timesteps             | 25600000       |
--------------------------------------------

 ---------------- Iteration 80 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 80             |
| ItrTime                 | 65.5           |
| LossAfter               | -0.0073123546  |
| LossBefore              | 2.1477446e-09  |
| MeanKL                  | 0.006986937    |
| MeanKLBefore            | -2.9802323e-09 |
| Step_0-AverageDiscou... | 86.1           |
| Step_0-AveragePolicyStd | 0.56378436     |
| Step_0-AverageReturn    | 135            |
| Step_0-EnvExecTime      | 21.3           |
| Step_0-MaxReturn        | 287            |
| Step_0-MinReturn        | -1.08          |
| Step_0-NumTrajs         | 1987           |
| Step_0-PolicyExecTime   | 1.14           |
| Step_0-StdReturn        | 35.5           |
| Step_1-AverageDiscou... | 87.1           |
| Step_1-AveragePolicyStd | 0.5637754      |
| Step_1-AverageReturn    | 139            |
| Step_1-EnvExecTime      | 21.4           |
| Step_1-MaxReturn        | 286            |
| Step_1-MinReturn        | 4.77           |
| Step_1-NumTrajs         | 1923           |
| Step_1-PolicyExecTime   | 3.55           |
| Step_1-StdReturn        | 36.1           |
| Time                    | 5.11e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.18           |
| Time-Sampling           | 50.5           |
| Time-TotalInner         | 51.9           |
| dLoss                   | 0.007312357    |
| n_timesteps             | 25920000       |
--------------------------------------------

 ---------------- Iteration 81 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 81             |
| ItrTime                 | 66.3           |
| LossAfter               | -0.0068805786  |
| LossBefore              | -8.503112e-09  |
| MeanKL                  | 0.007613638    |
| MeanKLBefore            | -1.4901161e-09 |
| Step_0-AverageDiscou... | 84             |
| Step_0-AveragePolicyStd | 0.5618262      |
| Step_0-AverageReturn    | 133            |
| Step_0-EnvExecTime      | 21.7           |
| Step_0-MaxReturn        | 273            |
| Step_0-MinReturn        | -15            |
| Step_0-NumTrajs         | 1910           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 39.7           |
| Step_1-AverageDiscou... | 84.8           |
| Step_1-AveragePolicyStd | 0.56171626     |
| Step_1-AverageReturn    | 137            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 289            |
| Step_1-MinReturn        | 0.366          |
| Step_1-NumTrajs         | 1836           |
| Step_1-PolicyExecTime   | 3.6            |
| Step_1-StdReturn        | 42.5           |
| Time                    | 5.18e+03       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.15           |
| Time-Sampling           | 51.3           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.00688057     |
| n_timesteps             | 26240000       |
--------------------------------------------

 ---------------- Iteration 82 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 82            |
| ItrTime                 | 66.4          |
| LossAfter               | -0.006957636  |
| LossBefore              | -8.352905e-11 |
| MeanKL                  | 0.0075580617  |
| MeanKLBefore            | 8.4252603e-13 |
| Step_0-AverageDiscou... | 82.8          |
| Step_0-AveragePolicyStd | 0.55827326    |
| Step_0-AverageReturn    | 132           |
| Step_0-EnvExecTime      | 21.7          |
| Step_0-MaxReturn        | 289           |
| Step_0-MinReturn        | 0.0829        |
| Step_0-NumTrajs         | 1879          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 44.1          |
| Step_1-AverageDiscou... | 84.3          |
| Step_1-AveragePolicyStd | 0.5581971     |
| Step_1-AverageReturn    | 136           |
| Step_1-EnvExecTime      | 21.8          |
| Step_1-MaxReturn        | 272           |
| Step_1-MinReturn        | -1.67         |
| Step_1-NumTrajs         | 1856          |
| Step_1-PolicyExecTime   | 3.61          |
| Step_1-StdReturn        | 43.5          |
| Time                    | 5.24e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 51.4          |
| Time-TotalInner         | 52.8          |
| dLoss                   | 0.006957636   |
| n_timesteps             | 26560000      |
-------------------------------------------

 ---------------- Iteration 83 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 83             |
| ItrTime                 | 67.3           |
| LossAfter               | -0.0067382827  |
| LossBefore              | 5.6721623e-09  |
| MeanKL                  | 0.0066965693   |
| MeanKLBefore            | -1.4897423e-09 |
| Step_0-AverageDiscou... | 85.3           |
| Step_0-AveragePolicyStd | 0.55855817     |
| Step_0-AverageReturn    | 133            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 275            |
| Step_0-MinReturn        | 4.69           |
| Step_0-NumTrajs         | 2013           |
| Step_0-PolicyExecTime   | 1.23           |
| Step_0-StdReturn        | 36.5           |
| Step_1-AverageDiscou... | 86.9           |
| Step_1-AveragePolicyStd | 0.5585861      |
| Step_1-AverageReturn    | 136            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 270            |
| Step_1-MinReturn        | 2.93           |
| Step_1-NumTrajs         | 1990           |
| Step_1-PolicyExecTime   | 3.94           |
| Step_1-StdReturn        | 36             |
| Time                    | 5.31e+03       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.9           |
| Time-OuterStep          | 13.9           |
| Time-SampleProc         | 1.34           |
| Time-Sampling           | 51.9           |
| Time-TotalInner         | 53.4           |
| dLoss                   | 0.0067382883   |
| n_timesteps             | 26880000       |
--------------------------------------------

 ---------------- Iteration 84 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 84             |
| ItrTime                 | 66.2           |
| LossAfter               | -0.0067448067  |
| LossBefore              | -4.0163677e-09 |
| MeanKL                  | 0.0069601946   |
| MeanKLBefore            | -1.4901161e-09 |
| Step_0-AverageDiscou... | 83             |
| Step_0-AveragePolicyStd | 0.55935144     |
| Step_0-AverageReturn    | 132            |
| Step_0-EnvExecTime      | 21.8           |
| Step_0-MaxReturn        | 269            |
| Step_0-MinReturn        | 7.26           |
| Step_0-NumTrajs         | 1887           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 43.7           |
| Step_1-AverageDiscou... | 84.1           |
| Step_1-AveragePolicyStd | 0.5593814      |
| Step_1-AverageReturn    | 134            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 294            |
| Step_1-MinReturn        | 4.42           |
| Step_1-NumTrajs         | 1860           |
| Step_1-PolicyExecTime   | 3.53           |
| Step_1-StdReturn        | 42.1           |
| Time                    | 5.38e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.14           |
| Time-Sampling           | 51.3           |
| Time-TotalInner         | 52.6           |
| dLoss                   | 0.0067448025   |
| n_timesteps             | 27200000       |
--------------------------------------------

 ---------------- Iteration 85 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 85             |
| ItrTime                 | 66.8           |
| LossAfter               | -0.0068990155  |
| LossBefore              | -6.3993436e-11 |
| MeanKL                  | 0.006826186    |
| MeanKLBefore            | 1.4896838e-09  |
| Step_0-AverageDiscou... | 85.4           |
| Step_0-AveragePolicyStd | 0.5582261      |
| Step_0-AverageReturn    | 137            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 272            |
| Step_0-MinReturn        | -14.4          |
| Step_0-NumTrajs         | 1894           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 40.5           |
| Step_1-AverageDiscou... | 85.2           |
| Step_1-AveragePolicyStd | 0.5581842      |
| Step_1-AverageReturn    | 139            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 321            |
| Step_1-MinReturn        | 8.71           |
| Step_1-NumTrajs         | 1793           |
| Step_1-PolicyExecTime   | 3.59           |
| Step_1-StdReturn        | 43.3           |
| Time                    | 5.44e+03       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 51.9           |
| Time-TotalInner         | 53.2           |
| dLoss                   | 0.0068990155   |
| n_timesteps             | 27520000       |
--------------------------------------------

 ---------------- Iteration 86 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 86            |
| ItrTime                 | 67.4          |
| LossAfter               | -0.006941286  |
| LossBefore              | 3.0535763e-09 |
| MeanKL                  | 0.0067608594  |
| MeanKLBefore            | 2.9802323e-09 |
| Step_0-AverageDiscou... | 83.3          |
| Step_0-AveragePolicyStd | 0.5608268     |
| Step_0-AverageReturn    | 133           |
| Step_0-EnvExecTime      | 21.6          |
| Step_0-MaxReturn        | 275           |
| Step_0-MinReturn        | -8.39         |
| Step_0-NumTrajs         | 1914          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 44.5          |
| Step_1-AverageDiscou... | 83.7          |
| Step_1-AveragePolicyStd | 0.56063783    |
| Step_1-AverageReturn    | 134           |
| Step_1-EnvExecTime      | 22            |
| Step_1-MaxReturn        | 273           |
| Step_1-MinReturn        | 3.54          |
| Step_1-NumTrajs         | 1860          |
| Step_1-PolicyExecTime   | 3.89          |
| Step_1-StdReturn        | 45            |
| Time                    | 5.51e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 14.1          |
| Time-OuterStep          | 14.1          |
| Time-SampleProc         | 1.18          |
| Time-Sampling           | 51.9          |
| Time-TotalInner         | 53.3          |
| dLoss                   | 0.006941289   |
| n_timesteps             | 27840000      |
-------------------------------------------

 ---------------- Iteration 87 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 87            |
| ItrTime                 | 66.8          |
| LossAfter               | -0.006883788  |
| LossBefore              | 3.9049426e-09 |
| MeanKL                  | 0.007440024   |
| MeanKLBefore            | 5.960125e-09  |
| Step_0-AverageDiscou... | 86            |
| Step_0-AveragePolicyStd | 0.55818766    |
| Step_0-AverageReturn    | 141           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 281           |
| Step_0-MinReturn        | -4.22         |
| Step_0-NumTrajs         | 1794          |
| Step_0-PolicyExecTime   | 1.2           |
| Step_0-StdReturn        | 42.2          |
| Step_1-AverageDiscou... | 85.5          |
| Step_1-AveragePolicyStd | 0.5582366     |
| Step_1-AverageReturn    | 141           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 287           |
| Step_1-MinReturn        | 4.98          |
| Step_1-NumTrajs         | 1761          |
| Step_1-PolicyExecTime   | 3.64          |
| Step_1-StdReturn        | 44.7          |
| Time                    | 5.58e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.09          |
| Time-Sampling           | 51.9          |
| Time-TotalInner         | 53.2          |
| dLoss                   | 0.0068837916  |
| n_timesteps             | 28160000      |
-------------------------------------------

 ---------------- Iteration 88 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 88             |
| ItrTime                 | 66.3           |
| LossAfter               | -0.006859819   |
| LossBefore              | -3.6206997e-09 |
| MeanKL                  | 0.0067378776   |
| MeanKLBefore            | -2.9795293e-09 |
| Step_0-AverageDiscou... | 83.3           |
| Step_0-AveragePolicyStd | 0.55644196     |
| Step_0-AverageReturn    | 133            |
| Step_0-EnvExecTime      | 21.8           |
| Step_0-MaxReturn        | 276            |
| Step_0-MinReturn        | 0.389          |
| Step_0-NumTrajs         | 1861           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 44.5           |
| Step_1-AverageDiscou... | 84.4           |
| Step_1-AveragePolicyStd | 0.5563842      |
| Step_1-AverageReturn    | 137            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 289            |
| Step_1-MinReturn        | -4.58          |
| Step_1-NumTrajs         | 1802           |
| Step_1-PolicyExecTime   | 3.57           |
| Step_1-StdReturn        | 45.5           |
| Time                    | 5.64e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 51.4           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.006859815    |
| n_timesteps             | 28480000       |
--------------------------------------------

 ---------------- Iteration 89 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 89            |
| ItrTime                 | 66.7          |
| LossAfter               | -0.0069690505 |
| LossBefore              | -2.009611e-09 |
| MeanKL                  | 0.0066424184  |
| MeanKLBefore            | 2.9798564e-09 |
| Step_0-AverageDiscou... | 82.2          |
| Step_0-AveragePolicyStd | 0.55432904    |
| Step_0-AverageReturn    | 131           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 274           |
| Step_0-MinReturn        | 5.6           |
| Step_0-NumTrajs         | 1906          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 47.1          |
| Step_1-AverageDiscou... | 82.8          |
| Step_1-AveragePolicyStd | 0.55436635    |
| Step_1-AverageReturn    | 132           |
| Step_1-EnvExecTime      | 22            |
| Step_1-MaxReturn        | 291           |
| Step_1-MinReturn        | 6.37          |
| Step_1-NumTrajs         | 1883          |
| Step_1-PolicyExecTime   | 3.54          |
| Step_1-StdReturn        | 47.9          |
| Time                    | 5.71e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 51.7          |
| Time-TotalInner         | 53.1          |
| dLoss                   | 0.0069690486  |
| n_timesteps             | 28800000      |
-------------------------------------------

 ---------------- Iteration 90 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 90             |
| ItrTime                 | 66.4           |
| LossAfter               | -0.006825535   |
| LossBefore              | -2.7935951e-09 |
| MeanKL                  | 0.00722816     |
| MeanKLBefore            | 4.4699955e-09  |
| Step_0-AverageDiscou... | 83.5           |
| Step_0-AveragePolicyStd | 0.5543386      |
| Step_0-AverageReturn    | 135            |
| Step_0-EnvExecTime      | 21.7           |
| Step_0-MaxReturn        | 309            |
| Step_0-MinReturn        | -2.23          |
| Step_0-NumTrajs         | 1797           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 46.5           |
| Step_1-AverageDiscou... | 83.6           |
| Step_1-AveragePolicyStd | 0.55430895     |
| Step_1-AverageReturn    | 136            |
| Step_1-EnvExecTime      | 22             |
| Step_1-MaxReturn        | 267            |
| Step_1-MinReturn        | 2.59           |
| Step_1-NumTrajs         | 1763           |
| Step_1-PolicyExecTime   | 3.58           |
| Step_1-StdReturn        | 47.6           |
| Time                    | 5.78e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.1            |
| Time-Sampling           | 51.5           |
| Time-TotalInner         | 52.8           |
| dLoss                   | 0.0068255323   |
| n_timesteps             | 29120000       |
--------------------------------------------

 ---------------- Iteration 91 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 91             |
| ItrTime                 | 66.4           |
| LossAfter               | -0.007030184   |
| LossBefore              | -3.6048404e-09 |
| MeanKL                  | 0.0069452277   |
| MeanKLBefore            | -5.959746e-09  |
| Step_0-AverageDiscou... | 86.1           |
| Step_0-AveragePolicyStd | 0.5498743      |
| Step_0-AverageReturn    | 139            |
| Step_0-EnvExecTime      | 21.8           |
| Step_0-MaxReturn        | 267            |
| Step_0-MinReturn        | 7.92           |
| Step_0-NumTrajs         | 1847           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 40.4           |
| Step_1-AverageDiscou... | 86.8           |
| Step_1-AveragePolicyStd | 0.549832       |
| Step_1-AverageReturn    | 142            |
| Step_1-EnvExecTime      | 21.9           |
| Step_1-MaxReturn        | 347            |
| Step_1-MinReturn        | 0.176          |
| Step_1-NumTrajs         | 1785           |
| Step_1-PolicyExecTime   | 3.58           |
| Step_1-StdReturn        | 43.3           |
| Time                    | 5.84e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 51.5           |
| Time-TotalInner         | 52.8           |
| dLoss                   | 0.00703018     |
| n_timesteps             | 29440000       |
--------------------------------------------

 ---------------- Iteration 92 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 92             |
| ItrTime                 | 66.7           |
| LossAfter               | -0.007151616   |
| LossBefore              | -5.6217706e-09 |
| MeanKL                  | 0.006714615    |
| MeanKLBefore            | 5.9589986e-09  |
| Step_0-AverageDiscou... | 85.6           |
| Step_0-AveragePolicyStd | 0.5488281      |
| Step_0-AverageReturn    | 139            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 320            |
| Step_0-MinReturn        | -3.33          |
| Step_0-NumTrajs         | 1807           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 43.5           |
| Step_1-AverageDiscou... | 86.2           |
| Step_1-AveragePolicyStd | 0.54878634     |
| Step_1-AverageReturn    | 141            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 308            |
| Step_1-MinReturn        | 9.71           |
| Step_1-NumTrajs         | 1766           |
| Step_1-PolicyExecTime   | 3.61           |
| Step_1-StdReturn        | 44.1           |
| Time                    | 5.91e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.11           |
| Time-Sampling           | 51.8           |
| Time-TotalInner         | 53.1           |
| dLoss                   | 0.00715161     |
| n_timesteps             | 29760000       |
--------------------------------------------

 ---------------- Iteration 93 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 93             |
| ItrTime                 | 66.9           |
| LossAfter               | -0.006667105   |
| LossBefore              | 1.3530745e-09  |
| MeanKL                  | 0.0066463486   |
| MeanKLBefore            | -1.4893333e-09 |
| Step_0-AverageDiscou... | 85.2           |
| Step_0-AveragePolicyStd | 0.5476806      |
| Step_0-AverageReturn    | 136            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 285            |
| Step_0-MinReturn        | -2.42          |
| Step_0-NumTrajs         | 1885           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 42.3           |
| Step_1-AverageDiscou... | 86.3           |
| Step_1-AveragePolicyStd | 0.54770726     |
| Step_1-AverageReturn    | 140            |
| Step_1-EnvExecTime      | 22             |
| Step_1-MaxReturn        | 295            |
| Step_1-MinReturn        | 3.23           |
| Step_1-NumTrajs         | 1834           |
| Step_1-PolicyExecTime   | 3.61           |
| Step_1-StdReturn        | 42             |
| Time                    | 5.98e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.13           |
| Time-Sampling           | 51.8           |
| Time-TotalInner         | 53.2           |
| dLoss                   | 0.0066671064   |
| n_timesteps             | 30080000       |
--------------------------------------------

 ---------------- Iteration 94 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 94             |
| ItrTime                 | 66.1           |
| LossAfter               | -0.006717892   |
| LossBefore              | -3.3169023e-10 |
| MeanKL                  | 0.007038798    |
| MeanKLBefore            | -7.4495303e-09 |
| Step_0-AverageDiscou... | 84.8           |
| Step_0-AveragePolicyStd | 0.54881364     |
| Step_0-AverageReturn    | 139            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 277            |
| Step_0-MinReturn        | 8              |
| Step_0-NumTrajs         | 1774           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 45             |
| Step_1-AverageDiscou... | 85.7           |
| Step_1-AveragePolicyStd | 0.5487386      |
| Step_1-AverageReturn    | 142            |
| Step_1-EnvExecTime      | 21.5           |
| Step_1-MaxReturn        | 308            |
| Step_1-MinReturn        | 1.73           |
| Step_1-NumTrajs         | 1737           |
| Step_1-PolicyExecTime   | 3.57           |
| Step_1-StdReturn        | 46.7           |
| Time                    | 6.04e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.09           |
| Time-Sampling           | 51.2           |
| Time-TotalInner         | 52.5           |
| dLoss                   | 0.0067178914   |
| n_timesteps             | 30400000       |
--------------------------------------------

 ---------------- Iteration 95 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 95             |
| ItrTime                 | 65.9           |
| LossAfter               | -0.0070220456  |
| LossBefore              | 4.6303645e-09  |
| MeanKL                  | 0.006632951    |
| MeanKLBefore            | -1.4904351e-09 |
| Step_0-AverageDiscou... | 86.6           |
| Step_0-AveragePolicyStd | 0.54968363     |
| Step_0-AverageReturn    | 139            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 277            |
| Step_0-MinReturn        | 1.05           |
| Step_0-NumTrajs         | 1886           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 39.4           |
| Step_1-AverageDiscou... | 87.2           |
| Step_1-AveragePolicyStd | 0.54962236     |
| Step_1-AverageReturn    | 142            |
| Step_1-EnvExecTime      | 21.6           |
| Step_1-MaxReturn        | 306            |
| Step_1-MinReturn        | 4.43           |
| Step_1-NumTrajs         | 1834           |
| Step_1-PolicyExecTime   | 3.59           |
| Step_1-StdReturn        | 41.9           |
| Time                    | 6.11e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 51             |
| Time-TotalInner         | 52.3           |
| dLoss                   | 0.00702205     |
| n_timesteps             | 30720000       |
--------------------------------------------

 ---------------- Iteration 96 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 96             |
| ItrTime                 | 67.3           |
| LossAfter               | -0.0064208447  |
| LossBefore              | -3.4933905e-09 |
| MeanKL                  | 0.00642882     |
| MeanKLBefore            | 4.4696176e-09  |
| Step_0-AverageDiscou... | 84             |
| Step_0-AveragePolicyStd | 0.5484619      |
| Step_0-AverageReturn    | 135            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 291            |
| Step_0-MinReturn        | 8.64           |
| Step_0-NumTrajs         | 1856           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 46             |
| Step_1-AverageDiscou... | 85             |
| Step_1-AveragePolicyStd | 0.54853046     |
| Step_1-AverageReturn    | 139            |
| Step_1-EnvExecTime      | 22.2           |
| Step_1-MaxReturn        | 298            |
| Step_1-MinReturn        | 3.67           |
| Step_1-NumTrajs         | 1779           |
| Step_1-PolicyExecTime   | 3.55           |
| Step_1-StdReturn        | 47.9           |
| Time                    | 6.18e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.11           |
| Time-Sampling           | 52.3           |
| Time-TotalInner         | 53.7           |
| dLoss                   | 0.006420841    |
| n_timesteps             | 31040000       |
--------------------------------------------

 ---------------- Iteration 97 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 97            |
| ItrTime                 | 65.5          |
| LossAfter               | -0.0067962212 |
| LossBefore              | 1.3015804e-09 |
| MeanKL                  | 0.006809977   |
| MeanKLBefore            | 1.4901703e-09 |
| Step_0-AverageDiscou... | 88.5          |
| Step_0-AveragePolicyStd | 0.55031365    |
| Step_0-AverageReturn    | 141           |
| Step_0-EnvExecTime      | 21.3          |
| Step_0-MaxReturn        | 272           |
| Step_0-MinReturn        | 1.99          |
| Step_0-NumTrajs         | 1948          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 35.5          |
| Step_1-AverageDiscou... | 88.7          |
| Step_1-AveragePolicyStd | 0.5503197     |
| Step_1-AverageReturn    | 143           |
| Step_1-EnvExecTime      | 21.6          |
| Step_1-MaxReturn        | 309           |
| Step_1-MinReturn        | -6.9          |
| Step_1-NumTrajs         | 1878          |
| Step_1-PolicyExecTime   | 3.55          |
| Step_1-StdReturn        | 37.7          |
| Time                    | 6.24e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 50.6          |
| Time-TotalInner         | 51.9          |
| dLoss                   | 0.0067962226  |
| n_timesteps             | 31360000      |
-------------------------------------------

 ---------------- Iteration 98 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 98            |
| ItrTime                 | 66            |
| LossAfter               | -0.006755984  |
| LossBefore              | 1.1574123e-09 |
| MeanKL                  | 0.0065368963  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 86            |
| Step_0-AveragePolicyStd | 0.5476395     |
| Step_0-AverageReturn    | 139           |
| Step_0-EnvExecTime      | 21.7          |
| Step_0-MaxReturn        | 313           |
| Step_0-MinReturn        | 9.41          |
| Step_0-NumTrajs         | 1834          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 46.4          |
| Step_1-AverageDiscou... | 87.3          |
| Step_1-AveragePolicyStd | 0.54771906    |
| Step_1-AverageReturn    | 144           |
| Step_1-EnvExecTime      | 21.6          |
| Step_1-MaxReturn        | 306           |
| Step_1-MinReturn        | 8             |
| Step_1-NumTrajs         | 1778          |
| Step_1-PolicyExecTime   | 3.6           |
| Step_1-StdReturn        | 46.8          |
| Time                    | 6.31e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.1           |
| Time-Sampling           | 51.1          |
| Time-TotalInner         | 52.4          |
| dLoss                   | 0.006755985   |
| n_timesteps             | 31680000      |
-------------------------------------------

 ---------------- Iteration 99 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 99             |
| ItrTime                 | 66             |
| LossAfter               | -0.0069744587  |
| LossBefore              | -1.1629633e-09 |
| MeanKL                  | 0.0070667653   |
| MeanKLBefore            | -2.9802656e-09 |
| Step_0-AverageDiscou... | 86.1           |
| Step_0-AveragePolicyStd | 0.5488955      |
| Step_0-AverageReturn    | 137            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 297            |
| Step_0-MinReturn        | 11             |
| Step_0-NumTrajs         | 1860           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 41.9           |
| Step_1-AverageDiscou... | 86.8           |
| Step_1-AveragePolicyStd | 0.5488748      |
| Step_1-AverageReturn    | 140            |
| Step_1-EnvExecTime      | 21.4           |
| Step_1-MaxReturn        | 322            |
| Step_1-MinReturn        | -1.94          |
| Step_1-NumTrajs         | 1828           |
| Step_1-PolicyExecTime   | 3.54           |
| Step_1-StdReturn        | 44.4           |
| Time                    | 6.37e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.13           |
| Time-Sampling           | 51.1           |
| Time-TotalInner         | 52.4           |
| dLoss                   | 0.0069744578   |
| n_timesteps             | 32000000       |
--------------------------------------------

 ---------------- Iteration 100 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 100            |
| ItrTime                 | 66             |
| LossAfter               | -0.006607397   |
| LossBefore              | -1.3347775e-09 |
| MeanKL                  | 0.007065936    |
| MeanKLBefore            | 1.4901161e-09  |
| Step_0-AverageDiscou... | 86.4           |
| Step_0-AveragePolicyStd | 0.547582       |
| Step_0-AverageReturn    | 141            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 279            |
| Step_0-MinReturn        | 5.12           |
| Step_0-NumTrajs         | 1839           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 44.2           |
| Step_1-AverageDiscou... | 87.2           |
| Step_1-AveragePolicyStd | 0.54755956     |
| Step_1-AverageReturn    | 145            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 282            |
| Step_1-MinReturn        | 5.62           |
| Step_1-NumTrajs         | 1736           |
| Step_1-PolicyExecTime   | 3.59           |
| Step_1-StdReturn        | 47.7           |
| Time                    | 6.44e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.11           |
| Time-Sampling           | 51.1           |
| Time-TotalInner         | 52.4           |
| dLoss                   | 0.0066073956   |
| n_timesteps             | 32320000       |
--------------------------------------------

 ---------------- Iteration 101 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 101           |
| ItrTime                 | 65.9          |
| LossAfter               | -0.0066102883 |
| LossBefore              | 2.3698376e-12 |
| MeanKL                  | 0.0068008685  |
| MeanKLBefore            | -7.44975e-09  |
| Step_0-AverageDiscou... | 88.8          |
| Step_0-AveragePolicyStd | 0.54568094    |
| Step_0-AverageReturn    | 146           |
| Step_0-EnvExecTime      | 21.5          |
| Step_0-MaxReturn        | 298           |
| Step_0-MinReturn        | -0.473        |
| Step_0-NumTrajs         | 1798          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 42            |
| Step_1-AverageDiscou... | 89            |
| Step_1-AveragePolicyStd | 0.54564744    |
| Step_1-AverageReturn    | 149           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 314           |
| Step_1-MinReturn        | 10.4          |
| Step_1-NumTrajs         | 1726          |
| Step_1-PolicyExecTime   | 3.64          |
| Step_1-StdReturn        | 46.1          |
| Time                    | 6.51e+03      |
| Time-InnerStep          | 0.157         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.09          |
| Time-Sampling           | 51            |
| Time-TotalInner         | 52.3          |
| dLoss                   | 0.0066102883  |
| n_timesteps             | 32640000      |
-------------------------------------------

 ---------------- Iteration 102 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 102           |
| ItrTime                 | 67.2          |
| LossAfter               | -0.0069190515 |
| LossBefore              | 8.7070473e-10 |
| MeanKL                  | 0.0065793246  |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 85.7          |
| Step_0-AveragePolicyStd | 0.5431242     |
| Step_0-AverageReturn    | 139           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 352           |
| Step_0-MinReturn        | 0.953         |
| Step_0-NumTrajs         | 1824          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 46.4          |
| Step_1-AverageDiscou... | 87            |
| Step_1-AveragePolicyStd | 0.5431368     |
| Step_1-AverageReturn    | 144           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 292           |
| Step_1-MinReturn        | 6.61          |
| Step_1-NumTrajs         | 1763          |
| Step_1-PolicyExecTime   | 3.64          |
| Step_1-StdReturn        | 47.1          |
| Time                    | 6.57e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.1           |
| Time-Sampling           | 52.3          |
| Time-TotalInner         | 53.6          |
| dLoss                   | 0.0069190525  |
| n_timesteps             | 32960000      |
-------------------------------------------

 ---------------- Iteration 103 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 103            |
| ItrTime                 | 67.3           |
| LossAfter               | -0.0066815554  |
| LossBefore              | 3.3034382e-09  |
| MeanKL                  | 0.006718824    |
| MeanKLBefore            | -1.9984014e-14 |
| Step_0-AverageDiscou... | 86.7           |
| Step_0-AveragePolicyStd | 0.5440232      |
| Step_0-AverageReturn    | 143            |
| Step_0-EnvExecTime      | 22.2           |
| Step_0-MaxReturn        | 359            |
| Step_0-MinReturn        | 4.22           |
| Step_0-NumTrajs         | 1741           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 46.5           |
| Step_1-AverageDiscou... | 87.4           |
| Step_1-AveragePolicyStd | 0.54405046     |
| Step_1-AverageReturn    | 145            |
| Step_1-EnvExecTime      | 22.2           |
| Step_1-MaxReturn        | 296            |
| Step_1-MinReturn        | -4.23          |
| Step_1-NumTrajs         | 1722           |
| Step_1-PolicyExecTime   | 3.67           |
| Step_1-StdReturn        | 46             |
| Time                    | 6.64e+03       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.21           |
| Time-Sampling           | 52.3           |
| Time-TotalInner         | 53.7           |
| dLoss                   | 0.0066815587   |
| n_timesteps             | 33280000       |
--------------------------------------------

 ---------------- Iteration 104 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 104           |
| ItrTime                 | 66.1          |
| LossAfter               | -0.0067534586 |
| LossBefore              | 3.656421e-09  |
| MeanKL                  | 0.007779437   |
| MeanKLBefore            | -8.938361e-09 |
| Step_0-AverageDiscou... | 87.6          |
| Step_0-AveragePolicyStd | 0.5426682     |
| Step_0-AverageReturn    | 143           |
| Step_0-EnvExecTime      | 21.6          |
| Step_0-MaxReturn        | 276           |
| Step_0-MinReturn        | 9.39          |
| Step_0-NumTrajs         | 1823          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 41.8          |
| Step_1-AverageDiscou... | 88.1          |
| Step_1-AveragePolicyStd | 0.5425865     |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 280           |
| Step_1-MinReturn        | 7.76          |
| Step_1-NumTrajs         | 1762          |
| Step_1-PolicyExecTime   | 3.58          |
| Step_1-StdReturn        | 44.6          |
| Time                    | 6.71e+03      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.13          |
| Time-Sampling           | 51.1          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.0067534624  |
| n_timesteps             | 33600000      |
-------------------------------------------

 ---------------- Iteration 105 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 105           |
| ItrTime                 | 65.8          |
| LossAfter               | -0.0065491586 |
| LossBefore              | 1.3561212e-09 |
| MeanKL                  | 0.006786925   |
| MeanKLBefore            | 5.9590093e-09 |
| Step_0-AverageDiscou... | 89.3          |
| Step_0-AveragePolicyStd | 0.54057056    |
| Step_0-AverageReturn    | 143           |
| Step_0-EnvExecTime      | 21.4          |
| Step_0-MaxReturn        | 286           |
| Step_0-MinReturn        | 4.2           |
| Step_0-NumTrajs         | 1920          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 38.5          |
| Step_1-AverageDiscou... | 90.2          |
| Step_1-AveragePolicyStd | 0.5405292     |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 329           |
| Step_1-MinReturn        | 5.3           |
| Step_1-NumTrajs         | 1862          |
| Step_1-PolicyExecTime   | 3.59          |
| Step_1-StdReturn        | 40.8          |
| Time                    | 6.77e+03      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.14          |
| Time-Sampling           | 50.9          |
| Time-TotalInner         | 52.2          |
| dLoss                   | 0.00654916    |
| n_timesteps             | 33920000      |
-------------------------------------------

 ---------------- Iteration 106 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 106            |
| ItrTime                 | 65.5           |
| LossAfter               | -0.0069252267  |
| LossBefore              | -2.4015248e-09 |
| MeanKL                  | 0.0071926876   |
| MeanKLBefore            | -5.9597944e-09 |
| Step_0-AverageDiscou... | 88.8           |
| Step_0-AveragePolicyStd | 0.54151404     |
| Step_0-AverageReturn    | 142            |
| Step_0-EnvExecTime      | 21.4           |
| Step_0-MaxReturn        | 302            |
| Step_0-MinReturn        | -0.504         |
| Step_0-NumTrajs         | 1909           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 38.6           |
| Step_1-AverageDiscou... | 89.6           |
| Step_1-AveragePolicyStd | 0.54138446     |
| Step_1-AverageReturn    | 146            |
| Step_1-EnvExecTime      | 21.4           |
| Step_1-MaxReturn        | 303            |
| Step_1-MinReturn        | 0.367          |
| Step_1-NumTrajs         | 1834           |
| Step_1-PolicyExecTime   | 3.56           |
| Step_1-StdReturn        | 42.7           |
| Time                    | 6.84e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.15           |
| Time-Sampling           | 50.6           |
| Time-TotalInner         | 51.9           |
| dLoss                   | 0.0069252243   |
| n_timesteps             | 34240000       |
--------------------------------------------

 ---------------- Iteration 107 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 107           |
| ItrTime                 | 67            |
| LossAfter               | -0.0068490067 |
| LossBefore              | 3.1955676e-09 |
| MeanKL                  | 0.006853518   |
| MeanKLBefore            | 5.9604646e-09 |
| Step_0-AverageDiscou... | 86.3          |
| Step_0-AveragePolicyStd | 0.536719      |
| Step_0-AverageReturn    | 141           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 300           |
| Step_0-MinReturn        | 6.33          |
| Step_0-NumTrajs         | 1772          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 48.7          |
| Step_1-AverageDiscou... | 87.2          |
| Step_1-AveragePolicyStd | 0.5366927     |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 311           |
| Step_1-MinReturn        | 3.86          |
| Step_1-NumTrajs         | 1693          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 51.1          |
| Time                    | 6.9e+03       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 52.1          |
| Time-TotalInner         | 53.3          |
| dLoss                   | 0.00684901    |
| n_timesteps             | 34560000      |
-------------------------------------------

 ---------------- Iteration 108 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 108           |
| ItrTime                 | 66.1          |
| LossAfter               | -0.007343362  |
| LossBefore              | 2.7362532e-09 |
| MeanKL                  | 0.006919773   |
| MeanKLBefore            | 1.4897409e-09 |
| Step_0-AverageDiscou... | 88.2          |
| Step_0-AveragePolicyStd | 0.5364612     |
| Step_0-AverageReturn    | 145           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 310           |
| Step_0-MinReturn        | 6.91          |
| Step_0-NumTrajs         | 1769          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 43.5          |
| Step_1-AverageDiscou... | 88.2          |
| Step_1-AveragePolicyStd | 0.5364379     |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 21.4          |
| Step_1-MaxReturn        | 328           |
| Step_1-MinReturn        | 3.3           |
| Step_1-NumTrajs         | 1728          |
| Step_1-PolicyExecTime   | 3.59          |
| Step_1-StdReturn        | 45.2          |
| Time                    | 6.97e+03      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.12          |
| Time-Sampling           | 51.2          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.007343365   |
| n_timesteps             | 34880000      |
-------------------------------------------

 ---------------- Iteration 109 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 109            |
| ItrTime                 | 66             |
| LossAfter               | -0.0071741184  |
| LossBefore              | -2.9028602e-09 |
| MeanKL                  | 0.00688262     |
| MeanKLBefore            | 1.06847867e-13 |
| Step_0-AverageDiscou... | 90.7           |
| Step_0-AveragePolicyStd | 0.53312427     |
| Step_0-AverageReturn    | 148            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 298            |
| Step_0-MinReturn        | 0.216          |
| Step_0-NumTrajs         | 1828           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 40             |
| Step_1-AverageDiscou... | 89.6           |
| Step_1-AveragePolicyStd | 0.5330336      |
| Step_1-AverageReturn    | 149            |
| Step_1-EnvExecTime      | 21.6           |
| Step_1-MaxReturn        | 299            |
| Step_1-MinReturn        | 12.3           |
| Step_1-NumTrajs         | 1756           |
| Step_1-PolicyExecTime   | 3.61           |
| Step_1-StdReturn        | 44.4           |
| Time                    | 7.04e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 51             |
| Time-TotalInner         | 52.4           |
| dLoss                   | 0.0071741156   |
| n_timesteps             | 35200000       |
--------------------------------------------

 ---------------- Iteration 110 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 110           |
| ItrTime                 | 66.5          |
| LossAfter               | -0.006640193  |
| LossBefore              | 2.3186528e-09 |
| MeanKL                  | 0.0067765056  |
| MeanKLBefore            | 2.9798706e-09 |
| Step_0-AverageDiscou... | 88            |
| Step_0-AveragePolicyStd | 0.53334045    |
| Step_0-AverageReturn    | 141           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 298           |
| Step_0-MinReturn        | 3.06          |
| Step_0-NumTrajs         | 1910          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 43.1          |
| Step_1-AverageDiscou... | 90.1          |
| Step_1-AveragePolicyStd | 0.5332503     |
| Step_1-AverageReturn    | 148           |
| Step_1-EnvExecTime      | 21.8          |
| Step_1-MaxReturn        | 309           |
| Step_1-MinReturn        | 0.511         |
| Step_1-NumTrajs         | 1817          |
| Step_1-PolicyExecTime   | 3.6           |
| Step_1-StdReturn        | 44.3          |
| Time                    | 7.1e+03       |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 51.5          |
| Time-TotalInner         | 52.9          |
| dLoss                   | 0.0066401954  |
| n_timesteps             | 35520000      |
-------------------------------------------

 ---------------- Iteration 111 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 111           |
| ItrTime                 | 67.1          |
| LossAfter               | -0.006515015  |
| LossBefore              | 3.4962604e-09 |
| MeanKL                  | 0.0065552867  |
| MeanKLBefore            | -4.973799e-15 |
| Step_0-AverageDiscou... | 84.7          |
| Step_0-AveragePolicyStd | 0.5316272     |
| Step_0-AverageReturn    | 139           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 281           |
| Step_0-MinReturn        | 1.72          |
| Step_0-NumTrajs         | 1735          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 49.6          |
| Step_1-AverageDiscou... | 85.4          |
| Step_1-AveragePolicyStd | 0.53154975    |
| Step_1-AverageReturn    | 144           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 444           |
| Step_1-MinReturn        | 1.62          |
| Step_1-NumTrajs         | 1645          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 52.5          |
| Time                    | 7.17e+03      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.06          |
| Time-Sampling           | 52.2          |
| Time-TotalInner         | 53.5          |
| dLoss                   | 0.0065150186  |
| n_timesteps             | 35840000      |
-------------------------------------------

 ---------------- Iteration 112 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 112           |
| ItrTime                 | 66.9          |
| LossAfter               | -0.0065667825 |
| LossBefore              | -3.555126e-10 |
| MeanKL                  | 0.00649218    |
| MeanKLBefore            | 2.9798626e-09 |
| Step_0-AverageDiscou... | 91.2          |
| Step_0-AveragePolicyStd | 0.5311931     |
| Step_0-AverageReturn    | 153           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 280           |
| Step_0-MinReturn        | 5.96          |
| Step_0-NumTrajs         | 1729          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 40.6          |
| Step_1-AverageDiscou... | 91.5          |
| Step_1-AveragePolicyStd | 0.5312413     |
| Step_1-AverageReturn    | 156           |
| Step_1-EnvExecTime      | 22.1          |
| Step_1-MaxReturn        | 355           |
| Step_1-MinReturn        | 8.31          |
| Step_1-NumTrajs         | 1673          |
| Step_1-PolicyExecTime   | 3.64          |
| Step_1-StdReturn        | 45            |
| Time                    | 7.24e+03      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 52            |
| Time-TotalInner         | 53.3          |
| dLoss                   | 0.006566782   |
| n_timesteps             | 36160000      |
-------------------------------------------

 ---------------- Iteration 113 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 113           |
| ItrTime                 | 66.4          |
| LossAfter               | -0.0066898814 |
| LossBefore              | -5.382309e-09 |
| MeanKL                  | 0.0068969494  |
| MeanKLBefore            | 1.4901351e-09 |
| Step_0-AverageDiscou... | 90.6          |
| Step_0-AveragePolicyStd | 0.53130245    |
| Step_0-AverageReturn    | 149           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 288           |
| Step_0-MinReturn        | 7.87          |
| Step_0-NumTrajs         | 1800          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 41.9          |
| Step_1-AverageDiscou... | 89.5          |
| Step_1-AveragePolicyStd | 0.5312747     |
| Step_1-AverageReturn    | 151           |
| Step_1-EnvExecTime      | 21.8          |
| Step_1-MaxReturn        | 327           |
| Step_1-MinReturn        | 0.147         |
| Step_1-NumTrajs         | 1705          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 49.3          |
| Time                    | 7.3e+03       |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.12          |
| Time-Sampling           | 51.5          |
| Time-TotalInner         | 52.8          |
| dLoss                   | 0.006689876   |
| n_timesteps             | 36480000      |
-------------------------------------------

 ---------------- Iteration 114 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 114           |
| ItrTime                 | 67.4          |
| LossAfter               | -0.0068130516 |
| LossBefore              | 7.043129e-09  |
| MeanKL                  | 0.007213396   |
| MeanKLBefore            | -1.489331e-09 |
| Step_0-AverageDiscou... | 88.1          |
| Step_0-AveragePolicyStd | 0.5314797     |
| Step_0-AverageReturn    | 145           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 303           |
| Step_0-MinReturn        | -3.89         |
| Step_0-NumTrajs         | 1768          |
| Step_0-PolicyExecTime   | 1.56          |
| Step_0-StdReturn        | 47.8          |
| Step_1-AverageDiscou... | 87.2          |
| Step_1-AveragePolicyStd | 0.53144133    |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 326           |
| Step_1-MinReturn        | -1.99         |
| Step_1-NumTrajs         | 1689          |
| Step_1-PolicyExecTime   | 3.63          |
| Step_1-StdReturn        | 53.5          |
| Time                    | 7.37e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 52.5          |
| Time-TotalInner         | 53.8          |
| dLoss                   | 0.0068130586  |
| n_timesteps             | 36800000      |
-------------------------------------------

 ---------------- Iteration 115 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 115           |
| ItrTime                 | 66.7          |
| LossAfter               | -0.0065984996 |
| LossBefore              | 1.7101458e-10 |
| MeanKL                  | 0.0066451468  |
| MeanKLBefore            | 6.705747e-14  |
| Step_0-AverageDiscou... | 90.2          |
| Step_0-AveragePolicyStd | 0.52762675    |
| Step_0-AverageReturn    | 148           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 310           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1807          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 41.3          |
| Step_1-AverageDiscou... | 90.6          |
| Step_1-AveragePolicyStd | 0.52755904    |
| Step_1-AverageReturn    | 151           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 318           |
| Step_1-MinReturn        | 9.88          |
| Step_1-NumTrajs         | 1732          |
| Step_1-PolicyExecTime   | 3.63          |
| Step_1-StdReturn        | 44.5          |
| Time                    | 7.44e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.12          |
| Time-Sampling           | 51.8          |
| Time-TotalInner         | 53.1          |
| dLoss                   | 0.0065984996  |
| n_timesteps             | 37120000      |
-------------------------------------------

 ---------------- Iteration 116 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 116           |
| ItrTime                 | 66.6          |
| LossAfter               | -0.0070012854 |
| LossBefore              | 5.7211985e-10 |
| MeanKL                  | 0.006881416   |
| MeanKLBefore            | 3.6965987e-13 |
| Step_0-AverageDiscou... | 87.9          |
| Step_0-AveragePolicyStd | 0.5259767     |
| Step_0-AverageReturn    | 147           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 288           |
| Step_0-MinReturn        | 0.143         |
| Step_0-NumTrajs         | 1715          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 47            |
| Step_1-AverageDiscou... | 89.6          |
| Step_1-AveragePolicyStd | 0.52596045    |
| Step_1-AverageReturn    | 151           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 297           |
| Step_1-MinReturn        | 9.34          |
| Step_1-NumTrajs         | 1703          |
| Step_1-PolicyExecTime   | 3.61          |
| Step_1-StdReturn        | 45            |
| Time                    | 7.5e+03       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 51.7          |
| Time-TotalInner         | 53            |
| dLoss                   | 0.007001286   |
| n_timesteps             | 37440000      |
-------------------------------------------

 ---------------- Iteration 117 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 117           |
| ItrTime                 | 65.2          |
| LossAfter               | -0.006919989  |
| LossBefore              | 1.6848801e-09 |
| MeanKL                  | 0.0067330613  |
| MeanKLBefore            | 1.4901161e-09 |
| Step_0-AverageDiscou... | 88.9          |
| Step_0-AveragePolicyStd | 0.5229928     |
| Step_0-AverageReturn    | 142           |
| Step_0-EnvExecTime      | 21.3          |
| Step_0-MaxReturn        | 282           |
| Step_0-MinReturn        | 2.58          |
| Step_0-NumTrajs         | 1892          |
| Step_0-PolicyExecTime   | 1.14          |
| Step_0-StdReturn        | 42.5          |
| Step_1-AverageDiscou... | 90.2          |
| Step_1-AveragePolicyStd | 0.52300704    |
| Step_1-AverageReturn    | 148           |
| Step_1-EnvExecTime      | 21.2          |
| Step_1-MaxReturn        | 330           |
| Step_1-MinReturn        | 8.34          |
| Step_1-NumTrajs         | 1812          |
| Step_1-PolicyExecTime   | 3.59          |
| Step_1-StdReturn        | 45.2          |
| Time                    | 7.57e+03      |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.13          |
| Time-Sampling           | 50.3          |
| Time-TotalInner         | 51.6          |
| dLoss                   | 0.0069199908  |
| n_timesteps             | 37760000      |
-------------------------------------------

 ---------------- Iteration 118 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 118           |
| ItrTime                 | 66.4          |
| LossAfter               | -0.006842293  |
| LossBefore              | 7.295428e-09  |
| MeanKL                  | 0.00787355    |
| MeanKLBefore            | 2.9794631e-09 |
| Step_0-AverageDiscou... | 89.1          |
| Step_0-AveragePolicyStd | 0.523586      |
| Step_0-AverageReturn    | 148           |
| Step_0-EnvExecTime      | 21.8          |
| Step_0-MaxReturn        | 316           |
| Step_0-MinReturn        | -20.2         |
| Step_0-NumTrajs         | 1752          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 49.8          |
| Step_1-AverageDiscou... | 90.1          |
| Step_1-AveragePolicyStd | 0.52351165    |
| Step_1-AverageReturn    | 153           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 350           |
| Step_1-MinReturn        | 6.99          |
| Step_1-NumTrajs         | 1665          |
| Step_1-PolicyExecTime   | 3.66          |
| Step_1-StdReturn        | 52.9          |
| Time                    | 7.64e+03      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.1           |
| Time-Sampling           | 51.4          |
| Time-TotalInner         | 52.7          |
| dLoss                   | 0.0068423003  |
| n_timesteps             | 38080000      |
-------------------------------------------

 ---------------- Iteration 119 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 119           |
| ItrTime                 | 66.1          |
| LossAfter               | -0.0063549215 |
| LossBefore              | 2.431628e-09  |
| MeanKL                  | 0.0067199715  |
| MeanKLBefore            | -7.449883e-09 |
| Step_0-AverageDiscou... | 89            |
| Step_0-AveragePolicyStd | 0.52002907    |
| Step_0-AverageReturn    | 147           |
| Step_0-EnvExecTime      | 21.6          |
| Step_0-MaxReturn        | 293           |
| Step_0-MinReturn        | 3.44          |
| Step_0-NumTrajs         | 1755          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 48            |
| Step_1-AverageDiscou... | 90.2          |
| Step_1-AveragePolicyStd | 0.5200938     |
| Step_1-AverageReturn    | 153           |
| Step_1-EnvExecTime      | 21.7          |
| Step_1-MaxReturn        | 317           |
| Step_1-MinReturn        | 5.01          |
| Step_1-NumTrajs         | 1674          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 50.6          |
| Time                    | 7.7e+03       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.07          |
| Time-Sampling           | 51.2          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.006354924   |
| n_timesteps             | 38400000      |
-------------------------------------------

 ---------------- Iteration 120 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 120            |
| ItrTime                 | 67             |
| LossAfter               | -0.0069794008  |
| LossBefore              | 5.032276e-09   |
| MeanKL                  | 0.0067301877   |
| MeanKLBefore            | -1.4908149e-09 |
| Step_0-AverageDiscou... | 87.4           |
| Step_0-AveragePolicyStd | 0.5178831      |
| Step_0-AverageReturn    | 146            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 309            |
| Step_0-MinReturn        | 9.79           |
| Step_0-NumTrajs         | 1700           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 51.9           |
| Step_1-AverageDiscou... | 88.9           |
| Step_1-AveragePolicyStd | 0.5179797      |
| Step_1-AverageReturn    | 151            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 353            |
| Step_1-MinReturn        | 8.41           |
| Step_1-NumTrajs         | 1640           |
| Step_1-PolicyExecTime   | 3.64           |
| Step_1-StdReturn        | 51.3           |
| Time                    | 7.77e+03       |
| Time-InnerStep          | 0.176          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.12           |
| Time-Sampling           | 52             |
| Time-TotalInner         | 53.3           |
| dLoss                   | 0.006979406    |
| n_timesteps             | 38720000       |
--------------------------------------------

 ---------------- Iteration 121 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 121            |
| ItrTime                 | 66.6           |
| LossAfter               | -0.006874627   |
| LossBefore              | -4.2027376e-10 |
| MeanKL                  | 0.006856649    |
| MeanKLBefore            | 4.4699418e-09  |
| Step_0-AverageDiscou... | 92.1           |
| Step_0-AveragePolicyStd | 0.5168248      |
| Step_0-AverageReturn    | 152            |
| Step_0-EnvExecTime      | 22.1           |
| Step_0-MaxReturn        | 302            |
| Step_0-MinReturn        | -0.492         |
| Step_0-NumTrajs         | 1792           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 41.8           |
| Step_1-AverageDiscou... | 93.3           |
| Step_1-AveragePolicyStd | 0.5167515      |
| Step_1-AverageReturn    | 157            |
| Step_1-EnvExecTime      | 21.7           |
| Step_1-MaxReturn        | 358            |
| Step_1-MinReturn        | 9.73           |
| Step_1-NumTrajs         | 1741           |
| Step_1-PolicyExecTime   | 3.62           |
| Step_1-StdReturn        | 44.3           |
| Time                    | 7.84e+03       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.1            |
| Time-Sampling           | 51.7           |
| Time-TotalInner         | 53             |
| dLoss                   | 0.0068746265   |
| n_timesteps             | 39040000       |
--------------------------------------------

 ---------------- Iteration 122 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 122           |
| ItrTime                 | 66.6          |
| LossAfter               | -0.006521585  |
| LossBefore              | 2.7546079e-09 |
| MeanKL                  | 0.0066396976  |
| MeanKLBefore            | 4.4703823e-09 |
| Step_0-AverageDiscou... | 89.4          |
| Step_0-AveragePolicyStd | 0.5179201     |
| Step_0-AverageReturn    | 147           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 341           |
| Step_0-MinReturn        | 9.92          |
| Step_0-NumTrajs         | 1753          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 49.6          |
| Step_1-AverageDiscou... | 90.1          |
| Step_1-AveragePolicyStd | 0.51798683    |
| Step_1-AverageReturn    | 150           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 329           |
| Step_1-MinReturn        | 5.53          |
| Step_1-NumTrajs         | 1734          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 51.4          |
| Time                    | 7.9e+03       |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.13          |
| Time-Sampling           | 51.7          |
| Time-TotalInner         | 53.1          |
| dLoss                   | 0.0065215877  |
| n_timesteps             | 39360000      |
-------------------------------------------

 ---------------- Iteration 123 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 123           |
| ItrTime                 | 67.3          |
| LossAfter               | -0.006900318  |
| LossBefore              | -6.944848e-10 |
| MeanKL                  | 0.0071642585  |
| MeanKLBefore            | 2.9798444e-09 |
| Step_0-AverageDiscou... | 82.6          |
| Step_0-AveragePolicyStd | 0.5187708     |
| Step_0-AverageReturn    | 140           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 331           |
| Step_0-MinReturn        | -0.297        |
| Step_0-NumTrajs         | 1644          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 63.9          |
| Step_1-AverageDiscou... | 84.7          |
| Step_1-AveragePolicyStd | 0.51870435    |
| Step_1-AverageReturn    | 146           |
| Step_1-EnvExecTime      | 22.3          |
| Step_1-MaxReturn        | 300           |
| Step_1-MinReturn        | 4.17          |
| Step_1-NumTrajs         | 1617          |
| Step_1-PolicyExecTime   | 3.7           |
| Step_1-StdReturn        | 63.2          |
| Time                    | 7.97e+03      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 52.4          |
| Time-TotalInner         | 53.7          |
| dLoss                   | 0.0069003175  |
| n_timesteps             | 39680000      |
-------------------------------------------

 ---------------- Iteration 124 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 124            |
| ItrTime                 | 66.4           |
| LossAfter               | -0.0071748435  |
| LossBefore              | 8.9216423e-10  |
| MeanKL                  | 0.006639102    |
| MeanKLBefore            | -1.4897626e-09 |
| Step_0-AverageDiscou... | 93.6           |
| Step_0-AveragePolicyStd | 0.5165233      |
| Step_0-AverageReturn    | 154            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 313            |
| Step_0-MinReturn        | 6.42           |
| Step_0-NumTrajs         | 1822           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 40.6           |
| Step_1-AverageDiscou... | 93.9           |
| Step_1-AveragePolicyStd | 0.5164431      |
| Step_1-AverageReturn    | 158            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 336            |
| Step_1-MinReturn        | -0.252         |
| Step_1-NumTrajs         | 1715           |
| Step_1-PolicyExecTime   | 3.62           |
| Step_1-StdReturn        | 45.4           |
| Time                    | 8.04e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.09           |
| Time-Sampling           | 51.5           |
| Time-TotalInner         | 52.8           |
| dLoss                   | 0.0071748444   |
| n_timesteps             | 40000000       |
--------------------------------------------

 ---------------- Iteration 125 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 125           |
| ItrTime                 | 68            |
| LossAfter               | -0.0065502254 |
| LossBefore              | 1.8132745e-09 |
| MeanKL                  | 0.00687626    |
| MeanKLBefore            | 4.468694e-09  |
| Step_0-AverageDiscou... | 88.7          |
| Step_0-AveragePolicyStd | 0.51633805    |
| Step_0-AverageReturn    | 146           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 299           |
| Step_0-MinReturn        | 0.297         |
| Step_0-NumTrajs         | 1796          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 50            |
| Step_1-AverageDiscou... | 90.1          |
| Step_1-AveragePolicyStd | 0.5161237     |
| Step_1-AverageReturn    | 152           |
| Step_1-EnvExecTime      | 22.3          |
| Step_1-MaxReturn        | 344           |
| Step_1-MinReturn        | -0.0952       |
| Step_1-NumTrajs         | 1700          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 54.9          |
| Time                    | 8.1e+03       |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 53.1          |
| Time-TotalInner         | 54.4          |
| dLoss                   | 0.0065502273  |
| n_timesteps             | 40320000      |
-------------------------------------------

 ---------------- Iteration 126 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 126            |
| ItrTime                 | 66.7           |
| LossAfter               | -0.0068357354  |
| LossBefore              | 3.7232564e-09  |
| MeanKL                  | 0.006833619    |
| MeanKLBefore            | -1.4897589e-09 |
| Step_0-AverageDiscou... | 94.2           |
| Step_0-AveragePolicyStd | 0.51248944     |
| Step_0-AverageReturn    | 158            |
| Step_0-EnvExecTime      | 21.9           |
| Step_0-MaxReturn        | 307            |
| Step_0-MinReturn        | 11.6           |
| Step_0-NumTrajs         | 1743           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 40.7           |
| Step_1-AverageDiscou... | 94             |
| Step_1-AveragePolicyStd | 0.5124664      |
| Step_1-AverageReturn    | 162            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 345            |
| Step_1-MinReturn        | 2.55           |
| Step_1-NumTrajs         | 1624           |
| Step_1-PolicyExecTime   | 3.66           |
| Step_1-StdReturn        | 47             |
| Time                    | 8.17e+03       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.08           |
| Time-Sampling           | 51.9           |
| Time-TotalInner         | 53.2           |
| dLoss                   | 0.006835739    |
| n_timesteps             | 40640000       |
--------------------------------------------

 ---------------- Iteration 127 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 127           |
| ItrTime                 | 67.3          |
| LossAfter               | -0.006852041  |
| LossBefore              | 3.6126395e-09 |
| MeanKL                  | 0.0067015537  |
| MeanKLBefore            | -1.490021e-09 |
| Step_0-AverageDiscou... | 92.6          |
| Step_0-AveragePolicyStd | 0.51134616    |
| Step_0-AverageReturn    | 158           |
| Step_0-EnvExecTime      | 21.9          |
| Step_0-MaxReturn        | 322           |
| Step_0-MinReturn        | 5.01          |
| Step_0-NumTrajs         | 1677          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 46.7          |
| Step_1-AverageDiscou... | 91.7          |
| Step_1-AveragePolicyStd | 0.51113784    |
| Step_1-AverageReturn    | 161           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 349           |
| Step_1-MinReturn        | 6.85          |
| Step_1-NumTrajs         | 1574          |
| Step_1-PolicyExecTime   | 3.75          |
| Step_1-StdReturn        | 55            |
| Time                    | 8.24e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.05          |
| Time-Sampling           | 52.5          |
| Time-TotalInner         | 53.8          |
| dLoss                   | 0.0068520447  |
| n_timesteps             | 40960000      |
-------------------------------------------

 ---------------- Iteration 128 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 128            |
| ItrTime                 | 68.1           |
| LossAfter               | -0.0068094977  |
| LossBefore              | -4.2172994e-09 |
| MeanKL                  | 0.0064757587   |
| MeanKLBefore            | -3.514522e-13  |
| Step_0-AverageDiscou... | 89.2           |
| Step_0-AveragePolicyStd | 0.51090145     |
| Step_0-AverageReturn    | 153            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 291            |
| Step_0-MinReturn        | 0.544          |
| Step_0-NumTrajs         | 1653           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 54.8           |
| Step_1-AverageDiscou... | 88.1           |
| Step_1-AveragePolicyStd | 0.5108004      |
| Step_1-AverageReturn    | 155            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 310            |
| Step_1-MinReturn        | -1.27          |
| Step_1-NumTrajs         | 1560           |
| Step_1-PolicyExecTime   | 3.71           |
| Step_1-StdReturn        | 60.3           |
| Time                    | 8.31e+03       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 53.2           |
| Time-TotalInner         | 54.5           |
| dLoss                   | 0.0068094935   |
| n_timesteps             | 41280000       |
--------------------------------------------

 ---------------- Iteration 129 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 129           |
| ItrTime                 | 66.6          |
| LossAfter               | -0.0070747556 |
| LossBefore              | -5.110978e-09 |
| MeanKL                  | 0.0075408155  |
| MeanKLBefore            | 4.550138e-13  |
| Step_0-AverageDiscou... | 92.4          |
| Step_0-AveragePolicyStd | 0.5100451     |
| Step_0-AverageReturn    | 153           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 322           |
| Step_0-MinReturn        | 0.675         |
| Step_0-NumTrajs         | 1776          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 43.6          |
| Step_1-AverageDiscou... | 92.6          |
| Step_1-AveragePolicyStd | 0.5098598     |
| Step_1-AverageReturn    | 160           |
| Step_1-EnvExecTime      | 22            |
| Step_1-MaxReturn        | 352           |
| Step_1-MinReturn        | 2.39          |
| Step_1-NumTrajs         | 1595          |
| Step_1-PolicyExecTime   | 3.6           |
| Step_1-StdReturn        | 53.6          |
| Time                    | 8.37e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.06          |
| Time-Sampling           | 51.7          |
| Time-TotalInner         | 53            |
| dLoss                   | 0.0070747505  |
| n_timesteps             | 41600000      |
-------------------------------------------

 ---------------- Iteration 130 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 130            |
| ItrTime                 | 68.1           |
| LossAfter               | -0.0066409456  |
| LossBefore              | 4.092707e-09   |
| MeanKL                  | 0.006612614    |
| MeanKLBefore            | -2.9803033e-09 |
| Step_0-AverageDiscou... | 91.6           |
| Step_0-AveragePolicyStd | 0.5056429      |
| Step_0-AverageReturn    | 156            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 328            |
| Step_0-MinReturn        | 12.2           |
| Step_0-NumTrajs         | 1680           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 50.4           |
| Step_1-AverageDiscou... | 91.5           |
| Step_1-AveragePolicyStd | 0.50557786     |
| Step_1-AverageReturn    | 159            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 390            |
| Step_1-MinReturn        | 7.07           |
| Step_1-NumTrajs         | 1608           |
| Step_1-PolicyExecTime   | 3.68           |
| Step_1-StdReturn        | 52.3           |
| Time                    | 8.44e+03       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.05           |
| Time-Sampling           | 53.3           |
| Time-TotalInner         | 54.5           |
| dLoss                   | 0.0066409498   |
| n_timesteps             | 41920000       |
--------------------------------------------

 ---------------- Iteration 131 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 131            |
| ItrTime                 | 65.9           |
| LossAfter               | -0.0069450126  |
| LossBefore              | -2.1197202e-09 |
| MeanKL                  | 0.006875229    |
| MeanKLBefore            | 2.9795157e-09  |
| Step_0-AverageDiscou... | 91.5           |
| Step_0-AveragePolicyStd | 0.504895       |
| Step_0-AverageReturn    | 151            |
| Step_0-EnvExecTime      | 21.6           |
| Step_0-MaxReturn        | 302            |
| Step_0-MinReturn        | 2.5            |
| Step_0-NumTrajs         | 1809           |
| Step_0-PolicyExecTime   | 1.15           |
| Step_0-StdReturn        | 47.8           |
| Step_1-AverageDiscou... | 91.3           |
| Step_1-AveragePolicyStd | 0.5048383      |
| Step_1-AverageReturn    | 153            |
| Step_1-EnvExecTime      | 21.6           |
| Step_1-MaxReturn        | 342            |
| Step_1-MinReturn        | 3.5            |
| Step_1-NumTrajs         | 1712           |
| Step_1-PolicyExecTime   | 3.58           |
| Step_1-StdReturn        | 53.5           |
| Time                    | 8.51e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.09           |
| Time-Sampling           | 51             |
| Time-TotalInner         | 52.3           |
| dLoss                   | 0.0069450103   |
| n_timesteps             | 42240000       |
--------------------------------------------

 ---------------- Iteration 132 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 132           |
| ItrTime                 | 67.1          |
| LossAfter               | -0.0067519387 |
| LossBefore              | -2.399958e-09 |
| MeanKL                  | 0.007028489   |
| MeanKLBefore            | 3.6015635e-13 |
| Step_0-AverageDiscou... | 91.9          |
| Step_0-AveragePolicyStd | 0.5036505     |
| Step_0-AverageReturn    | 157           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 333           |
| Step_0-MinReturn        | 7.89          |
| Step_0-NumTrajs         | 1639          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 49.9          |
| Step_1-AverageDiscou... | 92.8          |
| Step_1-AveragePolicyStd | 0.5035781     |
| Step_1-AverageReturn    | 162           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 338           |
| Step_1-MinReturn        | 2.85          |
| Step_1-NumTrajs         | 1589          |
| Step_1-PolicyExecTime   | 3.67          |
| Step_1-StdReturn        | 52.4          |
| Time                    | 8.57e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 52.3          |
| Time-TotalInner         | 53.5          |
| dLoss                   | 0.0067519364  |
| n_timesteps             | 42560000      |
-------------------------------------------

 ---------------- Iteration 133 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 133           |
| ItrTime                 | 67.4          |
| LossAfter               | -0.0068075806 |
| LossBefore              | 2.1101474e-09 |
| MeanKL                  | 0.0067166267  |
| MeanKLBefore            | 5.9599903e-09 |
| Step_0-AverageDiscou... | 93.4          |
| Step_0-AveragePolicyStd | 0.50092185    |
| Step_0-AverageReturn    | 155           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 311           |
| Step_0-MinReturn        | 11.3          |
| Step_0-NumTrajs         | 1766          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 43.3          |
| Step_1-AverageDiscou... | 94.4          |
| Step_1-AveragePolicyStd | 0.500854      |
| Step_1-AverageReturn    | 161           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 367           |
| Step_1-MinReturn        | 15.6          |
| Step_1-NumTrajs         | 1658          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 47.2          |
| Time                    | 8.64e+03      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.07          |
| Time-Sampling           | 52.5          |
| Time-TotalInner         | 53.8          |
| dLoss                   | 0.006807583   |
| n_timesteps             | 42880000      |
-------------------------------------------

 ---------------- Iteration 134 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 134            |
| ItrTime                 | 68             |
| LossAfter               | -0.0068635503  |
| LossBefore              | 3.4723318e-09  |
| MeanKL                  | 0.0068378923   |
| MeanKLBefore            | -7.4490814e-09 |
| Step_0-AverageDiscou... | 91.4           |
| Step_0-AveragePolicyStd | 0.4987186      |
| Step_0-AverageReturn    | 156            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 357            |
| Step_0-MinReturn        | 13.4           |
| Step_0-NumTrajs         | 1674           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 50.1           |
| Step_1-AverageDiscou... | 92.8           |
| Step_1-AveragePolicyStd | 0.4985297      |
| Step_1-AverageReturn    | 161            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 325            |
| Step_1-MinReturn        | 11.9           |
| Step_1-NumTrajs         | 1621           |
| Step_1-PolicyExecTime   | 3.65           |
| Step_1-StdReturn        | 51.2           |
| Time                    | 8.71e+03       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 53.1           |
| Time-TotalInner         | 54.4           |
| dLoss                   | 0.0068635535   |
| n_timesteps             | 43200000       |
--------------------------------------------

 ---------------- Iteration 135 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 135            |
| ItrTime                 | 68.4           |
| LossAfter               | -0.0069009415  |
| LossBefore              | -8.3988744e-10 |
| MeanKL                  | 0.006884289    |
| MeanKLBefore            | 1.4901881e-09  |
| Step_0-AverageDiscou... | 91             |
| Step_0-AveragePolicyStd | 0.49786353     |
| Step_0-AverageReturn    | 155            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 320            |
| Step_0-MinReturn        | 15.3           |
| Step_0-NumTrajs         | 1646           |
| Step_0-PolicyExecTime   | 1.18           |
| Step_0-StdReturn        | 50.1           |
| Step_1-AverageDiscou... | 91.3           |
| Step_1-AveragePolicyStd | 0.4977567      |
| Step_1-AverageReturn    | 159            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 346            |
| Step_1-MinReturn        | 14             |
| Step_1-NumTrajs         | 1586           |
| Step_1-PolicyExecTime   | 3.7            |
| Step_1-StdReturn        | 55             |
| Time                    | 8.78e+03       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.8           |
| dLoss                   | 0.0069009406   |
| n_timesteps             | 43520000       |
--------------------------------------------

 ---------------- Iteration 136 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 136           |
| ItrTime                 | 66.1          |
| LossAfter               | -0.0072353696 |
| LossBefore              | 9.444756e-09  |
| MeanKL                  | 0.006865126   |
| MeanKLBefore            | 5.960473e-09  |
| Step_0-AverageDiscou... | 93.5          |
| Step_0-AveragePolicyStd | 0.49692574    |
| Step_0-AverageReturn    | 158           |
| Step_0-EnvExecTime      | 21.5          |
| Step_0-MaxReturn        | 376           |
| Step_0-MinReturn        | -0.258        |
| Step_0-NumTrajs         | 1699          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 49.1          |
| Step_1-AverageDiscou... | 92.7          |
| Step_1-AveragePolicyStd | 0.49697185    |
| Step_1-AverageReturn    | 161           |
| Step_1-EnvExecTime      | 21.8          |
| Step_1-MaxReturn        | 348           |
| Step_1-MinReturn        | -11.8         |
| Step_1-NumTrajs         | 1603          |
| Step_1-PolicyExecTime   | 3.66          |
| Step_1-StdReturn        | 56.6          |
| Time                    | 8.84e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.05          |
| Time-Sampling           | 51.3          |
| Time-TotalInner         | 52.5          |
| dLoss                   | 0.007235379   |
| n_timesteps             | 43840000      |
-------------------------------------------

 ---------------- Iteration 137 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 137            |
| ItrTime                 | 68.2           |
| LossAfter               | -0.00686457    |
| LossBefore              | -6.8603243e-09 |
| MeanKL                  | 0.006608209    |
| MeanKLBefore            | -4.4699577e-09 |
| Step_0-AverageDiscou... | 95.4           |
| Step_0-AveragePolicyStd | 0.49853766     |
| Step_0-AverageReturn    | 162            |
| Step_0-EnvExecTime      | 22.2           |
| Step_0-MaxReturn        | 355            |
| Step_0-MinReturn        | 14.7           |
| Step_0-NumTrajs         | 1686           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 45.8           |
| Step_1-AverageDiscou... | 95.1           |
| Step_1-AveragePolicyStd | 0.498459       |
| Step_1-AverageReturn    | 167            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 386            |
| Step_1-MinReturn        | -1.32          |
| Step_1-NumTrajs         | 1562           |
| Step_1-PolicyExecTime   | 3.73           |
| Step_1-StdReturn        | 54.1           |
| Time                    | 8.91e+03       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.06           |
| Time-Sampling           | 53.3           |
| Time-TotalInner         | 54.6           |
| dLoss                   | 0.006864563    |
| n_timesteps             | 44160000       |
--------------------------------------------

 ---------------- Iteration 138 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 138           |
| ItrTime                 | 67.3          |
| LossAfter               | -0.007090953  |
| LossBefore              | 2.6987446e-09 |
| MeanKL                  | 0.006929164   |
| MeanKLBefore            | -6.306067e-13 |
| Step_0-AverageDiscou... | 95.1          |
| Step_0-AveragePolicyStd | 0.4972881     |
| Step_0-AverageReturn    | 162           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 351           |
| Step_0-MinReturn        | 7.5           |
| Step_0-NumTrajs         | 1667          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 50.5          |
| Step_1-AverageDiscou... | 95.6          |
| Step_1-AveragePolicyStd | 0.49724904    |
| Step_1-AverageReturn    | 168           |
| Step_1-EnvExecTime      | 22.3          |
| Step_1-MaxReturn        | 342           |
| Step_1-MinReturn        | 12.3          |
| Step_1-NumTrajs         | 1557          |
| Step_1-PolicyExecTime   | 3.7           |
| Step_1-StdReturn        | 54.7          |
| Time                    | 8.98e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 52.5          |
| Time-TotalInner         | 53.7          |
| dLoss                   | 0.007090956   |
| n_timesteps             | 44480000      |
-------------------------------------------

 ---------------- Iteration 139 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 139           |
| ItrTime                 | 67.2          |
| LossAfter               | -0.006911774  |
| LossBefore              | 1.4635994e-09 |
| MeanKL                  | 0.006857884   |
| MeanKLBefore            | 8.939987e-09  |
| Step_0-AverageDiscou... | 96.2          |
| Step_0-AveragePolicyStd | 0.4951426     |
| Step_0-AverageReturn    | 161           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 340           |
| Step_0-MinReturn        | 20.9          |
| Step_0-NumTrajs         | 1751          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 43            |
| Step_1-AverageDiscou... | 97.3          |
| Step_1-AveragePolicyStd | 0.49500445    |
| Step_1-AverageReturn    | 167           |
| Step_1-EnvExecTime      | 22.1          |
| Step_1-MaxReturn        | 336           |
| Step_1-MinReturn        | 8.75          |
| Step_1-NumTrajs         | 1652          |
| Step_1-PolicyExecTime   | 3.62          |
| Step_1-StdReturn        | 46.7          |
| Time                    | 9.05e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.06          |
| Time-Sampling           | 52.3          |
| Time-TotalInner         | 53.6          |
| dLoss                   | 0.0069117756  |
| n_timesteps             | 44800000      |
-------------------------------------------

 ---------------- Iteration 140 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 140           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.0064195665 |
| LossBefore              | 1.9868898e-09 |
| MeanKL                  | 0.006828549   |
| MeanKLBefore            | 5.959283e-09  |
| Step_0-AverageDiscou... | 95.6          |
| Step_0-AveragePolicyStd | 0.4922341     |
| Step_0-AverageReturn    | 162           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 316           |
| Step_0-MinReturn        | 15.1          |
| Step_0-NumTrajs         | 1704          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 49            |
| Step_1-AverageDiscou... | 95.7          |
| Step_1-AveragePolicyStd | 0.49198213    |
| Step_1-AverageReturn    | 164           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 340           |
| Step_1-MinReturn        | 9.23          |
| Step_1-NumTrajs         | 1641          |
| Step_1-PolicyExecTime   | 3.67          |
| Step_1-StdReturn        | 52.5          |
| Time                    | 9.11e+03      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.06          |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0064195683  |
| n_timesteps             | 45120000      |
-------------------------------------------

 ---------------- Iteration 141 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 141           |
| ItrTime                 | 68.7          |
| LossAfter               | -0.0069117686 |
| LossBefore              | -4.087405e-09 |
| MeanKL                  | 0.0068761334  |
| MeanKLBefore            | 4.469979e-09  |
| Step_0-AverageDiscou... | 89.3          |
| Step_0-AveragePolicyStd | 0.4904157     |
| Step_0-AverageReturn    | 153           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 326           |
| Step_0-MinReturn        | 12.3          |
| Step_0-NumTrajs         | 1611          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 60.7          |
| Step_1-AverageDiscou... | 88.4          |
| Step_1-AveragePolicyStd | 0.49023667    |
| Step_1-AverageReturn    | 156           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 362           |
| Step_1-MinReturn        | 3.93          |
| Step_1-NumTrajs         | 1491          |
| Step_1-PolicyExecTime   | 3.71          |
| Step_1-StdReturn        | 64.9          |
| Time                    | 9.18e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 53.9          |
| Time-TotalInner         | 55.2          |
| dLoss                   | 0.0069117644  |
| n_timesteps             | 45440000      |
-------------------------------------------

 ---------------- Iteration 142 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 142            |
| ItrTime                 | 67.3           |
| LossAfter               | -0.0067273504  |
| LossBefore              | -5.9101685e-10 |
| MeanKL                  | 0.006667182    |
| MeanKLBefore            | 4.4704285e-09  |
| Step_0-AverageDiscou... | 96.4           |
| Step_0-AveragePolicyStd | 0.48939607     |
| Step_0-AverageReturn    | 164            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 319            |
| Step_0-MinReturn        | 2.86           |
| Step_0-NumTrajs         | 1696           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 43.6           |
| Step_1-AverageDiscou... | 96.6           |
| Step_1-AveragePolicyStd | 0.48933008     |
| Step_1-AverageReturn    | 167            |
| Step_1-EnvExecTime      | 22.3           |
| Step_1-MaxReturn        | 360            |
| Step_1-MinReturn        | 4.75           |
| Step_1-NumTrajs         | 1619           |
| Step_1-PolicyExecTime   | 3.62           |
| Step_1-StdReturn        | 50.2           |
| Time                    | 9.25e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 52.5           |
| Time-TotalInner         | 53.7           |
| dLoss                   | 0.00672735     |
| n_timesteps             | 45760000       |
--------------------------------------------

 ---------------- Iteration 143 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 143           |
| ItrTime                 | 67.5          |
| LossAfter               | -0.007009757  |
| LossBefore              | 2.0834393e-10 |
| MeanKL                  | 0.0071387747  |
| MeanKLBefore            | -7.188028e-13 |
| Step_0-AverageDiscou... | 96.4          |
| Step_0-AveragePolicyStd | 0.48929715    |
| Step_0-AverageReturn    | 166           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 339           |
| Step_0-MinReturn        | 11.8          |
| Step_0-NumTrajs         | 1659          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 50.1          |
| Step_1-AverageDiscou... | 97.3          |
| Step_1-AveragePolicyStd | 0.48916566    |
| Step_1-AverageReturn    | 171           |
| Step_1-EnvExecTime      | 22.5          |
| Step_1-MaxReturn        | 379           |
| Step_1-MinReturn        | 9.63          |
| Step_1-NumTrajs         | 1574          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 51.2          |
| Time                    | 9.32e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 52.6          |
| Time-TotalInner         | 53.9          |
| dLoss                   | 0.007009757   |
| n_timesteps             | 46080000      |
-------------------------------------------

 ---------------- Iteration 144 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 144            |
| ItrTime                 | 66.7           |
| LossAfter               | -0.006738058   |
| LossBefore              | -1.4863952e-09 |
| MeanKL                  | 0.0071620033   |
| MeanKLBefore            | -7.449002e-09  |
| Step_0-AverageDiscou... | 94.5           |
| Step_0-AveragePolicyStd | 0.48864108     |
| Step_0-AverageReturn    | 160            |
| Step_0-EnvExecTime      | 21.5           |
| Step_0-MaxReturn        | 369            |
| Step_0-MinReturn        | 7.35           |
| Step_0-NumTrajs         | 1712           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 52.1           |
| Step_1-AverageDiscou... | 94.8           |
| Step_1-AveragePolicyStd | 0.48852023     |
| Step_1-AverageReturn    | 164            |
| Step_1-EnvExecTime      | 22.3           |
| Step_1-MaxReturn        | 351            |
| Step_1-MinReturn        | 4.8            |
| Step_1-NumTrajs         | 1636           |
| Step_1-PolicyExecTime   | 3.71           |
| Step_1-StdReturn        | 53.7           |
| Time                    | 9.38e+03       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.06           |
| Time-Sampling           | 51.8           |
| Time-TotalInner         | 53             |
| dLoss                   | 0.0067380564   |
| n_timesteps             | 46400000       |
--------------------------------------------

 ---------------- Iteration 145 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 145            |
| ItrTime                 | 67             |
| LossAfter               | -0.0071989135  |
| LossBefore              | 1.0619227e-09  |
| MeanKL                  | 0.0072010234   |
| MeanKLBefore            | -2.9801168e-09 |
| Step_0-AverageDiscou... | 96.2           |
| Step_0-AveragePolicyStd | 0.4870755      |
| Step_0-AverageReturn    | 163            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 368            |
| Step_0-MinReturn        | 11.3           |
| Step_0-NumTrajs         | 1689           |
| Step_0-PolicyExecTime   | 1.18           |
| Step_0-StdReturn        | 48.9           |
| Step_1-AverageDiscou... | 94.3           |
| Step_1-AveragePolicyStd | 0.48698306     |
| Step_1-AverageReturn    | 165            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 363            |
| Step_1-MinReturn        | 1.79           |
| Step_1-NumTrajs         | 1549           |
| Step_1-PolicyExecTime   | 3.69           |
| Step_1-StdReturn        | 59.2           |
| Time                    | 9.45e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 52.1           |
| Time-TotalInner         | 53.4           |
| dLoss                   | 0.0071989144   |
| n_timesteps             | 46720000       |
--------------------------------------------

 ---------------- Iteration 146 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 146           |
| ItrTime                 | 66.2          |
| LossAfter               | -0.0070445985 |
| LossBefore              | -4.236763e-09 |
| MeanKL                  | 0.006828291   |
| MeanKLBefore            | 5.958696e-09  |
| Step_0-AverageDiscou... | 99.6          |
| Step_0-AveragePolicyStd | 0.4843257     |
| Step_0-AverageReturn    | 171           |
| Step_0-EnvExecTime      | 21.4          |
| Step_0-MaxReturn        | 366           |
| Step_0-MinReturn        | 5.23          |
| Step_0-NumTrajs         | 1667          |
| Step_0-PolicyExecTime   | 1.15          |
| Step_0-StdReturn        | 41.5          |
| Step_1-AverageDiscou... | 99.5          |
| Step_1-AveragePolicyStd | 0.484176      |
| Step_1-AverageReturn    | 177           |
| Step_1-EnvExecTime      | 22            |
| Step_1-MaxReturn        | 374           |
| Step_1-MinReturn        | 1.22          |
| Step_1-NumTrajs         | 1536          |
| Step_1-PolicyExecTime   | 3.73          |
| Step_1-StdReturn        | 51.2          |
| Time                    | 9.52e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 51.4          |
| Time-TotalInner         | 52.6          |
| dLoss                   | 0.0070445943  |
| n_timesteps             | 47040000      |
-------------------------------------------

 ---------------- Iteration 147 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 147           |
| ItrTime                 | 67.9          |
| LossAfter               | -0.006593317  |
| LossBefore              | -3.965186e-09 |
| MeanKL                  | 0.006980783   |
| MeanKLBefore            | 7.449427e-09  |
| Step_0-AverageDiscou... | 94.1          |
| Step_0-AveragePolicyStd | 0.48403302    |
| Step_0-AverageReturn    | 162           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 335           |
| Step_0-MinReturn        | 1.97          |
| Step_0-NumTrajs         | 1665          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 57.5          |
| Step_1-AverageDiscou... | 94            |
| Step_1-AveragePolicyStd | 0.48392376    |
| Step_1-AverageReturn    | 168           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 375           |
| Step_1-MinReturn        | 9.58          |
| Step_1-NumTrajs         | 1527          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 64.9          |
| Time                    | 9.59e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 53            |
| Time-TotalInner         | 54.2          |
| dLoss                   | 0.0065933126  |
| n_timesteps             | 47360000      |
-------------------------------------------

 ---------------- Iteration 148 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 148            |
| ItrTime                 | 66.9           |
| LossAfter               | -0.0064976723  |
| LossBefore              | -4.4675765e-09 |
| MeanKL                  | 0.0065182797   |
| MeanKLBefore            | 8.217427e-13   |
| Step_0-AverageDiscou... | 96.6           |
| Step_0-AveragePolicyStd | 0.48360598     |
| Step_0-AverageReturn    | 161            |
| Step_0-EnvExecTime      | 22.2           |
| Step_0-MaxReturn        | 319            |
| Step_0-MinReturn        | 11.2           |
| Step_0-NumTrajs         | 1753           |
| Step_0-PolicyExecTime   | 1.17           |
| Step_0-StdReturn        | 46.8           |
| Step_1-AverageDiscou... | 97             |
| Step_1-AveragePolicyStd | 0.48351273     |
| Step_1-AverageReturn    | 165            |
| Step_1-EnvExecTime      | 22             |
| Step_1-MaxReturn        | 380            |
| Step_1-MinReturn        | 4.9            |
| Step_1-NumTrajs         | 1667           |
| Step_1-PolicyExecTime   | 3.59           |
| Step_1-StdReturn        | 50.5           |
| Time                    | 9.65e+03       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.07           |
| Time-Sampling           | 52             |
| Time-TotalInner         | 53.3           |
| dLoss                   | 0.0064976676   |
| n_timesteps             | 47680000       |
--------------------------------------------

 ---------------- Iteration 149 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 149           |
| ItrTime                 | 67.7          |
| LossAfter               | -0.0067341    |
| LossBefore              | -4.535754e-09 |
| MeanKL                  | 0.006484083   |
| MeanKLBefore            | 8.940387e-09  |
| Step_0-AverageDiscou... | 98.3          |
| Step_0-AveragePolicyStd | 0.48514995    |
| Step_0-AverageReturn    | 168           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 369           |
| Step_0-MinReturn        | 6.66          |
| Step_0-NumTrajs         | 1659          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 48.5          |
| Step_1-AverageDiscou... | 97.9          |
| Step_1-AveragePolicyStd | 0.48499563    |
| Step_1-AverageReturn    | 173           |
| Step_1-EnvExecTime      | 22.7          |
| Step_1-MaxReturn        | 397           |
| Step_1-MinReturn        | 7.81          |
| Step_1-NumTrajs         | 1544          |
| Step_1-PolicyExecTime   | 3.67          |
| Step_1-StdReturn        | 55.9          |
| Time                    | 9.72e+03      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 52.8          |
| Time-TotalInner         | 54.1          |
| dLoss                   | 0.0067340955  |
| n_timesteps             | 48000000      |
-------------------------------------------

 ---------------- Iteration 150 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 150            |
| ItrTime                 | 68.3           |
| LossAfter               | -0.0070960773  |
| LossBefore              | -1.2044402e-09 |
| MeanKL                  | 0.006761208    |
| MeanKLBefore            | -1.4907012e-09 |
| Step_0-AverageDiscou... | 92.1           |
| Step_0-AveragePolicyStd | 0.4847153      |
| Step_0-AverageReturn    | 159            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 358            |
| Step_0-MinReturn        | 12.9           |
| Step_0-NumTrajs         | 1597           |
| Step_0-PolicyExecTime   | 1.18           |
| Step_0-StdReturn        | 60.7           |
| Step_1-AverageDiscou... | 94             |
| Step_1-AveragePolicyStd | 0.4845274      |
| Step_1-AverageReturn    | 167            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 421            |
| Step_1-MinReturn        | 10.4           |
| Step_1-NumTrajs         | 1515           |
| Step_1-PolicyExecTime   | 3.68           |
| Step_1-StdReturn        | 62.2           |
| Time                    | 9.79e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.8           |
| dLoss                   | 0.007096076    |
| n_timesteps             | 48320000       |
--------------------------------------------

 ---------------- Iteration 151 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 151           |
| ItrTime                 | 67.2          |
| LossAfter               | -0.006758089  |
| LossBefore              | -8.170747e-09 |
| MeanKL                  | 0.0069571747  |
| MeanKLBefore            | 7.448663e-09  |
| Step_0-AverageDiscou... | 100           |
| Step_0-AveragePolicyStd | 0.48466372    |
| Step_0-AverageReturn    | 176           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 354           |
| Step_0-MinReturn        | 3.91          |
| Step_0-NumTrajs         | 1597          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 47.6          |
| Step_1-AverageDiscou... | 99            |
| Step_1-AveragePolicyStd | 0.4847792     |
| Step_1-AverageReturn    | 178           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 421           |
| Step_1-MinReturn        | 7.73          |
| Step_1-NumTrajs         | 1521          |
| Step_1-PolicyExecTime   | 3.67          |
| Step_1-StdReturn        | 55.2          |
| Time                    | 9.86e+03      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 52.4          |
| Time-TotalInner         | 53.6          |
| dLoss                   | 0.006758081   |
| n_timesteps             | 48640000      |
-------------------------------------------

 ---------------- Iteration 152 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 152            |
| ItrTime                 | 66.4           |
| LossAfter               | -0.0068374276  |
| LossBefore              | -3.7486982e-09 |
| MeanKL                  | 0.006688313    |
| MeanKLBefore            | -5.9593894e-09 |
| Step_0-AverageDiscou... | 101            |
| Step_0-AveragePolicyStd | 0.4838708      |
| Step_0-AverageReturn    | 172            |
| Step_0-EnvExecTime      | 21.7           |
| Step_0-MaxReturn        | 372            |
| Step_0-MinReturn        | -3.38          |
| Step_0-NumTrajs         | 1678           |
| Step_0-PolicyExecTime   | 1.18           |
| Step_0-StdReturn        | 45.9           |
| Step_1-AverageDiscou... | 100            |
| Step_1-AveragePolicyStd | 0.48365775     |
| Step_1-AverageReturn    | 175            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 363            |
| Step_1-MinReturn        | 12.3           |
| Step_1-NumTrajs         | 1599           |
| Step_1-PolicyExecTime   | 3.7            |
| Step_1-StdReturn        | 50.9           |
| Time                    | 9.92e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.06           |
| Time-Sampling           | 51.5           |
| Time-TotalInner         | 52.7           |
| dLoss                   | 0.006837424    |
| n_timesteps             | 48960000       |
--------------------------------------------

 ---------------- Iteration 153 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 153            |
| ItrTime                 | 66.8           |
| LossAfter               | -0.0069576157  |
| LossBefore              | -4.3345266e-10 |
| MeanKL                  | 0.006839396    |
| MeanKLBefore            | 4.4688937e-09  |
| Step_0-AverageDiscou... | 95.6           |
| Step_0-AveragePolicyStd | 0.48326555     |
| Step_0-AverageReturn    | 166            |
| Step_0-EnvExecTime      | 22.1           |
| Step_0-MaxReturn        | 352            |
| Step_0-MinReturn        | 10.9           |
| Step_0-NumTrajs         | 1627           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 57             |
| Step_1-AverageDiscou... | 96.3           |
| Step_1-AveragePolicyStd | 0.48313797     |
| Step_1-AverageReturn    | 169            |
| Step_1-EnvExecTime      | 22             |
| Step_1-MaxReturn        | 411            |
| Step_1-MinReturn        | 5.11           |
| Step_1-NumTrajs         | 1580           |
| Step_1-PolicyExecTime   | 3.62           |
| Step_1-StdReturn        | 57.4           |
| Time                    | 9.99e+03       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.04           |
| Time-Sampling           | 52             |
| Time-TotalInner         | 53.2           |
| dLoss                   | 0.0069576153   |
| n_timesteps             | 49280000       |
--------------------------------------------

 ---------------- Iteration 154 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 154           |
| ItrTime                 | 67.7          |
| LossAfter               | -0.006915593  |
| LossBefore              | 2.1606101e-09 |
| MeanKL                  | 0.0073808925  |
| MeanKLBefore            | 2.980245e-09  |
| Step_0-AverageDiscou... | 96.8          |
| Step_0-AveragePolicyStd | 0.48209646    |
| Step_0-AverageReturn    | 168           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 386           |
| Step_0-MinReturn        | 8.31          |
| Step_0-NumTrajs         | 1613          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 54.4          |
| Step_1-AverageDiscou... | 97.8          |
| Step_1-AveragePolicyStd | 0.48202434    |
| Step_1-AverageReturn    | 173           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 363           |
| Step_1-MinReturn        | 3.16          |
| Step_1-NumTrajs         | 1562          |
| Step_1-PolicyExecTime   | 3.66          |
| Step_1-StdReturn        | 55.6          |
| Time                    | 1.01e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 52.9          |
| Time-TotalInner         | 54.1          |
| dLoss                   | 0.0069155954  |
| n_timesteps             | 49600000      |
-------------------------------------------

 ---------------- Iteration 155 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 155           |
| ItrTime                 | 67.4          |
| LossAfter               | -0.006707871  |
| LossBefore              | -7.476911e-09 |
| MeanKL                  | 0.007073437   |
| MeanKLBefore            | 4.4694666e-09 |
| Step_0-AverageDiscou... | 97.2          |
| Step_0-AveragePolicyStd | 0.48005795    |
| Step_0-AverageReturn    | 168           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 432           |
| Step_0-MinReturn        | 9.96          |
| Step_0-NumTrajs         | 1653          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 52.5          |
| Step_1-AverageDiscou... | 98.7          |
| Step_1-AveragePolicyStd | 0.47988942    |
| Step_1-AverageReturn    | 177           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 424           |
| Step_1-MinReturn        | 12.3          |
| Step_1-NumTrajs         | 1541          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 58.6          |
| Time                    | 1.01e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 52.6          |
| Time-TotalInner         | 53.8          |
| dLoss                   | 0.0067078634  |
| n_timesteps             | 49920000      |
-------------------------------------------

 ---------------- Iteration 156 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 156           |
| ItrTime                 | 67            |
| LossAfter               | -0.0065932632 |
| LossBefore              | 1.7527177e-09 |
| MeanKL                  | 0.0070557133  |
| MeanKLBefore            | 5.9601524e-09 |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.48063955    |
| Step_0-AverageReturn    | 176           |
| Step_0-EnvExecTime      | 21.8          |
| Step_0-MaxReturn        | 426           |
| Step_0-MinReturn        | 12.6          |
| Step_0-NumTrajs         | 1637          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 50.4          |
| Step_1-AverageDiscou... | 100           |
| Step_1-AveragePolicyStd | 0.48065612    |
| Step_1-AverageReturn    | 178           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 363           |
| Step_1-MinReturn        | 11.6          |
| Step_1-NumTrajs         | 1541          |
| Step_1-PolicyExecTime   | 3.71          |
| Step_1-StdReturn        | 53.6          |
| Time                    | 1.02e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 52.2          |
| Time-TotalInner         | 53.4          |
| dLoss                   | 0.006593265   |
| n_timesteps             | 50240000      |
-------------------------------------------

 ---------------- Iteration 157 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 157           |
| ItrTime                 | 67.1          |
| LossAfter               | -0.006564596  |
| LossBefore              | 7.2268223e-09 |
| MeanKL                  | 0.006718823   |
| MeanKLBefore            | 1.49141e-09   |
| Step_0-AverageDiscou... | 99.8          |
| Step_0-AveragePolicyStd | 0.47863942    |
| Step_0-AverageReturn    | 177           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 411           |
| Step_0-MinReturn        | -6.85         |
| Step_0-NumTrajs         | 1580          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 55.2          |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.4785182     |
| Step_1-AverageReturn    | 183           |
| Step_1-EnvExecTime      | 22.3          |
| Step_1-MaxReturn        | 479           |
| Step_1-MinReturn        | 4.64          |
| Step_1-NumTrajs         | 1492          |
| Step_1-PolicyExecTime   | 3.7           |
| Step_1-StdReturn        | 57.2          |
| Time                    | 1.03e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 52.3          |
| Time-TotalInner         | 53.5          |
| dLoss                   | 0.0065646037  |
| n_timesteps             | 50560000      |
-------------------------------------------

 ---------------- Iteration 158 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 158           |
| ItrTime                 | 67.8          |
| LossAfter               | -0.0067058937 |
| LossBefore              | 3.733094e-09  |
| MeanKL                  | 0.0072288425  |
| MeanKLBefore            | 8.281376e-13  |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.47797295    |
| Step_0-AverageReturn    | 176           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 353           |
| Step_0-MinReturn        | 17.5          |
| Step_0-NumTrajs         | 1634          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 49.1          |
| Step_1-AverageDiscou... | 100           |
| Step_1-AveragePolicyStd | 0.4778948     |
| Step_1-AverageReturn    | 178           |
| Step_1-EnvExecTime      | 22.5          |
| Step_1-MaxReturn        | 382           |
| Step_1-MinReturn        | 13.5          |
| Step_1-NumTrajs         | 1552          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 53            |
| Time                    | 1.03e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 53            |
| Time-TotalInner         | 54.2          |
| dLoss                   | 0.0067058974  |
| n_timesteps             | 50880000      |
-------------------------------------------

 ---------------- Iteration 159 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 159           |
| ItrTime                 | 67.6          |
| LossAfter               | -0.006755084  |
| LossBefore              | 6.707991e-09  |
| MeanKL                  | 0.0071565136  |
| MeanKLBefore            | -2.979828e-09 |
| Step_0-AverageDiscou... | 98.8          |
| Step_0-AveragePolicyStd | 0.4760166     |
| Step_0-AverageReturn    | 173           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 375           |
| Step_0-MinReturn        | 10.6          |
| Step_0-NumTrajs         | 1601          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 52.8          |
| Step_1-AverageDiscou... | 99.5          |
| Step_1-AveragePolicyStd | 0.4759471     |
| Step_1-AverageReturn    | 177           |
| Step_1-EnvExecTime      | 22.5          |
| Step_1-MaxReturn        | 484           |
| Step_1-MinReturn        | 8.95          |
| Step_1-NumTrajs         | 1550          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 56.4          |
| Time                    | 1.04e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 52.8          |
| Time-TotalInner         | 54            |
| dLoss                   | 0.0067550903  |
| n_timesteps             | 51200000      |
-------------------------------------------

 ---------------- Iteration 160 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 160           |
| ItrTime                 | 66.2          |
| LossAfter               | -0.006623991  |
| LossBefore              | -3.219401e-09 |
| MeanKL                  | 0.006786355   |
| MeanKLBefore            | 5.9604526e-09 |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.47603193    |
| Step_0-AverageReturn    | 175           |
| Step_0-EnvExecTime      | 21.5          |
| Step_0-MaxReturn        | 386           |
| Step_0-MinReturn        | 5.25          |
| Step_0-NumTrajs         | 1647          |
| Step_0-PolicyExecTime   | 1.16          |
| Step_0-StdReturn        | 49.4          |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.47596952    |
| Step_1-AverageReturn    | 179           |
| Step_1-EnvExecTime      | 21.9          |
| Step_1-MaxReturn        | 419           |
| Step_1-MinReturn        | 6.4           |
| Step_1-NumTrajs         | 1577          |
| Step_1-PolicyExecTime   | 3.69          |
| Step_1-StdReturn        | 52.5          |
| Time                    | 1.05e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 51.4          |
| Time-TotalInner         | 52.6          |
| dLoss                   | 0.0066239876  |
| n_timesteps             | 51520000      |
-------------------------------------------

 ---------------- Iteration 161 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 161           |
| ItrTime                 | 68.3          |
| LossAfter               | -0.0067899623 |
| LossBefore              | -3.941221e-09 |
| MeanKL                  | 0.007042405   |
| MeanKLBefore            | 4.5758953e-13 |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.47453085    |
| Step_0-AverageReturn    | 174           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 364           |
| Step_0-MinReturn        | 15.4          |
| Step_0-NumTrajs         | 1632          |
| Step_0-PolicyExecTime   | 1.17          |
| Step_0-StdReturn        | 48.9          |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.47454494    |
| Step_1-AverageReturn    | 180           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 368           |
| Step_1-MinReturn        | 13.9          |
| Step_1-NumTrajs         | 1527          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 55.1          |
| Time                    | 1.05e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.0067899586  |
| n_timesteps             | 51840000      |
-------------------------------------------

 ---------------- Iteration 162 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 162           |
| ItrTime                 | 68.2          |
| LossAfter               | -0.00666255   |
| LossBefore              | 9.492451e-09  |
| MeanKL                  | 0.0069463677  |
| MeanKLBefore            | 3.9461766e-13 |
| Step_0-AverageDiscou... | 93.7          |
| Step_0-AveragePolicyStd | 0.4726597     |
| Step_0-AverageReturn    | 167           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 370           |
| Step_0-MinReturn        | 9.38          |
| Step_0-NumTrajs         | 1528          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 68.3          |
| Step_1-AverageDiscou... | 92.5          |
| Step_1-AveragePolicyStd | 0.4725477     |
| Step_1-AverageReturn    | 167           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 374           |
| Step_1-MinReturn        | 1.64          |
| Step_1-NumTrajs         | 1473          |
| Step_1-PolicyExecTime   | 3.71          |
| Step_1-StdReturn        | 69.7          |
| Time                    | 1.06e+04      |
| Time-InnerStep          | 0.183         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 53.4          |
| Time-TotalInner         | 54.6          |
| dLoss                   | 0.006662559   |
| n_timesteps             | 52160000      |
-------------------------------------------

 ---------------- Iteration 163 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 163           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.00728733   |
| LossBefore              | 7.05927e-10   |
| MeanKL                  | 0.0072235293  |
| MeanKLBefore            | -7.449766e-09 |
| Step_0-AverageDiscou... | 102           |
| Step_0-AveragePolicyStd | 0.47298488    |
| Step_0-AverageReturn    | 179           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 366           |
| Step_0-MinReturn        | 0.392         |
| Step_0-NumTrajs         | 1589          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 49            |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.47283897    |
| Step_1-AverageReturn    | 180           |
| Step_1-EnvExecTime      | 22.7          |
| Step_1-MaxReturn        | 396           |
| Step_1-MinReturn        | 11.9          |
| Step_1-NumTrajs         | 1542          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 54.2          |
| Time                    | 1.07e+04      |
| Time-InnerStep          | 0.157         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 53.6          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.007287331   |
| n_timesteps             | 52480000      |
-------------------------------------------

 ---------------- Iteration 164 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 164            |
| ItrTime                 | 68.1           |
| LossAfter               | -0.0070098126  |
| LossBefore              | 7.0542314e-09  |
| MeanKL                  | 0.006912372    |
| MeanKLBefore            | -1.4889258e-09 |
| Step_0-AverageDiscou... | 101            |
| Step_0-AveragePolicyStd | 0.46973184     |
| Step_0-AverageReturn    | 182            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 422            |
| Step_0-MinReturn        | 7.66           |
| Step_0-NumTrajs         | 1521           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 55.1           |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.4696406      |
| Step_1-AverageReturn    | 187            |
| Step_1-EnvExecTime      | 22.5           |
| Step_1-MaxReturn        | 522            |
| Step_1-MinReturn        | 8.94           |
| Step_1-NumTrajs         | 1445           |
| Step_1-PolicyExecTime   | 3.72           |
| Step_1-StdReturn        | 58.7           |
| Time                    | 1.07e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.994          |
| Time-Sampling           | 53.3           |
| Time-TotalInner         | 54.5           |
| dLoss                   | 0.0070098196   |
| n_timesteps             | 52800000       |
--------------------------------------------

 ---------------- Iteration 165 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 165            |
| ItrTime                 | 66.2           |
| LossAfter               | -0.0070271953  |
| LossBefore              | -2.9311507e-09 |
| MeanKL                  | 0.0068249456   |
| MeanKLBefore            | -5.960208e-09  |
| Step_0-AverageDiscou... | 103            |
| Step_0-AveragePolicyStd | 0.46928298     |
| Step_0-AverageReturn    | 180            |
| Step_0-EnvExecTime      | 21.5           |
| Step_0-MaxReturn        | 374            |
| Step_0-MinReturn        | 7.28           |
| Step_0-NumTrajs         | 1613           |
| Step_0-PolicyExecTime   | 1.16           |
| Step_0-StdReturn        | 54.1           |
| Step_1-AverageDiscou... | 104            |
| Step_1-AveragePolicyStd | 0.46910363     |
| Step_1-AverageReturn    | 185            |
| Step_1-EnvExecTime      | 21.9           |
| Step_1-MaxReturn        | 424            |
| Step_1-MinReturn        | 5.65           |
| Step_1-NumTrajs         | 1544           |
| Step_1-PolicyExecTime   | 3.69           |
| Step_1-StdReturn        | 59             |
| Time                    | 1.08e+04       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.03           |
| Time-Sampling           | 51.4           |
| Time-TotalInner         | 52.6           |
| dLoss                   | 0.0070271925   |
| n_timesteps             | 53120000       |
--------------------------------------------

 ---------------- Iteration 166 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 166            |
| ItrTime                 | 68.2           |
| LossAfter               | -0.0069089383  |
| LossBefore              | 2.314295e-09   |
| MeanKL                  | 0.007019695    |
| MeanKLBefore            | -2.9801543e-09 |
| Step_0-AverageDiscou... | 99.3           |
| Step_0-AveragePolicyStd | 0.4686647      |
| Step_0-AverageReturn    | 177            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 455            |
| Step_0-MinReturn        | 8.13           |
| Step_0-NumTrajs         | 1525           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 61.1           |
| Step_1-AverageDiscou... | 99.8           |
| Step_1-AveragePolicyStd | 0.46848196     |
| Step_1-AverageReturn    | 182            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 409            |
| Step_1-MinReturn        | 6.99           |
| Step_1-NumTrajs         | 1468           |
| Step_1-PolicyExecTime   | 3.73           |
| Step_1-StdReturn        | 63             |
| Time                    | 1.09e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1              |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.7           |
| dLoss                   | 0.0069089406   |
| n_timesteps             | 53440000       |
--------------------------------------------

 ---------------- Iteration 167 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 167           |
| ItrTime                 | 68.3          |
| LossAfter               | -0.0069593876 |
| LossBefore              | -9.885623e-09 |
| MeanKL                  | 0.007226768   |
| MeanKLBefore            | 1.4894258e-09 |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.46816528    |
| Step_0-AverageReturn    | 178           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 566           |
| Step_0-MinReturn        | 19            |
| Step_0-NumTrajs         | 1580          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 55.7          |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.46814033    |
| Step_1-AverageReturn    | 183           |
| Step_1-EnvExecTime      | 22.7          |
| Step_1-MaxReturn        | 366           |
| Step_1-MinReturn        | 15.9          |
| Step_1-NumTrajs         | 1490          |
| Step_1-PolicyExecTime   | 3.76          |
| Step_1-StdReturn        | 60            |
| Time                    | 1.09e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.006959378   |
| n_timesteps             | 53760000      |
-------------------------------------------

 ---------------- Iteration 168 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 168           |
| ItrTime                 | 67            |
| LossAfter               | -0.006859462  |
| LossBefore              | 3.0759746e-09 |
| MeanKL                  | 0.007504153   |
| MeanKLBefore            | 1.4894537e-09 |
| Step_0-AverageDiscou... | 103           |
| Step_0-AveragePolicyStd | 0.4664103     |
| Step_0-AverageReturn    | 183           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 382           |
| Step_0-MinReturn        | 9.21          |
| Step_0-NumTrajs         | 1581          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 52.1          |
| Step_1-AverageDiscou... | 102           |
| Step_1-AveragePolicyStd | 0.46630725    |
| Step_1-AverageReturn    | 185           |
| Step_1-EnvExecTime      | 22.2          |
| Step_1-MaxReturn        | 412           |
| Step_1-MinReturn        | 11            |
| Step_1-NumTrajs         | 1493          |
| Step_1-PolicyExecTime   | 3.68          |
| Step_1-StdReturn        | 57            |
| Time                    | 1.1e+04       |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.01          |
| Time-Sampling           | 52.1          |
| Time-TotalInner         | 53.4          |
| dLoss                   | 0.006859465   |
| n_timesteps             | 54080000      |
-------------------------------------------

 ---------------- Iteration 169 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 169           |
| ItrTime                 | 68.2          |
| LossAfter               | -0.0068248347 |
| LossBefore              | 1.7059847e-09 |
| MeanKL                  | 0.0068641156  |
| MeanKLBefore            | 1.490067e-09  |
| Step_0-AverageDiscou... | 101           |
| Step_0-AveragePolicyStd | 0.4660558     |
| Step_0-AverageReturn    | 179           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 436           |
| Step_0-MinReturn        | 4.4           |
| Step_0-NumTrajs         | 1570          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 62            |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.46594325    |
| Step_1-AverageReturn    | 186           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 402           |
| Step_1-MinReturn        | 9.95          |
| Step_1-NumTrajs         | 1446          |
| Step_1-PolicyExecTime   | 3.74          |
| Step_1-StdReturn        | 70.4          |
| Time                    | 1.11e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.991         |
| Time-Sampling           | 53.4          |
| Time-TotalInner         | 54.6          |
| dLoss                   | 0.0068248366  |
| n_timesteps             | 54400000      |
-------------------------------------------

 ---------------- Iteration 170 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 170            |
| ItrTime                 | 68.1           |
| LossAfter               | -0.006769163   |
| LossBefore              | 1.1814382e-09  |
| MeanKL                  | 0.0069028707   |
| MeanKLBefore            | -7.3541173e-13 |
| Step_0-AverageDiscou... | 104            |
| Step_0-AveragePolicyStd | 0.4661234      |
| Step_0-AverageReturn    | 189            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 454            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 1522           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 54             |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.4659885      |
| Step_1-AverageReturn    | 187            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 382            |
| Step_1-MinReturn        | -5.78          |
| Step_1-NumTrajs         | 1422           |
| Step_1-PolicyExecTime   | 3.74           |
| Step_1-StdReturn        | 63.4           |
| Time                    | 1.11e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.99           |
| Time-Sampling           | 53.2           |
| Time-TotalInner         | 54.4           |
| dLoss                   | 0.0067691645   |
| n_timesteps             | 54720000       |
--------------------------------------------

 ---------------- Iteration 171 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 171           |
| ItrTime                 | 68.2          |
| LossAfter               | -0.0070270197 |
| LossBefore              | -6.757176e-09 |
| MeanKL                  | 0.0075501935  |
| MeanKLBefore            | 2.9794698e-09 |
| Step_0-AverageDiscou... | 103           |
| Step_0-AveragePolicyStd | 0.4664179     |
| Step_0-AverageReturn    | 181           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 387           |
| Step_0-MinReturn        | 17.5          |
| Step_0-NumTrajs         | 1597          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 48.5          |
| Step_1-AverageDiscou... | 104           |
| Step_1-AveragePolicyStd | 0.46633765    |
| Step_1-AverageReturn    | 189           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 402           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1480          |
| Step_1-PolicyExecTime   | 3.7           |
| Step_1-StdReturn        | 55.4          |
| Time                    | 1.12e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1             |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.0070270128  |
| n_timesteps             | 55040000      |
-------------------------------------------

 ---------------- Iteration 172 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 172            |
| ItrTime                 | 68.3           |
| LossAfter               | -0.0069334405  |
| LossBefore              | -5.9817595e-09 |
| MeanKL                  | 0.0071772775   |
| MeanKLBefore            | -5.960073e-09  |
| Step_0-AverageDiscou... | 104            |
| Step_0-AveragePolicyStd | 0.464166       |
| Step_0-AverageReturn    | 185            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 432            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 1573           |
| Step_0-PolicyExecTime   | 1.2            |
| Step_0-StdReturn        | 50.5           |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.46417823     |
| Step_1-AverageReturn    | 183            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 441            |
| Step_1-MinReturn        | 5.93           |
| Step_1-NumTrajs         | 1498           |
| Step_1-PolicyExecTime   | 3.73           |
| Step_1-StdReturn        | 59.9           |
| Time                    | 1.13e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.99           |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.7           |
| dLoss                   | 0.0069334344   |
| n_timesteps             | 55360000       |
--------------------------------------------

 ---------------- Iteration 173 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 173            |
| ItrTime                 | 67.1           |
| LossAfter               | -0.007242188   |
| LossBefore              | -2.2391184e-09 |
| MeanKL                  | 0.0071190074   |
| MeanKLBefore            | -5.958628e-09  |
| Step_0-AverageDiscou... | 105            |
| Step_0-AveragePolicyStd | 0.463966       |
| Step_0-AverageReturn    | 186            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 404            |
| Step_0-MinReturn        | -17.6          |
| Step_0-NumTrajs         | 1593           |
| Step_0-PolicyExecTime   | 1.18           |
| Step_0-StdReturn        | 55.9           |
| Step_1-AverageDiscou... | 104            |
| Step_1-AveragePolicyStd | 0.463967       |
| Step_1-AverageReturn    | 188            |
| Step_1-EnvExecTime      | 22.3           |
| Step_1-MaxReturn        | 481            |
| Step_1-MinReturn        | 8.69           |
| Step_1-NumTrajs         | 1499           |
| Step_1-PolicyExecTime   | 3.7            |
| Step_1-StdReturn        | 65.9           |
| Time                    | 1.13e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.01           |
| Time-Sampling           | 52.3           |
| Time-TotalInner         | 53.5           |
| dLoss                   | 0.0072421855   |
| n_timesteps             | 55680000       |
--------------------------------------------

 ---------------- Iteration 174 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 174            |
| ItrTime                 | 67.1           |
| LossAfter               | -0.006783812   |
| LossBefore              | 5.7534213e-09  |
| MeanKL                  | 0.0067619346   |
| MeanKLBefore            | -2.9798781e-09 |
| Step_0-AverageDiscou... | 106            |
| Step_0-AveragePolicyStd | 0.464901       |
| Step_0-AverageReturn    | 191            |
| Step_0-EnvExecTime      | 22.1           |
| Step_0-MaxReturn        | 393            |
| Step_0-MinReturn        | 9.58           |
| Step_0-NumTrajs         | 1530           |
| Step_0-PolicyExecTime   | 1.57           |
| Step_0-StdReturn        | 51.8           |
| Step_1-AverageDiscou... | 105            |
| Step_1-AveragePolicyStd | 0.46475953     |
| Step_1-AverageReturn    | 191            |
| Step_1-EnvExecTime      | 21.8           |
| Step_1-MaxReturn        | 448            |
| Step_1-MinReturn        | -1.46          |
| Step_1-NumTrajs         | 1499           |
| Step_1-PolicyExecTime   | 3.65           |
| Step_1-StdReturn        | 54.3           |
| Time                    | 1.14e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1              |
| Time-Sampling           | 52.3           |
| Time-TotalInner         | 53.5           |
| dLoss                   | 0.0067838174   |
| n_timesteps             | 56000000       |
--------------------------------------------

 ---------------- Iteration 175 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 175           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.0067322655 |
| LossBefore              | -7.102696e-09 |
| MeanKL                  | 0.006885855   |
| MeanKLBefore            | 7.449819e-09  |
| Step_0-AverageDiscou... | 102           |
| Step_0-AveragePolicyStd | 0.46472147    |
| Step_0-AverageReturn    | 179           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 394           |
| Step_0-MinReturn        | 5.41          |
| Step_0-NumTrajs         | 1628          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 53.4          |
| Step_1-AverageDiscou... | 101           |
| Step_1-AveragePolicyStd | 0.4646049     |
| Step_1-AverageReturn    | 184           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 414           |
| Step_1-MinReturn        | 7.1           |
| Step_1-NumTrajs         | 1488          |
| Step_1-PolicyExecTime   | 3.76          |
| Step_1-StdReturn        | 61.4          |
| Time                    | 1.15e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.12          |
| Time-Sampling           | 53.4          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0067322585  |
| n_timesteps             | 56320000      |
-------------------------------------------

 ---------------- Iteration 176 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 176           |
| ItrTime                 | 68            |
| LossAfter               | -0.0071983696 |
| LossBefore              | 3.1398024e-09 |
| MeanKL                  | 0.0073405565  |
| MeanKLBefore            | 1.4892901e-09 |
| Step_0-AverageDiscou... | 105           |
| Step_0-AveragePolicyStd | 0.46418762    |
| Step_0-AverageReturn    | 187           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 474           |
| Step_0-MinReturn        | 12.6          |
| Step_0-NumTrajs         | 1560          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 60            |
| Step_1-AverageDiscou... | 105           |
| Step_1-AveragePolicyStd | 0.46409184    |
| Step_1-AverageReturn    | 191           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 434           |
| Step_1-MinReturn        | 17            |
| Step_1-NumTrajs         | 1498          |
| Step_1-PolicyExecTime   | 3.74          |
| Step_1-StdReturn        | 62.1          |
| Time                    | 1.15e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 52.9          |
| Time-TotalInner         | 54.1          |
| dLoss                   | 0.007198373   |
| n_timesteps             | 56640000      |
-------------------------------------------

 ---------------- Iteration 177 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 177            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0061492836  |
| LossBefore              | -1.3373412e-09 |
| MeanKL                  | 0.0071020783   |
| MeanKLBefore            | 1.4897572e-09  |
| Step_0-AverageDiscou... | 106            |
| Step_0-AveragePolicyStd | 0.46340376     |
| Step_0-AverageReturn    | 192            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 510            |
| Step_0-MinReturn        | 8.48           |
| Step_0-NumTrajs         | 1518           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 54.9           |
| Step_1-AverageDiscou... | 106            |
| Step_1-AveragePolicyStd | 0.46335435     |
| Step_1-AverageReturn    | 201            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 448            |
| Step_1-MinReturn        | 10.5           |
| Step_1-NumTrajs         | 1381           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 64.8           |
| Time                    | 1.16e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.9           |
| Time-OuterStep          | 13.9           |
| Time-SampleProc         | 1.03           |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.006149282    |
| n_timesteps             | 56960000       |
--------------------------------------------

 ---------------- Iteration 178 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 178            |
| ItrTime                 | 68.6           |
| LossAfter               | -0.006718315   |
| LossBefore              | 2.8565532e-09  |
| MeanKL                  | 0.0067347526   |
| MeanKLBefore            | -1.4901229e-09 |
| Step_0-AverageDiscou... | 105            |
| Step_0-AveragePolicyStd | 0.46213913     |
| Step_0-AverageReturn    | 189            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 439            |
| Step_0-MinReturn        | 14.6           |
| Step_0-NumTrajs         | 1559           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 59.6           |
| Step_1-AverageDiscou... | 104            |
| Step_1-AveragePolicyStd | 0.4621373      |
| Step_1-AverageReturn    | 191            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 498            |
| Step_1-MinReturn        | 9.41           |
| Step_1-NumTrajs         | 1472           |
| Step_1-PolicyExecTime   | 3.89           |
| Step_1-StdReturn        | 66.7           |
| Time                    | 1.17e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 1.06           |
| Time-Sampling           | 53.7           |
| Time-TotalInner         | 55             |
| dLoss                   | 0.006718318    |
| n_timesteps             | 57280000       |
--------------------------------------------

 ---------------- Iteration 179 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 179           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.0070321085 |
| LossBefore              | 2.8720387e-10 |
| MeanKL                  | 0.007011056   |
| MeanKLBefore            | 5.9622387e-09 |
| Step_0-AverageDiscou... | 105           |
| Step_0-AveragePolicyStd | 0.46036044    |
| Step_0-AverageReturn    | 190           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 435           |
| Step_0-MinReturn        | 15.6          |
| Step_0-NumTrajs         | 1520          |
| Step_0-PolicyExecTime   | 1.23          |
| Step_0-StdReturn        | 64.4          |
| Step_1-AverageDiscou... | 105           |
| Step_1-AveragePolicyStd | 0.46013784    |
| Step_1-AverageReturn    | 194           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 501           |
| Step_1-MinReturn        | 7.79          |
| Step_1-NumTrajs         | 1453          |
| Step_1-PolicyExecTime   | 3.85          |
| Step_1-StdReturn        | 67.8          |
| Time                    | 1.18e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 53.8          |
| Time-TotalInner         | 55            |
| dLoss                   | 0.007032109   |
| n_timesteps             | 57600000      |
-------------------------------------------

 ---------------- Iteration 180 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 180           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.006922035  |
| LossBefore              | 4.172081e-09  |
| MeanKL                  | 0.006964323   |
| MeanKLBefore            | 1.6390198e-08 |
| Step_0-AverageDiscou... | 102           |
| Step_0-AveragePolicyStd | 0.45939103    |
| Step_0-AverageReturn    | 180           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 402           |
| Step_0-MinReturn        | 16.7          |
| Step_0-NumTrajs         | 1599          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 61.8          |
| Step_1-AverageDiscou... | 103           |
| Step_1-AveragePolicyStd | 0.45929417    |
| Step_1-AverageReturn    | 186           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 535           |
| Step_1-MinReturn        | 18.6          |
| Step_1-NumTrajs         | 1518          |
| Step_1-PolicyExecTime   | 4.05          |
| Step_1-StdReturn        | 65.5          |
| Time                    | 1.18e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 14            |
| Time-OuterStep          | 14            |
| Time-SampleProc         | 1.15          |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.006922039   |
| n_timesteps             | 57920000      |
-------------------------------------------

 ---------------- Iteration 181 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 181           |
| ItrTime                 | 68.2          |
| LossAfter               | -0.00679057   |
| LossBefore              | 4.126074e-09  |
| MeanKL                  | 0.0067124106  |
| MeanKLBefore            | 1.0427837e-08 |
| Step_0-AverageDiscou... | 104           |
| Step_0-AveragePolicyStd | 0.45878077    |
| Step_0-AverageReturn    | 183           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 422           |
| Step_0-MinReturn        | 9.78          |
| Step_0-NumTrajs         | 1590          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 55.6          |
| Step_1-AverageDiscou... | 105           |
| Step_1-AveragePolicyStd | 0.45855954    |
| Step_1-AverageReturn    | 190           |
| Step_1-EnvExecTime      | 22.5          |
| Step_1-MaxReturn        | 483           |
| Step_1-MinReturn        | 10.6          |
| Step_1-NumTrajs         | 1519          |
| Step_1-PolicyExecTime   | 3.81          |
| Step_1-StdReturn        | 61.5          |
| Time                    | 1.19e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 53.2          |
| Time-TotalInner         | 54.5          |
| dLoss                   | 0.006790574   |
| n_timesteps             | 58240000      |
-------------------------------------------

 ---------------- Iteration 182 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 182            |
| ItrTime                 | 68.9           |
| LossAfter               | -0.006597814   |
| LossBefore              | -6.0564624e-09 |
| MeanKL                  | 0.0073884176   |
| MeanKLBefore            | 7.448841e-09   |
| Step_0-AverageDiscou... | 113            |
| Step_0-AveragePolicyStd | 0.45826083     |
| Step_0-AverageReturn    | 200            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 456            |
| Step_0-MinReturn        | 8.06           |
| Step_0-NumTrajs         | 1587           |
| Step_0-PolicyExecTime   | 1.22           |
| Step_0-StdReturn        | 44.6           |
| Step_1-AverageDiscou... | 111            |
| Step_1-AveragePolicyStd | 0.45815325     |
| Step_1-AverageReturn    | 211            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 460            |
| Step_1-MinReturn        | 10.8           |
| Step_1-NumTrajs         | 1372           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 61.3           |
| Time                    | 1.2e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 14.2           |
| Time-OuterStep          | 14.2           |
| Time-SampleProc         | 1.05           |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.7           |
| dLoss                   | 0.006597808    |
| n_timesteps             | 58560000       |
--------------------------------------------

 ---------------- Iteration 183 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 183           |
| ItrTime                 | 68.6          |
| LossAfter               | -0.0069368943 |
| LossBefore              | -7.132347e-09 |
| MeanKL                  | 0.0070700287  |
| MeanKLBefore            | 7.4481292e-09 |
| Step_0-AverageDiscou... | 107           |
| Step_0-AveragePolicyStd | 0.4570041     |
| Step_0-AverageReturn    | 189           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 412           |
| Step_0-MinReturn        | 10.6          |
| Step_0-NumTrajs         | 1628          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 52.8          |
| Step_1-AverageDiscou... | 106           |
| Step_1-AveragePolicyStd | 0.45683408    |
| Step_1-AverageReturn    | 190           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 536           |
| Step_1-MinReturn        | 15.3          |
| Step_1-NumTrajs         | 1532          |
| Step_1-PolicyExecTime   | 3.89          |
| Step_1-StdReturn        | 58.7          |
| Time                    | 1.2e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.9          |
| Time-OuterStep          | 13.9          |
| Time-SampleProc         | 1.1           |
| Time-Sampling           | 53.4          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.0069368873  |
| n_timesteps             | 58880000      |
-------------------------------------------

 ---------------- Iteration 184 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 184            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.0065851035  |
| LossBefore              | -6.593628e-09  |
| MeanKL                  | 0.006960395    |
| MeanKLBefore            | -1.4887049e-09 |
| Step_0-AverageDiscou... | 105            |
| Step_0-AveragePolicyStd | 0.45694536     |
| Step_0-AverageReturn    | 189            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 490            |
| Step_0-MinReturn        | 17.5           |
| Step_0-NumTrajs         | 1511           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 61.4           |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.45671618     |
| Step_1-AverageReturn    | 189            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 443            |
| Step_1-MinReturn        | 2.13           |
| Step_1-NumTrajs         | 1372           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 73.5           |
| Time                    | 1.21e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.9           |
| Time-OuterStep          | 13.9           |
| Time-SampleProc         | 1.05           |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.006585097    |
| n_timesteps             | 59200000       |
--------------------------------------------

 ---------------- Iteration 185 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 185           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0069775395 |
| LossBefore              | 1.2558354e-08 |
| MeanKL                  | 0.0071365675  |
| MeanKLBefore            | 1.4888608e-09 |
| Step_0-AverageDiscou... | 107           |
| Step_0-AveragePolicyStd | 0.45635697    |
| Step_0-AverageReturn    | 191           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 462           |
| Step_0-MinReturn        | 10.5          |
| Step_0-NumTrajs         | 1556          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 63.1          |
| Step_1-AverageDiscou... | 107           |
| Step_1-AveragePolicyStd | 0.45623088    |
| Step_1-AverageReturn    | 198           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 492           |
| Step_1-MinReturn        | 2.11          |
| Step_1-NumTrajs         | 1464          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 69.3          |
| Time                    | 1.22e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 14            |
| Time-OuterStep          | 14            |
| Time-SampleProc         | 1.08          |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.006977552   |
| n_timesteps             | 59520000      |
-------------------------------------------

 ---------------- Iteration 186 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 186            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.006962221   |
| LossBefore              | -6.9629935e-10 |
| MeanKL                  | 0.007069574    |
| MeanKLBefore            | -2.9783038e-09 |
| Step_0-AverageDiscou... | 107            |
| Step_0-AveragePolicyStd | 0.455934       |
| Step_0-AverageReturn    | 192            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 390            |
| Step_0-MinReturn        | 3.05           |
| Step_0-NumTrajs         | 1553           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 52.7           |
| Step_1-AverageDiscou... | 107            |
| Step_1-AveragePolicyStd | 0.45583144     |
| Step_1-AverageReturn    | 201            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 439            |
| Step_1-MinReturn        | 6.41           |
| Step_1-NumTrajs         | 1410           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 65             |
| Time                    | 1.22e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 14.1           |
| Time-OuterStep          | 14.1           |
| Time-SampleProc         | 1.1            |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0069622207   |
| n_timesteps             | 59840000       |
--------------------------------------------

 ---------------- Iteration 187 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 187            |
| ItrTime                 | 69             |
| LossAfter               | -0.00732726    |
| LossBefore              | 7.79284e-09    |
| MeanKL                  | 0.0068557397   |
| MeanKLBefore            | -1.4886357e-09 |
| Step_0-AverageDiscou... | 101            |
| Step_0-AveragePolicyStd | 0.4568898      |
| Step_0-AverageReturn    | 177            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 473            |
| Step_0-MinReturn        | 8.17           |
| Step_0-NumTrajs         | 1633           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 68.6           |
| Step_1-AverageDiscou... | 103            |
| Step_1-AveragePolicyStd | 0.45669425     |
| Step_1-AverageReturn    | 188            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 514            |
| Step_1-MinReturn        | -0.233         |
| Step_1-NumTrajs         | 1487           |
| Step_1-PolicyExecTime   | 3.74           |
| Step_1-StdReturn        | 74.6           |
| Time                    | 1.23e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1.05           |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55.3           |
| dLoss                   | 0.007327268    |
| n_timesteps             | 60160000       |
--------------------------------------------

 ---------------- Iteration 188 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 188            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0063685453  |
| LossBefore              | -2.6862814e-09 |
| MeanKL                  | 0.0071041347   |
| MeanKLBefore            | -2.9807263e-09 |
| Step_0-AverageDiscou... | 106            |
| Step_0-AveragePolicyStd | 0.45607138     |
| Step_0-AverageReturn    | 193            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 460            |
| Step_0-MinReturn        | 11             |
| Step_0-NumTrajs         | 1525           |
| Step_0-PolicyExecTime   | 1.23           |
| Step_0-StdReturn        | 56.9           |
| Step_1-AverageDiscou... | 108            |
| Step_1-AveragePolicyStd | 0.45595643     |
| Step_1-AverageReturn    | 204            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 442            |
| Step_1-MinReturn        | 11.6           |
| Step_1-NumTrajs         | 1393           |
| Step_1-PolicyExecTime   | 3.95           |
| Step_1-StdReturn        | 64.6           |
| Time                    | 1.24e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 1.02           |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0063685426   |
| n_timesteps             | 60480000       |
--------------------------------------------

 ---------------- Iteration 189 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 189            |
| ItrTime                 | 68.1           |
| LossAfter               | -0.0074482174  |
| LossBefore              | -1.671622e-09  |
| MeanKL                  | 0.007552533    |
| MeanKLBefore            | -1.4890084e-09 |
| Step_0-AverageDiscou... | 107            |
| Step_0-AveragePolicyStd | 0.4539305      |
| Step_0-AverageReturn    | 193            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 434            |
| Step_0-MinReturn        | 19.3           |
| Step_0-NumTrajs         | 1556           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 52.2           |
| Step_1-AverageDiscou... | 107            |
| Step_1-AveragePolicyStd | 0.45381248     |
| Step_1-AverageReturn    | 202            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 422            |
| Step_1-MinReturn        | 18.6           |
| Step_1-NumTrajs         | 1406           |
| Step_1-PolicyExecTime   | 3.75           |
| Step_1-StdReturn        | 62.5           |
| Time                    | 1.24e+04       |
| Time-InnerStep          | 0.178          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 1              |
| Time-Sampling           | 53.3           |
| Time-TotalInner         | 54.5           |
| dLoss                   | 0.0074482155   |
| n_timesteps             | 60800000       |
--------------------------------------------

 ---------------- Iteration 190 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 190           |
| ItrTime                 | 67            |
| LossAfter               | -0.0069040367 |
| LossBefore              | 4.2313126e-09 |
| MeanKL                  | 0.0070343674  |
| MeanKLBefore            | -8.939689e-09 |
| Step_0-AverageDiscou... | 108           |
| Step_0-AveragePolicyStd | 0.45157853    |
| Step_0-AverageReturn    | 194           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 471           |
| Step_0-MinReturn        | 7.94          |
| Step_0-NumTrajs         | 1557          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 61.4          |
| Step_1-AverageDiscou... | 106           |
| Step_1-AveragePolicyStd | 0.4515806     |
| Step_1-AverageReturn    | 194           |
| Step_1-EnvExecTime      | 22.1          |
| Step_1-MaxReturn        | 481           |
| Step_1-MinReturn        | 18.3          |
| Step_1-NumTrajs         | 1470          |
| Step_1-PolicyExecTime   | 3.69          |
| Step_1-StdReturn        | 65.7          |
| Time                    | 1.25e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.04          |
| Time-Sampling           | 52.2          |
| Time-TotalInner         | 53.5          |
| dLoss                   | 0.006904041   |
| n_timesteps             | 61120000      |
-------------------------------------------

 ---------------- Iteration 191 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 191           |
| ItrTime                 | 67.6          |
| LossAfter               | -0.0068425685 |
| LossBefore              | 5.0924456e-09 |
| MeanKL                  | 0.006908336   |
| MeanKLBefore            | 4.4697073e-09 |
| Step_0-AverageDiscou... | 108           |
| Step_0-AveragePolicyStd | 0.4513798     |
| Step_0-AverageReturn    | 197           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 460           |
| Step_0-MinReturn        | 9.62          |
| Step_0-NumTrajs         | 1512          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 59.2          |
| Step_1-AverageDiscou... | 109           |
| Step_1-AveragePolicyStd | 0.45129085    |
| Step_1-AverageReturn    | 207           |
| Step_1-EnvExecTime      | 22.7          |
| Step_1-MaxReturn        | 471           |
| Step_1-MinReturn        | 6.35          |
| Step_1-NumTrajs         | 1405          |
| Step_1-PolicyExecTime   | 3.77          |
| Step_1-StdReturn        | 65.8          |
| Time                    | 1.26e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.979         |
| Time-Sampling           | 52.9          |
| Time-TotalInner         | 54.1          |
| dLoss                   | 0.0068425736  |
| n_timesteps             | 61440000      |
-------------------------------------------

 ---------------- Iteration 192 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 192            |
| ItrTime                 | 67.9           |
| LossAfter               | -0.0064838245  |
| LossBefore              | -1.8231845e-09 |
| MeanKL                  | 0.0068307957   |
| MeanKLBefore            | -1.0430387e-08 |
| Step_0-AverageDiscou... | 102            |
| Step_0-AveragePolicyStd | 0.45176667     |
| Step_0-AverageReturn    | 187            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 422            |
| Step_0-MinReturn        | 3.74           |
| Step_0-NumTrajs         | 1524           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 73.3           |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.45175007     |
| Step_1-AverageReturn    | 196            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 497            |
| Step_1-MinReturn        | 4.17           |
| Step_1-NumTrajs         | 1358           |
| Step_1-PolicyExecTime   | 3.77           |
| Step_1-StdReturn        | 87.2           |
| Time                    | 1.27e+04       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.973          |
| Time-Sampling           | 53.2           |
| Time-TotalInner         | 54.3           |
| dLoss                   | 0.0064838226   |
| n_timesteps             | 61760000       |
--------------------------------------------

 ---------------- Iteration 193 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 193            |
| ItrTime                 | 67.8           |
| LossAfter               | -0.0071484325  |
| LossBefore              | -8.156026e-09  |
| MeanKL                  | 0.0072533377   |
| MeanKLBefore            | -4.4699515e-09 |
| Step_0-AverageDiscou... | 110            |
| Step_0-AveragePolicyStd | 0.4513654      |
| Step_0-AverageReturn    | 200            |
| Step_0-EnvExecTime      | 22.3           |
| Step_0-MaxReturn        | 447            |
| Step_0-MinReturn        | 14.5           |
| Step_0-NumTrajs         | 1504           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 58.9           |
| Step_1-AverageDiscou... | 108            |
| Step_1-AveragePolicyStd | 0.45133433     |
| Step_1-AverageReturn    | 202            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 500            |
| Step_1-MinReturn        | 17.7           |
| Step_1-NumTrajs         | 1424           |
| Step_1-PolicyExecTime   | 3.77           |
| Step_1-StdReturn        | 65.5           |
| Time                    | 1.27e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.98           |
| Time-Sampling           | 53             |
| Time-TotalInner         | 54.2           |
| dLoss                   | 0.007148424    |
| n_timesteps             | 62080000       |
--------------------------------------------

 ---------------- Iteration 194 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 194            |
| ItrTime                 | 68.7           |
| LossAfter               | -0.0069704913  |
| LossBefore              | -3.2431218e-09 |
| MeanKL                  | 0.0069829165   |
| MeanKLBefore            | 2.9816813e-09  |
| Step_0-AverageDiscou... | 108            |
| Step_0-AveragePolicyStd | 0.45135495     |
| Step_0-AverageReturn    | 196            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 417            |
| Step_0-MinReturn        | 11.7           |
| Step_0-NumTrajs         | 1526           |
| Step_0-PolicyExecTime   | 1.2            |
| Step_0-StdReturn        | 61.3           |
| Step_1-AverageDiscou... | 108            |
| Step_1-AveragePolicyStd | 0.45122665     |
| Step_1-AverageReturn    | 208            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 461            |
| Step_1-MinReturn        | 6.14           |
| Step_1-NumTrajs         | 1362           |
| Step_1-PolicyExecTime   | 3.85           |
| Step_1-StdReturn        | 75.2           |
| Time                    | 1.28e+04       |
| Time-InnerStep          | 0.158          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.981          |
| Time-Sampling           | 53.9           |
| Time-TotalInner         | 55.1           |
| dLoss                   | 0.006970488    |
| n_timesteps             | 62400000       |
--------------------------------------------

 ---------------- Iteration 195 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 195          |
| ItrTime                 | 66.7         |
| LossAfter               | -0.006724324 |
| LossBefore              | 3.242731e-09 |
| MeanKL                  | 0.0066376524 |
| MeanKLBefore            | 5.963898e-09 |
| Step_0-AverageDiscou... | 111          |
| Step_0-AveragePolicyStd | 0.4511239    |
| Step_0-AverageReturn    | 199          |
| Step_0-EnvExecTime      | 21.9         |
| Step_0-MaxReturn        | 479          |
| Step_0-MinReturn        | 12.3         |
| Step_0-NumTrajs         | 1575         |
| Step_0-PolicyExecTime   | 1.18         |
| Step_0-StdReturn        | 49.7         |
| Step_1-AverageDiscou... | 109          |
| Step_1-AveragePolicyStd | 0.4509506    |
| Step_1-AverageReturn    | 202          |
| Step_1-EnvExecTime      | 22           |
| Step_1-MaxReturn        | 480          |
| Step_1-MinReturn        | 5.42         |
| Step_1-NumTrajs         | 1463         |
| Step_1-PolicyExecTime   | 3.7          |
| Step_1-StdReturn        | 57.6         |
| Time                    | 1.29e+04     |
| Time-InnerStep          | 0.16         |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 1            |
| Time-Sampling           | 51.9         |
| Time-TotalInner         | 53.1         |
| dLoss                   | 0.0067243273 |
| n_timesteps             | 62720000     |
------------------------------------------

 ---------------- Iteration 196 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 196            |
| ItrTime                 | 67.3           |
| LossAfter               | -0.00736413    |
| LossBefore              | -3.8152663e-09 |
| MeanKL                  | 0.006999854    |
| MeanKLBefore            | 1.0429884e-08  |
| Step_0-AverageDiscou... | 111            |
| Step_0-AveragePolicyStd | 0.4503415      |
| Step_0-AverageReturn    | 201            |
| Step_0-EnvExecTime      | 22.2           |
| Step_0-MaxReturn        | 543            |
| Step_0-MinReturn        | 22.8           |
| Step_0-NumTrajs         | 1511           |
| Step_0-PolicyExecTime   | 1.2            |
| Step_0-StdReturn        | 63.1           |
| Step_1-AverageDiscou... | 110            |
| Step_1-AveragePolicyStd | 0.45007226     |
| Step_1-AverageReturn    | 208            |
| Step_1-EnvExecTime      | 22.3           |
| Step_1-MaxReturn        | 557            |
| Step_1-MinReturn        | -4.08          |
| Step_1-NumTrajs         | 1385           |
| Step_1-PolicyExecTime   | 3.79           |
| Step_1-StdReturn        | 71.4           |
| Time                    | 1.29e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.974          |
| Time-Sampling           | 52.6           |
| Time-TotalInner         | 53.8           |
| dLoss                   | 0.0073641264   |
| n_timesteps             | 63040000       |
--------------------------------------------

 ---------------- Iteration 197 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 197           |
| ItrTime                 | 67.3          |
| LossAfter               | -0.006867416  |
| LossBefore              | 3.5968157e-09 |
| MeanKL                  | 0.0072382344  |
| MeanKLBefore            | -7.451083e-09 |
| Step_0-AverageDiscou... | 104           |
| Step_0-AveragePolicyStd | 0.45010844    |
| Step_0-AverageReturn    | 187           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 469           |
| Step_0-MinReturn        | 4.82          |
| Step_0-NumTrajs         | 1567          |
| Step_0-PolicyExecTime   | 1.2           |
| Step_0-StdReturn        | 70.1          |
| Step_1-AverageDiscou... | 106           |
| Step_1-AveragePolicyStd | 0.4499854     |
| Step_1-AverageReturn    | 196           |
| Step_1-EnvExecTime      | 22.1          |
| Step_1-MaxReturn        | 465           |
| Step_1-MinReturn        | 3.27          |
| Step_1-NumTrajs         | 1472          |
| Step_1-PolicyExecTime   | 3.73          |
| Step_1-StdReturn        | 69.6          |
| Time                    | 1.3e+04       |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1             |
| Time-Sampling           | 52.5          |
| Time-TotalInner         | 53.7          |
| dLoss                   | 0.00686742    |
| n_timesteps             | 63360000      |
-------------------------------------------

 ---------------- Iteration 198 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 198            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0070140273  |
| LossBefore              | -6.6476873e-09 |
| MeanKL                  | 0.007503821    |
| MeanKLBefore            | -1.0430836e-08 |
| Step_0-AverageDiscou... | 103            |
| Step_0-AveragePolicyStd | 0.45121726     |
| Step_0-AverageReturn    | 190            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 569            |
| Step_0-MinReturn        | 12.8           |
| Step_0-NumTrajs         | 1478           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 78.7           |
| Step_1-AverageDiscou... | 101            |
| Step_1-AveragePolicyStd | 0.451146       |
| Step_1-AverageReturn    | 190            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 511            |
| Step_1-MinReturn        | 12.7           |
| Step_1-NumTrajs         | 1386           |
| Step_1-PolicyExecTime   | 3.85           |
| Step_1-StdReturn        | 86.1           |
| Time                    | 1.31e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.963          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.007014021    |
| n_timesteps             | 63680000       |
--------------------------------------------

 ---------------- Iteration 199 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 199           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0066533773 |
| LossBefore              | 6.6354375e-09 |
| MeanKL                  | 0.0067522875  |
| MeanKLBefore            | 4.4706905e-09 |
| Step_0-AverageDiscou... | 109           |
| Step_0-AveragePolicyStd | 0.45221016    |
| Step_0-AverageReturn    | 208           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 448           |
| Step_0-MinReturn        | 18.9          |
| Step_0-NumTrajs         | 1398          |
| Step_0-PolicyExecTime   | 1.23          |
| Step_0-StdReturn        | 67.9          |
| Step_1-AverageDiscou... | 108           |
| Step_1-AveragePolicyStd | 0.45208293    |
| Step_1-AverageReturn    | 209           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 495           |
| Step_1-MinReturn        | 16            |
| Step_1-NumTrajs         | 1334          |
| Step_1-PolicyExecTime   | 3.87          |
| Step_1-StdReturn        | 72.9          |
| Time                    | 1.31e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.96          |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.006653384   |
| n_timesteps             | 64000000      |
-------------------------------------------

 ---------------- Iteration 200 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 200           |
| ItrTime                 | 67.7          |
| LossAfter               | -0.0069488855 |
| LossBefore              | 7.912504e-09  |
| MeanKL                  | 0.0065965527  |
| MeanKLBefore            | 5.9616054e-09 |
| Step_0-AverageDiscou... | 107           |
| Step_0-AveragePolicyStd | 0.44979012    |
| Step_0-AverageReturn    | 197           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 494           |
| Step_0-MinReturn        | 12.2          |
| Step_0-NumTrajs         | 1505          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 72.6          |
| Step_1-AverageDiscou... | 107           |
| Step_1-AveragePolicyStd | 0.44975606    |
| Step_1-AverageReturn    | 198           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 501           |
| Step_1-MinReturn        | 15.8          |
| Step_1-NumTrajs         | 1442          |
| Step_1-PolicyExecTime   | 3.76          |
| Step_1-StdReturn        | 71.5          |
| Time                    | 1.32e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.985         |
| Time-Sampling           | 52.9          |
| Time-TotalInner         | 54.1          |
| dLoss                   | 0.0069488934  |
| n_timesteps             | 64320000      |
-------------------------------------------

 ---------------- Iteration 201 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 201           |
| ItrTime                 | 68.8          |
| LossAfter               | -0.0071791885 |
| LossBefore              | 3.036753e-09  |
| MeanKL                  | 0.0069815456  |
| MeanKLBefore            | 2.9794731e-09 |
| Step_0-AverageDiscou... | 113           |
| Step_0-AveragePolicyStd | 0.44864315    |
| Step_0-AverageReturn    | 215           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 480           |
| Step_0-MinReturn        | 21.4          |
| Step_0-NumTrajs         | 1413          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 64            |
| Step_1-AverageDiscou... | 111           |
| Step_1-AveragePolicyStd | 0.4485186     |
| Step_1-AverageReturn    | 218           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 517           |
| Step_1-MinReturn        | 9.49          |
| Step_1-NumTrajs         | 1333          |
| Step_1-PolicyExecTime   | 3.83          |
| Step_1-StdReturn        | 71.4          |
| Time                    | 1.33e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.03          |
| Time-Sampling           | 54            |
| Time-TotalInner         | 55.3          |
| dLoss                   | 0.007179192   |
| n_timesteps             | 64640000      |
-------------------------------------------

 ---------------- Iteration 202 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 202           |
| ItrTime                 | 68            |
| LossAfter               | -0.007007788  |
| LossBefore              | 7.3130835e-09 |
| MeanKL                  | 0.007476057   |
| MeanKLBefore            | 1.6389016e-08 |
| Step_0-AverageDiscou... | 111           |
| Step_0-AveragePolicyStd | 0.4475452     |
| Step_0-AverageReturn    | 203           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 539           |
| Step_0-MinReturn        | 13.1          |
| Step_0-NumTrajs         | 1538          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 67.6          |
| Step_1-AverageDiscou... | 109           |
| Step_1-AveragePolicyStd | 0.44744238    |
| Step_1-AverageReturn    | 206           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 548           |
| Step_1-MinReturn        | 11            |
| Step_1-NumTrajs         | 1364          |
| Step_1-PolicyExecTime   | 3.82          |
| Step_1-StdReturn        | 81.9          |
| Time                    | 1.33e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.98          |
| Time-Sampling           | 53.2          |
| Time-TotalInner         | 54.4          |
| dLoss                   | 0.0070077954  |
| n_timesteps             | 64960000      |
-------------------------------------------

 ---------------- Iteration 203 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 203            |
| ItrTime                 | 67.7           |
| LossAfter               | -0.0068206326  |
| LossBefore              | 6.5412236e-09  |
| MeanKL                  | 0.006701447    |
| MeanKLBefore            | -8.9394705e-09 |
| Step_0-AverageDiscou... | 110            |
| Step_0-AveragePolicyStd | 0.44857433     |
| Step_0-AverageReturn    | 197            |
| Step_0-EnvExecTime      | 22.1           |
| Step_0-MaxReturn        | 472            |
| Step_0-MinReturn        | 13.7           |
| Step_0-NumTrajs         | 1602           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 58.1           |
| Step_1-AverageDiscou... | 109            |
| Step_1-AveragePolicyStd | 0.44849578     |
| Step_1-AverageReturn    | 202            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 601            |
| Step_1-MinReturn        | 9.91           |
| Step_1-NumTrajs         | 1442           |
| Step_1-PolicyExecTime   | 3.81           |
| Step_1-StdReturn        | 73.3           |
| Time                    | 1.34e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.996          |
| Time-Sampling           | 52.9           |
| Time-TotalInner         | 54.1           |
| dLoss                   | 0.006820639    |
| n_timesteps             | 65280000       |
--------------------------------------------

 ---------------- Iteration 204 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 204            |
| ItrTime                 | 68.3           |
| LossAfter               | -0.006878299   |
| LossBefore              | 2.7040261e-09  |
| MeanKL                  | 0.0073304237   |
| MeanKLBefore            | -2.9791616e-09 |
| Step_0-AverageDiscou... | 104            |
| Step_0-AveragePolicyStd | 0.44944927     |
| Step_0-AverageReturn    | 189            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 546            |
| Step_0-MinReturn        | 10.4           |
| Step_0-NumTrajs         | 1537           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 73.6           |
| Step_1-AverageDiscou... | 105            |
| Step_1-AveragePolicyStd | 0.44929224     |
| Step_1-AverageReturn    | 195            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 460            |
| Step_1-MinReturn        | 11.9           |
| Step_1-NumTrajs         | 1462           |
| Step_1-PolicyExecTime   | 3.76           |
| Step_1-StdReturn        | 75.9           |
| Time                    | 1.35e+04       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.997          |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.7           |
| dLoss                   | 0.006878302    |
| n_timesteps             | 65600000       |
--------------------------------------------

 ---------------- Iteration 205 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 205           |
| ItrTime                 | 67.8          |
| LossAfter               | -0.0071838563 |
| LossBefore              | 2.7411202e-09 |
| MeanKL                  | 0.007208216   |
| MeanKLBefore            | -4.470315e-09 |
| Step_0-AverageDiscou... | 112           |
| Step_0-AveragePolicyStd | 0.44702554    |
| Step_0-AverageReturn    | 205           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 470           |
| Step_0-MinReturn        | 13.9          |
| Step_0-NumTrajs         | 1529          |
| Step_0-PolicyExecTime   | 1.18          |
| Step_0-StdReturn        | 59.1          |
| Step_1-AverageDiscou... | 109           |
| Step_1-AveragePolicyStd | 0.44692838    |
| Step_1-AverageReturn    | 204           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 474           |
| Step_1-MinReturn        | 11.4          |
| Step_1-NumTrajs         | 1451          |
| Step_1-PolicyExecTime   | 3.76          |
| Step_1-StdReturn        | 68.3          |
| Time                    | 1.35e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.994         |
| Time-Sampling           | 53            |
| Time-TotalInner         | 54.2          |
| dLoss                   | 0.007183859   |
| n_timesteps             | 65920000      |
-------------------------------------------

 ---------------- Iteration 206 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 206           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.0067611397 |
| LossBefore              | 8.759029e-09  |
| MeanKL                  | 0.0069719083  |
| MeanKLBefore            | 4.4691673e-09 |
| Step_0-AverageDiscou... | 106           |
| Step_0-AveragePolicyStd | 0.44719392    |
| Step_0-AverageReturn    | 200           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 624           |
| Step_0-MinReturn        | 15.5          |
| Step_0-NumTrajs         | 1423          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 83.7          |
| Step_1-AverageDiscou... | 107           |
| Step_1-AveragePolicyStd | 0.44715095    |
| Step_1-AverageReturn    | 210           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 568           |
| Step_1-MinReturn        | 9.01          |
| Step_1-NumTrajs         | 1300          |
| Step_1-PolicyExecTime   | 3.84          |
| Step_1-StdReturn        | 91.5          |
| Time                    | 1.36e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.949         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0067611486  |
| n_timesteps             | 66240000      |
-------------------------------------------

 ---------------- Iteration 207 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 207           |
| ItrTime                 | 68.3          |
| LossAfter               | -0.006860558  |
| LossBefore              | 2.9843874e-09 |
| MeanKL                  | 0.006780078   |
| MeanKLBefore            | 1.0430431e-08 |
| Step_0-AverageDiscou... | 109           |
| Step_0-AveragePolicyStd | 0.4468562     |
| Step_0-AverageReturn    | 201           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 471           |
| Step_0-MinReturn        | 8.68          |
| Step_0-NumTrajs         | 1512          |
| Step_0-PolicyExecTime   | 1.21          |
| Step_0-StdReturn        | 70.4          |
| Step_1-AverageDiscou... | 110           |
| Step_1-AveragePolicyStd | 0.44683394    |
| Step_1-AverageReturn    | 211           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 559           |
| Step_1-MinReturn        | 9.61          |
| Step_1-NumTrajs         | 1373          |
| Step_1-PolicyExecTime   | 3.79          |
| Step_1-StdReturn        | 78.4          |
| Time                    | 1.37e+04      |
| Time-InnerStep          | 0.186         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.982         |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.0068605607  |
| n_timesteps             | 66560000      |
-------------------------------------------

 ---------------- Iteration 208 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 208           |
| ItrTime                 | 67.5          |
| LossAfter               | -0.006780076  |
| LossBefore              | 3.6289165e-09 |
| MeanKL                  | 0.0069625108  |
| MeanKLBefore            | 4.471439e-09  |
| Step_0-AverageDiscou... | 113           |
| Step_0-AveragePolicyStd | 0.44602787    |
| Step_0-AverageReturn    | 204           |
| Step_0-EnvExecTime      | 22.2          |
| Step_0-MaxReturn        | 585           |
| Step_0-MinReturn        | 11.4          |
| Step_0-NumTrajs         | 1555          |
| Step_0-PolicyExecTime   | 1.2           |
| Step_0-StdReturn        | 65.7          |
| Step_1-AverageDiscou... | 112           |
| Step_1-AveragePolicyStd | 0.4459821     |
| Step_1-AverageReturn    | 207           |
| Step_1-EnvExecTime      | 22.3          |
| Step_1-MaxReturn        | 510           |
| Step_1-MinReturn        | 16.1          |
| Step_1-NumTrajs         | 1473          |
| Step_1-PolicyExecTime   | 3.78          |
| Step_1-StdReturn        | 71.9          |
| Time                    | 1.37e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 1.02          |
| Time-Sampling           | 52.7          |
| Time-TotalInner         | 53.9          |
| dLoss                   | 0.0067800796  |
| n_timesteps             | 66880000      |
-------------------------------------------

 ---------------- Iteration 209 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 209            |
| ItrTime                 | 68.6           |
| LossAfter               | -0.0071406425  |
| LossBefore              | -1.0828863e-09 |
| MeanKL                  | 0.0074931146   |
| MeanKLBefore            | -1.1920262e-08 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.44451988     |
| Step_0-AverageReturn    | 210            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 532            |
| Step_0-MinReturn        | 13.4           |
| Step_0-NumTrajs         | 1462           |
| Step_0-PolicyExecTime   | 1.23           |
| Step_0-StdReturn        | 67.5           |
| Step_1-AverageDiscou... | 110            |
| Step_1-AveragePolicyStd | 0.444456       |
| Step_1-AverageReturn    | 213            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 657            |
| Step_1-MinReturn        | 17.5           |
| Step_1-NumTrajs         | 1353           |
| Step_1-PolicyExecTime   | 3.77           |
| Step_1-StdReturn        | 78.3           |
| Time                    | 1.38e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.984          |
| Time-Sampling           | 53.8           |
| Time-TotalInner         | 55             |
| dLoss                   | 0.0071406416   |
| n_timesteps             | 67200000       |
--------------------------------------------

 ---------------- Iteration 210 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 210            |
| ItrTime                 | 68.7           |
| LossAfter               | -0.007152289   |
| LossBefore              | -1.507005e-09  |
| MeanKL                  | 0.0071250265   |
| MeanKLBefore            | -4.4695976e-09 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.44499528     |
| Step_0-AverageReturn    | 212            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 630            |
| Step_0-MinReturn        | 12.9           |
| Step_0-NumTrajs         | 1434           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 78.1           |
| Step_1-AverageDiscou... | 111            |
| Step_1-AveragePolicyStd | 0.4448065      |
| Step_1-AverageReturn    | 212            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 526            |
| Step_1-MinReturn        | 1.24           |
| Step_1-NumTrajs         | 1415           |
| Step_1-PolicyExecTime   | 3.78           |
| Step_1-StdReturn        | 80.9           |
| Time                    | 1.39e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.991          |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55.2           |
| dLoss                   | 0.0071522878   |
| n_timesteps             | 67520000       |
--------------------------------------------

 ---------------- Iteration 211 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 211           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0069269836 |
| LossBefore              | 4.737796e-09  |
| MeanKL                  | 0.0068467804  |
| MeanKLBefore            | 2.9812053e-09 |
| Step_0-AverageDiscou... | 108           |
| Step_0-AveragePolicyStd | 0.44493413    |
| Step_0-AverageReturn    | 201           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 574           |
| Step_0-MinReturn        | 7.54          |
| Step_0-NumTrajs         | 1467          |
| Step_0-PolicyExecTime   | 1.2           |
| Step_0-StdReturn        | 82.7          |
| Step_1-AverageDiscou... | 109           |
| Step_1-AveragePolicyStd | 0.44503292    |
| Step_1-AverageReturn    | 210           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 568           |
| Step_1-MinReturn        | 8.26          |
| Step_1-NumTrajs         | 1363          |
| Step_1-PolicyExecTime   | 3.81          |
| Step_1-StdReturn        | 87.7          |
| Time                    | 1.39e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.964         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0069269883  |
| n_timesteps             | 67840000      |
-------------------------------------------

 ---------------- Iteration 212 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 212            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007295242   |
| LossBefore              | -4.624188e-09  |
| MeanKL                  | 0.0073997215   |
| MeanKLBefore            | -2.9800735e-09 |
| Step_0-AverageDiscou... | 106            |
| Step_0-AveragePolicyStd | 0.44292635     |
| Step_0-AverageReturn    | 201            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 431            |
| Step_0-MinReturn        | 6.21           |
| Step_0-NumTrajs         | 1443           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 81.9           |
| Step_1-AverageDiscou... | 109            |
| Step_1-AveragePolicyStd | 0.4426899      |
| Step_1-AverageReturn    | 221            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 498            |
| Step_1-MinReturn        | 11             |
| Step_1-NumTrajs         | 1270           |
| Step_1-PolicyExecTime   | 3.93           |
| Step_1-StdReturn        | 89.9           |
| Time                    | 1.4e+04        |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.943          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0072952374   |
| n_timesteps             | 68160000       |
--------------------------------------------

 ---------------- Iteration 213 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 213           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0067030294 |
| LossBefore              | 3.9382497e-09 |
| MeanKL                  | 0.0073789805  |
| MeanKLBefore            | 1.4883874e-09 |
| Step_0-AverageDiscou... | 107           |
| Step_0-AveragePolicyStd | 0.44237617    |
| Step_0-AverageReturn    | 201           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 520           |
| Step_0-MinReturn        | 10.3          |
| Step_0-NumTrajs         | 1451          |
| Step_0-PolicyExecTime   | 1.21          |
| Step_0-StdReturn        | 78.9          |
| Step_1-AverageDiscou... | 106           |
| Step_1-AveragePolicyStd | 0.44225878    |
| Step_1-AverageReturn    | 203           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 518           |
| Step_1-MinReturn        | 13.2          |
| Step_1-NumTrajs         | 1386          |
| Step_1-PolicyExecTime   | 3.8           |
| Step_1-StdReturn        | 83.6          |
| Time                    | 1.41e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.981         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.006703033   |
| n_timesteps             | 68480000      |
-------------------------------------------

 ---------------- Iteration 214 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 214            |
| ItrTime                 | 68.7           |
| LossAfter               | -0.0068956963  |
| LossBefore              | -6.8936834e-09 |
| MeanKL                  | 0.0075466395   |
| MeanKLBefore            | 7.451222e-09   |
| Step_0-AverageDiscou... | 105            |
| Step_0-AveragePolicyStd | 0.4403731      |
| Step_0-AverageReturn    | 196            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 488            |
| Step_0-MinReturn        | 5.05           |
| Step_0-NumTrajs         | 1471           |
| Step_0-PolicyExecTime   | 1.19           |
| Step_0-StdReturn        | 81.1           |
| Step_1-AverageDiscou... | 106            |
| Step_1-AveragePolicyStd | 0.44026706     |
| Step_1-AverageReturn    | 204            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 537            |
| Step_1-MinReturn        | 7.53           |
| Step_1-NumTrajs         | 1383           |
| Step_1-PolicyExecTime   | 3.76           |
| Step_1-StdReturn        | 87.9           |
| Time                    | 1.42e+04       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.972          |
| Time-Sampling           | 53.9           |
| Time-TotalInner         | 55.1           |
| dLoss                   | 0.0068956893   |
| n_timesteps             | 68800000       |
--------------------------------------------

 ---------------- Iteration 215 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 215           |
| ItrTime                 | 68.3          |
| LossAfter               | -0.0075531965 |
| LossBefore              | 5.0235725e-11 |
| MeanKL                  | 0.0076214694  |
| MeanKLBefore            | 7.450507e-09  |
| Step_0-AverageDiscou... | 113           |
| Step_0-AveragePolicyStd | 0.43906072    |
| Step_0-AverageReturn    | 209           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 517           |
| Step_0-MinReturn        | 14.2          |
| Step_0-NumTrajs         | 1497          |
| Step_0-PolicyExecTime   | 1.19          |
| Step_0-StdReturn        | 68.1          |
| Step_1-AverageDiscou... | 111           |
| Step_1-AveragePolicyStd | 0.43889144    |
| Step_1-AverageReturn    | 214           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 543           |
| Step_1-MinReturn        | 12            |
| Step_1-NumTrajs         | 1377          |
| Step_1-PolicyExecTime   | 3.77          |
| Step_1-StdReturn        | 76            |
| Time                    | 1.42e+04      |
| Time-InnerStep          | 0.159         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.97          |
| Time-Sampling           | 53.6          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0075531965  |
| n_timesteps             | 69120000      |
-------------------------------------------

 ---------------- Iteration 216 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 216           |
| ItrTime                 | 70            |
| LossAfter               | -0.0070314994 |
| LossBefore              | 1.1760911e-09 |
| MeanKL                  | 0.007192068   |
| MeanKLBefore            | -7.447858e-09 |
| Step_0-AverageDiscou... | 107           |
| Step_0-AveragePolicyStd | 0.4375556     |
| Step_0-AverageReturn    | 203           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 554           |
| Step_0-MinReturn        | 7.49          |
| Step_0-NumTrajs         | 1431          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 90            |
| Step_1-AverageDiscou... | 108           |
| Step_1-AveragePolicyStd | 0.43761238    |
| Step_1-AverageReturn    | 209           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 584           |
| Step_1-MinReturn        | 7.39          |
| Step_1-NumTrajs         | 1378          |
| Step_1-PolicyExecTime   | 3.81          |
| Step_1-StdReturn        | 87            |
| Time                    | 1.43e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.939         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007031501   |
| n_timesteps             | 69440000      |
-------------------------------------------

 ---------------- Iteration 217 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 217           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.0066052973 |
| LossBefore              | -8.963669e-09 |
| MeanKL                  | 0.007291357   |
| MeanKLBefore            | -4.468782e-09 |
| Step_0-AverageDiscou... | 114           |
| Step_0-AveragePolicyStd | 0.4377421     |
| Step_0-AverageReturn    | 217           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 528           |
| Step_0-MinReturn        | 14.9          |
| Step_0-NumTrajs         | 1434          |
| Step_0-PolicyExecTime   | 1.21          |
| Step_0-StdReturn        | 78.2          |
| Step_1-AverageDiscou... | 112           |
| Step_1-AveragePolicyStd | 0.43765554    |
| Step_1-AverageReturn    | 217           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 500           |
| Step_1-MinReturn        | 3.36          |
| Step_1-NumTrajs         | 1361          |
| Step_1-PolicyExecTime   | 3.81          |
| Step_1-StdReturn        | 81.5          |
| Time                    | 1.44e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.946         |
| Time-Sampling           | 53.6          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0066052885  |
| n_timesteps             | 69760000      |
-------------------------------------------

 ---------------- Iteration 218 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 218            |
| ItrTime                 | 68.3           |
| LossAfter               | -0.0070619546  |
| LossBefore              | -8.5483266e-11 |
| MeanKL                  | 0.007968599    |
| MeanKLBefore            | -1.4916235e-09 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.436582       |
| Step_0-AverageReturn    | 210            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 490            |
| Step_0-MinReturn        | 15.1           |
| Step_0-NumTrajs         | 1447           |
| Step_0-PolicyExecTime   | 1.22           |
| Step_0-StdReturn        | 80.5           |
| Step_1-AverageDiscou... | 108            |
| Step_1-AveragePolicyStd | 0.4365432      |
| Step_1-AverageReturn    | 205            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 506            |
| Step_1-MinReturn        | 15.6           |
| Step_1-NumTrajs         | 1380           |
| Step_1-PolicyExecTime   | 3.76           |
| Step_1-StdReturn        | 84             |
| Time                    | 1.44e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.968          |
| Time-Sampling           | 53.5           |
| Time-TotalInner         | 54.7           |
| dLoss                   | 0.0070619546   |
| n_timesteps             | 70080000       |
--------------------------------------------

 ---------------- Iteration 219 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 219            |
| ItrTime                 | 67             |
| LossAfter               | -0.006885958   |
| LossBefore              | 3.2537764e-09  |
| MeanKL                  | 0.0070251287   |
| MeanKLBefore            | -1.7877856e-08 |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.43708864     |
| Step_0-AverageReturn    | 217            |
| Step_0-EnvExecTime      | 22             |
| Step_0-MaxReturn        | 559            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 1469           |
| Step_0-PolicyExecTime   | 1.2            |
| Step_0-StdReturn        | 70.9           |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.43705466     |
| Step_1-AverageReturn    | 218            |
| Step_1-EnvExecTime      | 22.1           |
| Step_1-MaxReturn        | 610            |
| Step_1-MinReturn        | 18.8           |
| Step_1-NumTrajs         | 1435           |
| Step_1-PolicyExecTime   | 3.73           |
| Step_1-StdReturn        | 73.5           |
| Time                    | 1.45e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.989          |
| Time-Sampling           | 52.2           |
| Time-TotalInner         | 53.4           |
| dLoss                   | 0.006885961    |
| n_timesteps             | 70400000       |
--------------------------------------------

 ---------------- Iteration 220 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 220           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.007088183  |
| LossBefore              | 5.795676e-09  |
| MeanKL                  | 0.0068053333  |
| MeanKLBefore            | -5.960082e-09 |
| Step_0-AverageDiscou... | 114           |
| Step_0-AveragePolicyStd | 0.436365      |
| Step_0-AverageReturn    | 216           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 641           |
| Step_0-MinReturn        | 9.2           |
| Step_0-NumTrajs         | 1400          |
| Step_0-PolicyExecTime   | 1.21          |
| Step_0-StdReturn        | 67.8          |
| Step_1-AverageDiscou... | 115           |
| Step_1-AveragePolicyStd | 0.43638676    |
| Step_1-AverageReturn    | 220           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 515           |
| Step_1-MinReturn        | 23.5          |
| Step_1-NumTrajs         | 1403          |
| Step_1-PolicyExecTime   | 3.72          |
| Step_1-StdReturn        | 64.5          |
| Time                    | 1.46e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 1.12          |
| Time-Sampling           | 53.5          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0070881885  |
| n_timesteps             | 70720000      |
-------------------------------------------

 ---------------- Iteration 221 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 221           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.007058355  |
| LossBefore              | -7.844029e-09 |
| MeanKL                  | 0.0074417656  |
| MeanKLBefore            | 8.941027e-09  |
| Step_0-AverageDiscou... | 112           |
| Step_0-AveragePolicyStd | 0.43598676    |
| Step_0-AverageReturn    | 223           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 569           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 1327          |
| Step_0-PolicyExecTime   | 1.25          |
| Step_0-StdReturn        | 90.4          |
| Step_1-AverageDiscou... | 111           |
| Step_1-AveragePolicyStd | 0.43581563    |
| Step_1-AverageReturn    | 227           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 579           |
| Step_1-MinReturn        | 14            |
| Step_1-NumTrajs         | 1271          |
| Step_1-PolicyExecTime   | 3.88          |
| Step_1-StdReturn        | 91.8          |
| Time                    | 1.46e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.927         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007058347   |
| n_timesteps             | 71040000      |
-------------------------------------------

 ---------------- Iteration 222 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 222          |
| ItrTime                 | 70.6         |
| LossAfter               | -0.006880913 |
| LossBefore              | 8.105772e-10 |
| MeanKL                  | 0.0070495745 |
| MeanKLBefore            | 7.449015e-09 |
| Step_0-AverageDiscou... | 109          |
| Step_0-AveragePolicyStd | 0.4363088    |
| Step_0-AverageReturn    | 214          |
| Step_0-EnvExecTime      | 23.9         |
| Step_0-MaxReturn        | 505          |
| Step_0-MinReturn        | 5.68         |
| Step_0-NumTrajs         | 1362         |
| Step_0-PolicyExecTime   | 1.25         |
| Step_0-StdReturn        | 92.5         |
| Step_1-AverageDiscou... | 110          |
| Step_1-AveragePolicyStd | 0.4361896    |
| Step_1-AverageReturn    | 217          |
| Step_1-EnvExecTime      | 23.6         |
| Step_1-MaxReturn        | 564          |
| Step_1-MinReturn        | 4.5          |
| Step_1-NumTrajs         | 1331         |
| Step_1-PolicyExecTime   | 3.82         |
| Step_1-StdReturn        | 94.8         |
| Time                    | 1.47e+04     |
| Time-InnerStep          | 0.161        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.946        |
| Time-Sampling           | 55.8         |
| Time-TotalInner         | 57           |
| dLoss                   | 0.006880914  |
| n_timesteps             | 71360000     |
------------------------------------------

 ---------------- Iteration 223 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 223           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0070792446 |
| LossBefore              | -9.534274e-09 |
| MeanKL                  | 0.007038211   |
| MeanKLBefore            | -7.450884e-09 |
| Step_0-AverageDiscou... | 109           |
| Step_0-AveragePolicyStd | 0.4361489     |
| Step_0-AverageReturn    | 217           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 549           |
| Step_0-MinReturn        | 19.7          |
| Step_0-NumTrajs         | 1323          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 101           |
| Step_1-AverageDiscou... | 108           |
| Step_1-AveragePolicyStd | 0.4360847     |
| Step_1-AverageReturn    | 218           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 487           |
| Step_1-MinReturn        | 21.2          |
| Step_1-NumTrajs         | 1260          |
| Step_1-PolicyExecTime   | 3.89          |
| Step_1-StdReturn        | 97.6          |
| Time                    | 1.48e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.914         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0070792353  |
| n_timesteps             | 71680000      |
-------------------------------------------

 ---------------- Iteration 224 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 224           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0069354316 |
| LossBefore              | 5.7371525e-09 |
| MeanKL                  | 0.0067905076  |
| MeanKLBefore            | 1.4872743e-09 |
| Step_0-AverageDiscou... | 112           |
| Step_0-AveragePolicyStd | 0.43687937    |
| Step_0-AverageReturn    | 214           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 532           |
| Step_0-MinReturn        | 15.7          |
| Step_0-NumTrajs         | 1408          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 75.6          |
| Step_1-AverageDiscou... | 110           |
| Step_1-AveragePolicyStd | 0.4367918     |
| Step_1-AverageReturn    | 214           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 514           |
| Step_1-MinReturn        | 14.9          |
| Step_1-NumTrajs         | 1358          |
| Step_1-PolicyExecTime   | 3.82          |
| Step_1-StdReturn        | 77.2          |
| Time                    | 1.48e+04      |
| Time-InnerStep          | 0.188         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.994         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.006935437   |
| n_timesteps             | 72000000      |
-------------------------------------------

 ---------------- Iteration 225 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 225           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.006904154  |
| LossBefore              | 1.8006556e-09 |
| MeanKL                  | 0.007528095   |
| MeanKLBefore            | 1.1918728e-08 |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.43549576    |
| Step_0-AverageReturn    | 220           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 554           |
| Step_0-MinReturn        | 20.5          |
| Step_0-NumTrajs         | 1436          |
| Step_0-PolicyExecTime   | 1.22          |
| Step_0-StdReturn        | 76.6          |
| Step_1-AverageDiscou... | 114           |
| Step_1-AveragePolicyStd | 0.43526986    |
| Step_1-AverageReturn    | 222           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 510           |
| Step_1-MinReturn        | 23.6          |
| Step_1-NumTrajs         | 1347          |
| Step_1-PolicyExecTime   | 3.81          |
| Step_1-StdReturn        | 76.3          |
| Time                    | 1.49e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.968         |
| Time-Sampling           | 54.3          |
| Time-TotalInner         | 55.4          |
| dLoss                   | 0.006904156   |
| n_timesteps             | 72320000      |
-------------------------------------------

 ---------------- Iteration 226 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 226            |
| ItrTime                 | 68.5           |
| LossAfter               | -0.006879325   |
| LossBefore              | -3.2583571e-09 |
| MeanKL                  | 0.0071116523   |
| MeanKLBefore            | 1.04294875e-08 |
| Step_0-AverageDiscou... | 111            |
| Step_0-AveragePolicyStd | 0.43489242     |
| Step_0-AverageReturn    | 208            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 644            |
| Step_0-MinReturn        | 1.59           |
| Step_0-NumTrajs         | 1461           |
| Step_0-PolicyExecTime   | 1.21           |
| Step_0-StdReturn        | 77.5           |
| Step_1-AverageDiscou... | 109            |
| Step_1-AveragePolicyStd | 0.43478945     |
| Step_1-AverageReturn    | 210            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 527            |
| Step_1-MinReturn        | 2.14           |
| Step_1-NumTrajs         | 1353           |
| Step_1-PolicyExecTime   | 3.82           |
| Step_1-StdReturn        | 85.8           |
| Time                    | 1.5e+04        |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.982          |
| Time-Sampling           | 53.8           |
| Time-TotalInner         | 55             |
| dLoss                   | 0.0068793218   |
| n_timesteps             | 72640000       |
--------------------------------------------

 ---------------- Iteration 227 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 227           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0071489452 |
| LossBefore              | 1.1818472e-09 |
| MeanKL                  | 0.0073854104  |
| MeanKLBefore            | 4.468873e-09  |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.43375522    |
| Step_0-AverageReturn    | 231           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 563           |
| Step_0-MinReturn        | 14.6          |
| Step_0-NumTrajs         | 1292          |
| Step_0-PolicyExecTime   | 1.25          |
| Step_0-StdReturn        | 80.5          |
| Step_1-AverageDiscou... | 118           |
| Step_1-AveragePolicyStd | 0.43382913    |
| Step_1-AverageReturn    | 234           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 566           |
| Step_1-MinReturn        | 20.8          |
| Step_1-NumTrajs         | 1306          |
| Step_1-PolicyExecTime   | 3.95          |
| Step_1-StdReturn        | 79.1          |
| Time                    | 1.51e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.917         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0071489466  |
| n_timesteps             | 72960000      |
-------------------------------------------

 ---------------- Iteration 228 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 228           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.006960012  |
| LossBefore              | 5.0214024e-09 |
| MeanKL                  | 0.007106631   |
| MeanKLBefore            | -7.44938e-09  |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.4333725     |
| Step_0-AverageReturn    | 237           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 535           |
| Step_0-MinReturn        | 18.4          |
| Step_0-NumTrajs         | 1236          |
| Step_0-PolicyExecTime   | 1.26          |
| Step_0-StdReturn        | 92.5          |
| Step_1-AverageDiscou... | 117           |
| Step_1-AveragePolicyStd | 0.433203      |
| Step_1-AverageReturn    | 239           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 561           |
| Step_1-MinReturn        | 10.7          |
| Step_1-NumTrajs         | 1255          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 89.8          |
| Time                    | 1.51e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.944         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.006960017   |
| n_timesteps             | 73280000      |
-------------------------------------------

 ---------------- Iteration 229 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 229           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0071280757 |
| LossBefore              | 6.0520355e-10 |
| MeanKL                  | 0.007809016   |
| MeanKLBefore            | 2.0860444e-08 |
| Step_0-AverageDiscou... | 111           |
| Step_0-AveragePolicyStd | 0.43281418    |
| Step_0-AverageReturn    | 228           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 589           |
| Step_0-MinReturn        | 10.3          |
| Step_0-NumTrajs         | 1228          |
| Step_0-PolicyExecTime   | 1.27          |
| Step_0-StdReturn        | 98.7          |
| Step_1-AverageDiscou... | 110           |
| Step_1-AveragePolicyStd | 0.43284208    |
| Step_1-AverageReturn    | 226           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 533           |
| Step_1-MinReturn        | 11.9          |
| Step_1-NumTrajs         | 1238          |
| Step_1-PolicyExecTime   | 3.95          |
| Step_1-StdReturn        | 105           |
| Time                    | 1.52e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.943         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007128076   |
| n_timesteps             | 73600000      |
-------------------------------------------

 ---------------- Iteration 230 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 230            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.006597957   |
| LossBefore              | 4.08602e-09    |
| MeanKL                  | 0.007280449    |
| MeanKLBefore            | -5.9598757e-09 |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.43303418     |
| Step_0-AverageReturn    | 229            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 522            |
| Step_0-MinReturn        | 23.4           |
| Step_0-NumTrajs         | 1308           |
| Step_0-PolicyExecTime   | 1.25           |
| Step_0-StdReturn        | 85.2           |
| Step_1-AverageDiscou... | 116            |
| Step_1-AveragePolicyStd | 0.43308055     |
| Step_1-AverageReturn    | 228            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 529            |
| Step_1-MinReturn        | 29.1           |
| Step_1-NumTrajs         | 1327           |
| Step_1-PolicyExecTime   | 3.85           |
| Step_1-StdReturn        | 81             |
| Time                    | 1.53e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.957          |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0065979613   |
| n_timesteps             | 73920000       |
--------------------------------------------

 ---------------- Iteration 231 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 231            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007110028   |
| LossBefore              | 2.257703e-09   |
| MeanKL                  | 0.0069628907   |
| MeanKLBefore            | -5.9589427e-09 |
| Step_0-AverageDiscou... | 108            |
| Step_0-AveragePolicyStd | 0.43278372     |
| Step_0-AverageReturn    | 212            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 506            |
| Step_0-MinReturn        | 9              |
| Step_0-NumTrajs         | 1351           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 97.6           |
| Step_1-AverageDiscou... | 109            |
| Step_1-AveragePolicyStd | 0.43259588     |
| Step_1-AverageReturn    | 214            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 556            |
| Step_1-MinReturn        | 12.8           |
| Step_1-NumTrajs         | 1362           |
| Step_1-PolicyExecTime   | 3.92           |
| Step_1-StdReturn        | 99             |
| Time                    | 1.53e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.974          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0071100304   |
| n_timesteps             | 74240000       |
--------------------------------------------

 ---------------- Iteration 232 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 232            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.007175952   |
| LossBefore              | -3.900395e-09  |
| MeanKL                  | 0.0070332214   |
| MeanKLBefore            | -4.4692157e-09 |
| Step_0-AverageDiscou... | 113            |
| Step_0-AveragePolicyStd | 0.43257385     |
| Step_0-AverageReturn    | 228            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 555            |
| Step_0-MinReturn        | 14.4           |
| Step_0-NumTrajs         | 1276           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 98             |
| Step_1-AverageDiscou... | 111            |
| Step_1-AveragePolicyStd | 0.43249172     |
| Step_1-AverageReturn    | 218            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 529            |
| Step_1-MinReturn        | 13.6           |
| Step_1-NumTrajs         | 1316           |
| Step_1-PolicyExecTime   | 3.9            |
| Step_1-StdReturn        | 89.3           |
| Time                    | 1.54e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.962          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.0071759485   |
| n_timesteps             | 74560000       |
--------------------------------------------

 ---------------- Iteration 233 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 233           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0069429865 |
| LossBefore              | 8.232904e-09  |
| MeanKL                  | 0.007462059   |
| MeanKLBefore            | 1.6389768e-08 |
| Step_0-AverageDiscou... | 115           |
| Step_0-AveragePolicyStd | 0.43321347    |
| Step_0-AverageReturn    | 226           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 534           |
| Step_0-MinReturn        | 23.4          |
| Step_0-NumTrajs         | 1310          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 84.6          |
| Step_1-AverageDiscou... | 117           |
| Step_1-AveragePolicyStd | 0.43317094    |
| Step_1-AverageReturn    | 234           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 569           |
| Step_1-MinReturn        | 12.5          |
| Step_1-NumTrajs         | 1285          |
| Step_1-PolicyExecTime   | 3.88          |
| Step_1-StdReturn        | 83.6          |
| Time                    | 1.55e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.972         |
| Time-Sampling           | 54.4          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.006942995   |
| n_timesteps             | 74880000      |
-------------------------------------------

 ---------------- Iteration 234 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 234           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.0071358345 |
| LossBefore              | 2.2166582e-09 |
| MeanKL                  | 0.0065781353  |
| MeanKLBefore            | 1.4891859e-09 |
| Step_0-AverageDiscou... | 114           |
| Step_0-AveragePolicyStd | 0.43217504    |
| Step_0-AverageReturn    | 225           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 570           |
| Step_0-MinReturn        | 11.5          |
| Step_0-NumTrajs         | 1315          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 89.4          |
| Step_1-AverageDiscou... | 118           |
| Step_1-AveragePolicyStd | 0.43205822    |
| Step_1-AverageReturn    | 239           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 555           |
| Step_1-MinReturn        | 15            |
| Step_1-NumTrajs         | 1249          |
| Step_1-PolicyExecTime   | 4.05          |
| Step_1-StdReturn        | 85.9          |
| Time                    | 1.55e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 1             |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.007135837   |
| n_timesteps             | 75200000      |
-------------------------------------------

 ---------------- Iteration 235 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 235           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0071788197 |
| LossBefore              | 2.9430856e-09 |
| MeanKL                  | 0.0077967     |
| MeanKLBefore            | 8.940169e-09  |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.4312214     |
| Step_0-AverageReturn    | 239           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 553           |
| Step_0-MinReturn        | 5.96          |
| Step_0-NumTrajs         | 1264          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 90.7          |
| Step_1-AverageDiscou... | 118           |
| Step_1-AveragePolicyStd | 0.43114236    |
| Step_1-AverageReturn    | 237           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 683           |
| Step_1-MinReturn        | 9.55          |
| Step_1-NumTrajs         | 1289          |
| Step_1-PolicyExecTime   | 4.05          |
| Step_1-StdReturn        | 89.2          |
| Time                    | 1.56e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.986         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.0071788225  |
| n_timesteps             | 75520000      |
-------------------------------------------

 ---------------- Iteration 236 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 236            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0068981103  |
| LossBefore              | 5.6809046e-10  |
| MeanKL                  | 0.0070594875   |
| MeanKLBefore            | -2.2349777e-08 |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.43149242     |
| Step_0-AverageReturn    | 241            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 500            |
| Step_0-MinReturn        | 11.4           |
| Step_0-NumTrajs         | 1292           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 80.1           |
| Step_1-AverageDiscou... | 120            |
| Step_1-AveragePolicyStd | 0.4313624      |
| Step_1-AverageReturn    | 236            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 542            |
| Step_1-MinReturn        | 6.23           |
| Step_1-NumTrajs         | 1307           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 78.7           |
| Time                    | 1.57e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.956          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.0068981107   |
| n_timesteps             | 75840000       |
--------------------------------------------

 ---------------- Iteration 237 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 237           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.007396196  |
| LossBefore              | 6.0348144e-09 |
| MeanKL                  | 0.0071394416  |
| MeanKLBefore            | -4.468599e-09 |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.4305118     |
| Step_0-AverageReturn    | 234           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 605           |
| Step_0-MinReturn        | 7.18          |
| Step_0-NumTrajs         | 1274          |
| Step_0-PolicyExecTime   | 1.69          |
| Step_0-StdReturn        | 91.8          |
| Step_1-AverageDiscou... | 117           |
| Step_1-AveragePolicyStd | 0.43041366    |
| Step_1-AverageReturn    | 236           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 535           |
| Step_1-MinReturn        | 24.6          |
| Step_1-NumTrajs         | 1282          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 91.9          |
| Time                    | 1.58e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.978         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.007396202   |
| n_timesteps             | 76160000      |
-------------------------------------------

 ---------------- Iteration 238 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 238            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.007245455   |
| LossBefore              | -6.86902e-09   |
| MeanKL                  | 0.0069731334   |
| MeanKLBefore            | -2.9805993e-09 |
| Step_0-AverageDiscou... | 114            |
| Step_0-AveragePolicyStd | 0.43007517     |
| Step_0-AverageReturn    | 227            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 530            |
| Step_0-MinReturn        | 14             |
| Step_0-NumTrajs         | 1264           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 92.7           |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.42998508     |
| Step_1-AverageReturn    | 226            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 538            |
| Step_1-MinReturn        | 27.4           |
| Step_1-NumTrajs         | 1308           |
| Step_1-PolicyExecTime   | 3.94           |
| Step_1-StdReturn        | 83.6           |
| Time                    | 1.58e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.952          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007245448    |
| n_timesteps             | 76480000       |
--------------------------------------------

 ---------------- Iteration 239 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 239           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0071363538 |
| LossBefore              | -5.10567e-09  |
| MeanKL                  | 0.007827194   |
| MeanKLBefore            | 2.6034285e-12 |
| Step_0-AverageDiscou... | 111           |
| Step_0-AveragePolicyStd | 0.42885175    |
| Step_0-AverageReturn    | 219           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 563           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 1344          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 89.8          |
| Step_1-AverageDiscou... | 112           |
| Step_1-AveragePolicyStd | 0.42877072    |
| Step_1-AverageReturn    | 219           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 556           |
| Step_1-MinReturn        | 16.4          |
| Step_1-NumTrajs         | 1360          |
| Step_1-PolicyExecTime   | 3.88          |
| Step_1-StdReturn        | 87.8          |
| Time                    | 1.59e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.991         |
| Time-Sampling           | 54.4          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.0071363486  |
| n_timesteps             | 76800000      |
-------------------------------------------

 ---------------- Iteration 240 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 240            |
| ItrTime                 | 70             |
| LossAfter               | -0.0072666286  |
| LossBefore              | -1.3098516e-09 |
| MeanKL                  | 0.0077763028   |
| MeanKLBefore            | -7.450191e-09  |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.4286681      |
| Step_0-AverageReturn    | 230            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 482            |
| Step_0-MinReturn        | 10.1           |
| Step_0-NumTrajs         | 1336           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 87.1           |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.42851472     |
| Step_1-AverageReturn    | 227            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 556            |
| Step_1-MinReturn        | 14             |
| Step_1-NumTrajs         | 1314           |
| Step_1-PolicyExecTime   | 4.01           |
| Step_1-StdReturn        | 88.6           |
| Time                    | 1.6e+04        |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.996          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007266627    |
| n_timesteps             | 77120000       |
--------------------------------------------

 ---------------- Iteration 241 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 241           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.00719887   |
| LossBefore              | 2.3679612e-09 |
| MeanKL                  | 0.00745258    |
| MeanKLBefore            | -2.979945e-09 |
| Step_0-AverageDiscou... | 120           |
| Step_0-AveragePolicyStd | 0.42896825    |
| Step_0-AverageReturn    | 239           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 520           |
| Step_0-MinReturn        | 27.3          |
| Step_0-NumTrajs         | 1298          |
| Step_0-PolicyExecTime   | 1.27          |
| Step_0-StdReturn        | 79.8          |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.42881867    |
| Step_1-AverageReturn    | 243           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 583           |
| Step_1-MinReturn        | 15            |
| Step_1-NumTrajs         | 1310          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 82.7          |
| Time                    | 1.6e+04       |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.976         |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.4          |
| dLoss                   | 0.0071988725  |
| n_timesteps             | 77440000      |
-------------------------------------------

 ---------------- Iteration 242 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 242            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0072533367  |
| LossBefore              | 5.5580573e-10  |
| MeanKL                  | 0.0076326937   |
| MeanKLBefore            | -4.4711674e-09 |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.42821428     |
| Step_0-AverageReturn    | 233            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 557            |
| Step_0-MinReturn        | 2.04           |
| Step_0-NumTrajs         | 1262           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 99.7           |
| Step_1-AverageDiscou... | 117            |
| Step_1-AveragePolicyStd | 0.42816967     |
| Step_1-AverageReturn    | 231            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 572            |
| Step_1-MinReturn        | 5.93           |
| Step_1-NumTrajs         | 1319           |
| Step_1-PolicyExecTime   | 3.96           |
| Step_1-StdReturn        | 92.7           |
| Time                    | 1.61e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.984          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007253337    |
| n_timesteps             | 77760000       |
--------------------------------------------

 ---------------- Iteration 243 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 243            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0072346567  |
| LossBefore              | 4.360557e-09   |
| MeanKL                  | 0.0070258332   |
| MeanKLBefore            | 1.04297015e-08 |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.42750606     |
| Step_0-AverageReturn    | 244            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 557            |
| Step_0-MinReturn        | 24.5           |
| Step_0-NumTrajs         | 1276           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 85.8           |
| Step_1-AverageDiscou... | 121            |
| Step_1-AveragePolicyStd | 0.42729536     |
| Step_1-AverageReturn    | 249            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 555            |
| Step_1-MinReturn        | 6.33           |
| Step_1-NumTrajs         | 1221           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 89.3           |
| Time                    | 1.62e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.977          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.007234661    |
| n_timesteps             | 78080000       |
--------------------------------------------

 ---------------- Iteration 244 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 244            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.007153322   |
| LossBefore              | 8.914361e-09   |
| MeanKL                  | 0.0072357394   |
| MeanKLBefore            | -1.3251622e-12 |
| Step_0-AverageDiscou... | 114            |
| Step_0-AveragePolicyStd | 0.42705074     |
| Step_0-AverageReturn    | 227            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 526            |
| Step_0-MinReturn        | 9.1            |
| Step_0-NumTrajs         | 1294           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 101            |
| Step_1-AverageDiscou... | 116            |
| Step_1-AveragePolicyStd | 0.42704123     |
| Step_1-AverageReturn    | 231            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 584            |
| Step_1-MinReturn        | 6.96           |
| Step_1-NumTrajs         | 1320           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 99.2           |
| Time                    | 1.62e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 1              |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.007153331    |
| n_timesteps             | 78400000       |
--------------------------------------------

 ---------------- Iteration 245 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 245           |
| ItrTime                 | 71            |
| LossAfter               | -0.006821507  |
| LossBefore              | -5.080478e-09 |
| MeanKL                  | 0.006814882   |
| MeanKLBefore            | 5.9584324e-09 |
| Step_0-AverageDiscou... | 112           |
| Step_0-AveragePolicyStd | 0.42631105    |
| Step_0-AverageReturn    | 220           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 573           |
| Step_0-MinReturn        | 16.4          |
| Step_0-NumTrajs         | 1300          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 97.2          |
| Step_1-AverageDiscou... | 111           |
| Step_1-AveragePolicyStd | 0.42631036    |
| Step_1-AverageReturn    | 219           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 528           |
| Step_1-MinReturn        | 11.6          |
| Step_1-NumTrajs         | 1314          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 93.9          |
| Time                    | 1.63e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.98          |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.006821502   |
| n_timesteps             | 78720000      |
-------------------------------------------

 ---------------- Iteration 246 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 246            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0073014856  |
| LossBefore              | 6.931339e-10   |
| MeanKL                  | 0.0073231766   |
| MeanKLBefore            | -1.7070789e-13 |
| Step_0-AverageDiscou... | 119            |
| Step_0-AveragePolicyStd | 0.42581257     |
| Step_0-AverageReturn    | 236            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 790            |
| Step_0-MinReturn        | 18.9           |
| Step_0-NumTrajs         | 1305           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 90.7           |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42573133     |
| Step_1-AverageReturn    | 239            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 610            |
| Step_1-MinReturn        | 17.9           |
| Step_1-NumTrajs         | 1273           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 92.4           |
| Time                    | 1.64e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.983          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007301486    |
| n_timesteps             | 79040000       |
--------------------------------------------

 ---------------- Iteration 247 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 247            |
| ItrTime                 | 72.9           |
| LossAfter               | -0.0071572587  |
| LossBefore              | 6.9981434e-09  |
| MeanKL                  | 0.0074030133   |
| MeanKLBefore            | -4.4685513e-09 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.42617068     |
| Step_0-AverageReturn    | 229            |
| Step_0-EnvExecTime      | 24.7           |
| Step_0-MaxReturn        | 529            |
| Step_0-MinReturn        | 20.4           |
| Step_0-NumTrajs         | 1250           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 94.1           |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.42611948     |
| Step_1-AverageReturn    | 236            |
| Step_1-EnvExecTime      | 24.6           |
| Step_1-MaxReturn        | 534            |
| Step_1-MinReturn        | 18             |
| Step_1-NumTrajs         | 1220           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 91.8           |
| Time                    | 1.65e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.949          |
| Time-Sampling           | 58             |
| Time-TotalInner         | 59.2           |
| dLoss                   | 0.0071572657   |
| n_timesteps             | 79360000       |
--------------------------------------------

 ---------------- Iteration 248 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 248            |
| ItrTime                 | 70             |
| LossAfter               | -0.0074241916  |
| LossBefore              | -1.1419058e-09 |
| MeanKL                  | 0.007388885    |
| MeanKLBefore            | 1.0427907e-08  |
| Step_0-AverageDiscou... | 120            |
| Step_0-AveragePolicyStd | 0.426206       |
| Step_0-AverageReturn    | 238            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 582            |
| Step_0-MinReturn        | 14.2           |
| Step_0-NumTrajs         | 1314           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 86.1           |
| Step_1-AverageDiscou... | 122            |
| Step_1-AveragePolicyStd | 0.42617682     |
| Step_1-AverageReturn    | 245            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 560            |
| Step_1-MinReturn        | 18.8           |
| Step_1-NumTrajs         | 1301           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 81.1           |
| Time                    | 1.65e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.974          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.0074241906   |
| n_timesteps             | 79680000       |
--------------------------------------------

 ---------------- Iteration 249 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 249            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0065775393  |
| LossBefore              | -2.9886018e-09 |
| MeanKL                  | 0.0069706365   |
| MeanKLBefore            | -1.3409067e-08 |
| Step_0-AverageDiscou... | 118            |
| Step_0-AveragePolicyStd | 0.42512435     |
| Step_0-AverageReturn    | 230            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 604            |
| Step_0-MinReturn        | 27.2           |
| Step_0-NumTrajs         | 1367           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 93.6           |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.4250795      |
| Step_1-AverageReturn    | 233            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 628            |
| Step_1-MinReturn        | 19.7           |
| Step_1-NumTrajs         | 1344           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 88.5           |
| Time                    | 1.66e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 1.01           |
| Time-Sampling           | 54.1           |
| Time-TotalInner         | 55.3           |
| dLoss                   | 0.0065775365   |
| n_timesteps             | 80000000       |
--------------------------------------------

 ---------------- Iteration 250 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 250            |
| ItrTime                 | 71             |
| LossAfter               | -0.006809826   |
| LossBefore              | 2.66986e-09    |
| MeanKL                  | 0.0066358447   |
| MeanKLBefore            | -1.4908276e-09 |
| Step_0-AverageDiscou... | 122            |
| Step_0-AveragePolicyStd | 0.42420936     |
| Step_0-AverageReturn    | 248            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 555            |
| Step_0-MinReturn        | 11.2           |
| Step_0-NumTrajs         | 1271           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 83.5           |
| Step_1-AverageDiscou... | 123            |
| Step_1-AveragePolicyStd | 0.42410633     |
| Step_1-AverageReturn    | 249            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 586            |
| Step_1-MinReturn        | 14.5           |
| Step_1-NumTrajs         | 1270           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 78.8           |
| Time                    | 1.67e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.99           |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.006809829    |
| n_timesteps             | 80320000       |
--------------------------------------------

 ---------------- Iteration 251 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 251            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.006831661   |
| LossBefore              | -4.6908755e-10 |
| MeanKL                  | 0.0069478997   |
| MeanKLBefore            | -2.9792209e-09 |
| Step_0-AverageDiscou... | 117            |
| Step_0-AveragePolicyStd | 0.4244263      |
| Step_0-AverageReturn    | 239            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 540            |
| Step_0-MinReturn        | 13.2           |
| Step_0-NumTrajs         | 1234           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 96.7           |
| Step_1-AverageDiscou... | 120            |
| Step_1-AveragePolicyStd | 0.42448246     |
| Step_1-AverageReturn    | 249            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 595            |
| Step_1-MinReturn        | 14.9           |
| Step_1-NumTrajs         | 1236           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 101            |
| Time                    | 1.67e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.959          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0068316604   |
| n_timesteps             | 80640000       |
--------------------------------------------

 ---------------- Iteration 252 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 252           |
| ItrTime                 | 70            |
| LossAfter               | -0.0069836467 |
| LossBefore              | -6.489663e-09 |
| MeanKL                  | 0.007068301   |
| MeanKLBefore            | 2.9809548e-09 |
| Step_0-AverageDiscou... | 115           |
| Step_0-AveragePolicyStd | 0.42453727    |
| Step_0-AverageReturn    | 229           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 612           |
| Step_0-MinReturn        | 8.77          |
| Step_0-NumTrajs         | 1328          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 91.5          |
| Step_1-AverageDiscou... | 116           |
| Step_1-AveragePolicyStd | 0.4244019     |
| Step_1-AverageReturn    | 229           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 600           |
| Step_1-MinReturn        | 8.21          |
| Step_1-NumTrajs         | 1329          |
| Step_1-PolicyExecTime   | 3.94          |
| Step_1-StdReturn        | 81.3          |
| Time                    | 1.68e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.995         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.00698364    |
| n_timesteps             | 80960000      |
-------------------------------------------

 ---------------- Iteration 253 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 253           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.007063086  |
| LossBefore              | -9.290261e-09 |
| MeanKL                  | 0.0071540535  |
| MeanKLBefore            | -5.959792e-09 |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.4249891     |
| Step_0-AverageReturn    | 244           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 540           |
| Step_0-MinReturn        | 15.3          |
| Step_0-NumTrajs         | 1223          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 106           |
| Step_1-AverageDiscou... | 114           |
| Step_1-AveragePolicyStd | 0.42490163    |
| Step_1-AverageReturn    | 233           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 511           |
| Step_1-MinReturn        | 15.3          |
| Step_1-NumTrajs         | 1252          |
| Step_1-PolicyExecTime   | 3.98          |
| Step_1-StdReturn        | 100           |
| Time                    | 1.69e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.964         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007063077   |
| n_timesteps             | 81280000      |
-------------------------------------------

 ---------------- Iteration 254 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 254           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.007107049  |
| LossBefore              | 1.0065866e-08 |
| MeanKL                  | 0.0070810355  |
| MeanKLBefore            | -4.468568e-09 |
| Step_0-AverageDiscou... | 119           |
| Step_0-AveragePolicyStd | 0.42559317    |
| Step_0-AverageReturn    | 247           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 585           |
| Step_0-MinReturn        | 17.9          |
| Step_0-NumTrajs         | 1210          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 99.4          |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.42554295    |
| Step_1-AverageReturn    | 256           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 584           |
| Step_1-MinReturn        | 20.6          |
| Step_1-NumTrajs         | 1191          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 97.5          |
| Time                    | 1.7e+04       |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.991         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0071070595  |
| n_timesteps             | 81600000      |
-------------------------------------------

 ---------------- Iteration 255 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 255            |
| ItrTime                 | 72.5           |
| LossAfter               | -0.007134276   |
| LossBefore              | -3.3698089e-09 |
| MeanKL                  | 0.007295883    |
| MeanKLBefore            | 1.1919279e-08  |
| Step_0-AverageDiscou... | 118            |
| Step_0-AveragePolicyStd | 0.42527556     |
| Step_0-AverageReturn    | 249            |
| Step_0-EnvExecTime      | 24.8           |
| Step_0-MaxReturn        | 561            |
| Step_0-MinReturn        | 6.2            |
| Step_0-NumTrajs         | 1182           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 97.1           |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42523515     |
| Step_1-AverageReturn    | 246            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 585            |
| Step_1-MinReturn        | 8.79           |
| Step_1-NumTrajs         | 1233           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 95.5           |
| Time                    | 1.7e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.929          |
| Time-Sampling           | 57.7           |
| Time-TotalInner         | 58.8           |
| dLoss                   | 0.0071342727   |
| n_timesteps             | 81920000       |
--------------------------------------------

 ---------------- Iteration 256 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 256            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0068663084  |
| LossBefore              | -4.6045177e-09 |
| MeanKL                  | 0.006969833    |
| MeanKLBefore            | -1.9369296e-08 |
| Step_0-AverageDiscou... | 123            |
| Step_0-AveragePolicyStd | 0.42554003     |
| Step_0-AverageReturn    | 252            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 613            |
| Step_0-MinReturn        | 23.3           |
| Step_0-NumTrajs         | 1258           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 100            |
| Step_1-AverageDiscou... | 123            |
| Step_1-AveragePolicyStd | 0.42540562     |
| Step_1-AverageReturn    | 250            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 601            |
| Step_1-MinReturn        | 25.2           |
| Step_1-NumTrajs         | 1268           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 92.6           |
| Time                    | 1.71e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.969          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0068663037   |
| n_timesteps             | 82240000       |
--------------------------------------------

 ---------------- Iteration 257 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 257            |
| ItrTime                 | 71             |
| LossAfter               | -0.007022568   |
| LossBefore              | 9.652862e-09   |
| MeanKL                  | 0.0071537704   |
| MeanKLBefore            | -1.4888215e-09 |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.42764708     |
| Step_0-AverageReturn    | 232            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 502            |
| Step_0-MinReturn        | 4.42           |
| Step_0-NumTrajs         | 1294           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 93             |
| Step_1-AverageDiscou... | 116            |
| Step_1-AveragePolicyStd | 0.42757314     |
| Step_1-AverageReturn    | 235            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 536            |
| Step_1-MinReturn        | 2.85           |
| Step_1-NumTrajs         | 1262           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 93.9           |
| Time                    | 1.72e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.978          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007022578    |
| n_timesteps             | 82560000       |
--------------------------------------------

 ---------------- Iteration 258 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 258            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.007078548   |
| LossBefore              | 6.6715726e-09  |
| MeanKL                  | 0.007722421    |
| MeanKLBefore            | -1.4906354e-09 |
| Step_0-AverageDiscou... | 118            |
| Step_0-AveragePolicyStd | 0.42771623     |
| Step_0-AverageReturn    | 236            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 547            |
| Step_0-MinReturn        | 11.6           |
| Step_0-NumTrajs         | 1289           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 99.4           |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42763805     |
| Step_1-AverageReturn    | 237            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 650            |
| Step_1-MinReturn        | 18.5           |
| Step_1-NumTrajs         | 1305           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 102            |
| Time                    | 1.72e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.986          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.0070785545   |
| n_timesteps             | 82880000       |
--------------------------------------------

 ---------------- Iteration 259 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 259            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.006965655   |
| LossBefore              | -3.6351728e-09 |
| MeanKL                  | 0.0074284496   |
| MeanKLBefore            | 2.9793013e-09  |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.42804456     |
| Step_0-AverageReturn    | 244            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 593            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 1263           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 92.2           |
| Step_1-AverageDiscou... | 122            |
| Step_1-AveragePolicyStd | 0.42797744     |
| Step_1-AverageReturn    | 247            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 591            |
| Step_1-MinReturn        | 12.5           |
| Step_1-NumTrajs         | 1298           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 95             |
| Time                    | 1.73e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.962          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.006965651    |
| n_timesteps             | 83200000       |
--------------------------------------------

 ---------------- Iteration 260 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 260            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.00695998    |
| LossBefore              | -2.0661801e-10 |
| MeanKL                  | 0.007214986    |
| MeanKLBefore            | -1.0428968e-08 |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.42708242     |
| Step_0-AverageReturn    | 262            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 525            |
| Step_0-MinReturn        | 23.1           |
| Step_0-NumTrajs         | 1203           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 83.1           |
| Step_1-AverageDiscou... | 126            |
| Step_1-AveragePolicyStd | 0.42693618     |
| Step_1-AverageReturn    | 266            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 587            |
| Step_1-MinReturn        | 25.6           |
| Step_1-NumTrajs         | 1156           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 80.2           |
| Time                    | 1.74e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.965          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.00695998     |
| n_timesteps             | 83520000       |
--------------------------------------------

 ---------------- Iteration 261 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 261            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.0066436767  |
| LossBefore              | -8.623629e-09  |
| MeanKL                  | 0.0074786916   |
| MeanKLBefore            | -4.4705173e-09 |
| Step_0-AverageDiscou... | 120            |
| Step_0-AveragePolicyStd | 0.4262258      |
| Step_0-AverageReturn    | 240            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 590            |
| Step_0-MinReturn        | 21.3           |
| Step_0-NumTrajs         | 1284           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 95.4           |
| Step_1-AverageDiscou... | 122            |
| Step_1-AveragePolicyStd | 0.4260607      |
| Step_1-AverageReturn    | 244            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 652            |
| Step_1-MinReturn        | 19.9           |
| Step_1-NumTrajs         | 1295           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 91.5           |
| Time                    | 1.75e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 1              |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.006643668    |
| n_timesteps             | 83840000       |
--------------------------------------------

 ---------------- Iteration 262 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 262            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.0070697106  |
| LossBefore              | -5.4231455e-09 |
| MeanKL                  | 0.007974029    |
| MeanKLBefore            | 4.4700283e-09  |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.42570642     |
| Step_0-AverageReturn    | 231            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 574            |
| Step_0-MinReturn        | 12.8           |
| Step_0-NumTrajs         | 1252           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 114            |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.4257065      |
| Step_1-AverageReturn    | 238            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 572            |
| Step_1-MinReturn        | 10.9           |
| Step_1-NumTrajs         | 1225           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 106            |
| Time                    | 1.75e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.957          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 58             |
| dLoss                   | 0.007069705    |
| n_timesteps             | 84160000       |
--------------------------------------------

 ---------------- Iteration 263 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 263            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0068792836  |
| LossBefore              | 2.8658218e-09  |
| MeanKL                  | 0.007454251    |
| MeanKLBefore            | -1.4901694e-09 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.425133       |
| Step_0-AverageReturn    | 213            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 560            |
| Step_0-MinReturn        | 12.2           |
| Step_0-NumTrajs         | 1409           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 99.2           |
| Step_1-AverageDiscou... | 116            |
| Step_1-AveragePolicyStd | 0.42505866     |
| Step_1-AverageReturn    | 227            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 658            |
| Step_1-MinReturn        | 14.8           |
| Step_1-NumTrajs         | 1371           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 100            |
| Time                    | 1.76e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 1.02           |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0068792864   |
| n_timesteps             | 84480000       |
--------------------------------------------

 ---------------- Iteration 264 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 264            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0068795816  |
| LossBefore              | -5.7429985e-09 |
| MeanKL                  | 0.007386072    |
| MeanKLBefore            | 1.4896748e-09  |
| Step_0-AverageDiscou... | 114            |
| Step_0-AveragePolicyStd | 0.42498502     |
| Step_0-AverageReturn    | 233            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 563            |
| Step_0-MinReturn        | 12.1           |
| Step_0-NumTrajs         | 1209           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 101            |
| Step_1-AverageDiscou... | 117            |
| Step_1-AveragePolicyStd | 0.42485818     |
| Step_1-AverageReturn    | 233            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 632            |
| Step_1-MinReturn        | 7.12           |
| Step_1-NumTrajs         | 1281           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 102            |
| Time                    | 1.77e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.954          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.006879576    |
| n_timesteps             | 84800000       |
--------------------------------------------

 ---------------- Iteration 265 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 265           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0070369644 |
| LossBefore              | -2.069373e-09 |
| MeanKL                  | 0.007399678   |
| MeanKLBefore            | 8.939817e-09  |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.42386043    |
| Step_0-AverageReturn    | 243           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 543           |
| Step_0-MinReturn        | 11.2          |
| Step_0-NumTrajs         | 1206          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 106           |
| Step_1-AverageDiscou... | 119           |
| Step_1-AveragePolicyStd | 0.42390087    |
| Step_1-AverageReturn    | 246           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 585           |
| Step_1-MinReturn        | 10            |
| Step_1-NumTrajs         | 1225          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 102           |
| Time                    | 1.77e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.995         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0070369625  |
| n_timesteps             | 85120000      |
-------------------------------------------

 ---------------- Iteration 266 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 266           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0065377965 |
| LossBefore              | -5.454327e-09 |
| MeanKL                  | 0.0075879456  |
| MeanKLBefore            | 1.490504e-09  |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.42324176    |
| Step_0-AverageReturn    | 233           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 525           |
| Step_0-MinReturn        | 6.87          |
| Step_0-NumTrajs         | 1261          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 88.9          |
| Step_1-AverageDiscou... | 121           |
| Step_1-AveragePolicyStd | 0.423095      |
| Step_1-AverageReturn    | 244           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 531           |
| Step_1-MinReturn        | 16.5          |
| Step_1-NumTrajs         | 1278          |
| Step_1-PolicyExecTime   | 4.05          |
| Step_1-StdReturn        | 92.4          |
| Time                    | 1.78e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.961         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.006537791   |
| n_timesteps             | 85440000      |
-------------------------------------------

 ---------------- Iteration 267 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 267            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.006644943   |
| LossBefore              | -1.8422999e-09 |
| MeanKL                  | 0.007544195    |
| MeanKLBefore            | -4.4697e-09    |
| Step_0-AverageDiscou... | 108            |
| Step_0-AveragePolicyStd | 0.42399552     |
| Step_0-AverageReturn    | 216            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 528            |
| Step_0-MinReturn        | 7              |
| Step_0-NumTrajs         | 1280           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 110            |
| Step_1-AverageDiscou... | 113            |
| Step_1-AveragePolicyStd | 0.4239398      |
| Step_1-AverageReturn    | 229            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 638            |
| Step_1-MinReturn        | 10             |
| Step_1-NumTrajs         | 1270           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 109            |
| Time                    | 1.79e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.953          |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 58             |
| dLoss                   | 0.006644941    |
| n_timesteps             | 85760000       |
--------------------------------------------

 ---------------- Iteration 268 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 268           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.006755973  |
| LossBefore              | 8.4794854e-10 |
| MeanKL                  | 0.0073946835  |
| MeanKLBefore            | 1.4911826e-09 |
| Step_0-AverageDiscou... | 125           |
| Step_0-AveragePolicyStd | 0.4226278     |
| Step_0-AverageReturn    | 265           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 713           |
| Step_0-MinReturn        | 5.98          |
| Step_0-NumTrajs         | 1156          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 96.6          |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.4227277     |
| Step_1-AverageReturn    | 257           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 588           |
| Step_1-MinReturn        | 3.89          |
| Step_1-NumTrajs         | 1227          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 95            |
| Time                    | 1.79e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.937         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.006755974   |
| n_timesteps             | 86080000      |
-------------------------------------------

 ---------------- Iteration 269 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 269           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0071905763 |
| LossBefore              | -3.186555e-09 |
| MeanKL                  | 0.0076458766  |
| MeanKLBefore            | 5.9600063e-09 |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.42288736    |
| Step_0-AverageReturn    | 261           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 521           |
| Step_0-MinReturn        | 13.3          |
| Step_0-NumTrajs         | 1177          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 96            |
| Step_1-AverageDiscou... | 125           |
| Step_1-AveragePolicyStd | 0.42289534    |
| Step_1-AverageReturn    | 259           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 647           |
| Step_1-MinReturn        | 14.7          |
| Step_1-NumTrajs         | 1231          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 90.1          |
| Time                    | 1.8e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.963         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007190573   |
| n_timesteps             | 86400000      |
-------------------------------------------

 ---------------- Iteration 270 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 270            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0070261965  |
| LossBefore              | -1.1789444e-08 |
| MeanKL                  | 0.00840508     |
| MeanKLBefore            | -1.3407873e-08 |
| Step_0-AverageDiscou... | 117            |
| Step_0-AveragePolicyStd | 0.42288485     |
| Step_0-AverageReturn    | 235            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 600            |
| Step_0-MinReturn        | 9.68           |
| Step_0-NumTrajs         | 1252           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 98             |
| Step_1-AverageDiscou... | 120            |
| Step_1-AveragePolicyStd | 0.42291337     |
| Step_1-AverageReturn    | 239            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 561            |
| Step_1-MinReturn        | 19             |
| Step_1-NumTrajs         | 1327           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 96.2           |
| Time                    | 1.81e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.965          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007026185    |
| n_timesteps             | 86720000       |
--------------------------------------------

 ---------------- Iteration 271 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 271           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0069490066 |
| LossBefore              | -5.916106e-09 |
| MeanKL                  | 0.0072771953  |
| MeanKLBefore            | 1.191985e-08  |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.42316008    |
| Step_0-AverageReturn    | 245           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 575           |
| Step_0-MinReturn        | 13.6          |
| Step_0-NumTrajs         | 1211          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 119           |
| Step_1-AveragePolicyStd | 0.4231249     |
| Step_1-AverageReturn    | 243           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 564           |
| Step_1-MinReturn        | 10.9          |
| Step_1-NumTrajs         | 1285          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 108           |
| Time                    | 1.82e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.978         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0069490005  |
| n_timesteps             | 87040000      |
-------------------------------------------

 ---------------- Iteration 272 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 272            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007248182   |
| LossBefore              | -1.0433073e-08 |
| MeanKL                  | 0.0073841526   |
| MeanKLBefore            | 1.489254e-09   |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.42316693     |
| Step_0-AverageReturn    | 232            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 551            |
| Step_0-MinReturn        | 20.8           |
| Step_0-NumTrajs         | 1283           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 92.4           |
| Step_1-AverageDiscou... | 118            |
| Step_1-AveragePolicyStd | 0.4230094      |
| Step_1-AverageReturn    | 236            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 574            |
| Step_1-MinReturn        | 18.2           |
| Step_1-NumTrajs         | 1291           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 92.3           |
| Time                    | 1.82e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.975          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.0072481716   |
| n_timesteps             | 87360000       |
--------------------------------------------

 ---------------- Iteration 273 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 273           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.0070150024 |
| LossBefore              | 3.939728e-09  |
| MeanKL                  | 0.008541996   |
| MeanKLBefore            | 7.4488637e-09 |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.42283332    |
| Step_0-AverageReturn    | 265           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 573           |
| Step_0-MinReturn        | 9.75          |
| Step_0-NumTrajs         | 1130          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 93.9          |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.42279592    |
| Step_1-AverageReturn    | 258           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 554           |
| Step_1-MinReturn        | 8.79          |
| Step_1-NumTrajs         | 1212          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 86.5          |
| Time                    | 1.83e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.96          |
| Time-Sampling           | 57            |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.007015006   |
| n_timesteps             | 87680000      |
-------------------------------------------

 ---------------- Iteration 274 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 274           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.00697874   |
| LossBefore              | 4.389188e-09  |
| MeanKL                  | 0.008279424   |
| MeanKLBefore            | 1.4900363e-08 |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.42329395    |
| Step_0-AverageReturn    | 245           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 642           |
| Step_0-MinReturn        | 12.9          |
| Step_0-NumTrajs         | 1151          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 113           |
| Step_1-AverageDiscou... | 120           |
| Step_1-AveragePolicyStd | 0.4232716     |
| Step_1-AverageReturn    | 252           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 595           |
| Step_1-MinReturn        | 13.5          |
| Step_1-NumTrajs         | 1219          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 111           |
| Time                    | 1.84e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.928         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.006978744   |
| n_timesteps             | 88000000      |
-------------------------------------------

 ---------------- Iteration 275 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 275           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.006399402  |
| LossBefore              | 1.6546441e-09 |
| MeanKL                  | 0.006940016   |
| MeanKLBefore            | 2.97957e-09   |
| Step_0-AverageDiscou... | 119           |
| Step_0-AveragePolicyStd | 0.42285535    |
| Step_0-AverageReturn    | 248           |
| Step_0-EnvExecTime      | 24.4          |
| Step_0-MaxReturn        | 546           |
| Step_0-MinReturn        | 7.48          |
| Step_0-NumTrajs         | 1209          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 105           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.4228981     |
| Step_1-AverageReturn    | 255           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 572           |
| Step_1-MinReturn        | 10.3          |
| Step_1-NumTrajs         | 1195          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 103           |
| Time                    | 1.84e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.943         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58            |
| dLoss                   | 0.006399404   |
| n_timesteps             | 88320000      |
-------------------------------------------

 ---------------- Iteration 276 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 276            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0069220513  |
| LossBefore              | -5.442521e-09  |
| MeanKL                  | 0.007744974    |
| MeanKLBefore            | -1.4911421e-09 |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.42220625     |
| Step_0-AverageReturn    | 251            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 551            |
| Step_0-MinReturn        | 23.9           |
| Step_0-NumTrajs         | 1176           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 102            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42220765     |
| Step_1-AverageReturn    | 237            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 570            |
| Step_1-MinReturn        | 9.21           |
| Step_1-NumTrajs         | 1321           |
| Step_1-PolicyExecTime   | 3.96           |
| Step_1-StdReturn        | 98.1           |
| Time                    | 1.85e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.951          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0069220457   |
| n_timesteps             | 88640000       |
--------------------------------------------

 ---------------- Iteration 277 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 277           |
| ItrTime                 | 73            |
| LossAfter               | -0.0062598893 |
| LossBefore              | -3.340982e-09 |
| MeanKL                  | 0.007915406   |
| MeanKLBefore            | -5.960101e-09 |
| Step_0-AverageDiscou... | 110           |
| Step_0-AveragePolicyStd | 0.4224421     |
| Step_0-AverageReturn    | 229           |
| Step_0-EnvExecTime      | 24.7          |
| Step_0-MaxReturn        | 554           |
| Step_0-MinReturn        | 13.5          |
| Step_0-NumTrajs         | 1208          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 115           |
| Step_1-AverageDiscou... | 113           |
| Step_1-AveragePolicyStd | 0.42236236    |
| Step_1-AverageReturn    | 236           |
| Step_1-EnvExecTime      | 24.7          |
| Step_1-MaxReturn        | 569           |
| Step_1-MinReturn        | 8.93          |
| Step_1-NumTrajs         | 1209          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 116           |
| Time                    | 1.86e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.926         |
| Time-Sampling           | 58.2          |
| Time-TotalInner         | 59.3          |
| dLoss                   | 0.006259886   |
| n_timesteps             | 88960000      |
-------------------------------------------

 ---------------- Iteration 278 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 278           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.0066339886 |
| LossBefore              | 2.3331084e-09 |
| MeanKL                  | 0.0074626813  |
| MeanKLBefore            | 4.469377e-09  |
| Step_0-AverageDiscou... | 120           |
| Step_0-AveragePolicyStd | 0.42297032    |
| Step_0-AverageReturn    | 250           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 578           |
| Step_0-MinReturn        | 18.1          |
| Step_0-NumTrajs         | 1204          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 102           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.42294088    |
| Step_1-AverageReturn    | 254           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 610           |
| Step_1-MinReturn        | 22.4          |
| Step_1-NumTrajs         | 1217          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 93.8          |
| Time                    | 1.87e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.944         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.006633991   |
| n_timesteps             | 89280000      |
-------------------------------------------

 ---------------- Iteration 279 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 279            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.006800811   |
| LossBefore              | -1.4576162e-10 |
| MeanKL                  | 0.007888297    |
| MeanKLBefore            | -1.4912104e-09 |
| Step_0-AverageDiscou... | 120            |
| Step_0-AveragePolicyStd | 0.42225012     |
| Step_0-AverageReturn    | 250            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 503            |
| Step_0-MinReturn        | 14.8           |
| Step_0-NumTrajs         | 1176           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 100            |
| Step_1-AverageDiscou... | 118            |
| Step_1-AveragePolicyStd | 0.42221883     |
| Step_1-AverageReturn    | 241            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 562            |
| Step_1-MinReturn        | 25.6           |
| Step_1-NumTrajs         | 1258           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 102            |
| Time                    | 1.87e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.929          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.006800811    |
| n_timesteps             | 89600000       |
--------------------------------------------

 ---------------- Iteration 280 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 280            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0064920513  |
| LossBefore              | -4.4222035e-09 |
| MeanKL                  | 0.007404089    |
| MeanKLBefore            | -1.4909908e-09 |
| Step_0-AverageDiscou... | 118            |
| Step_0-AveragePolicyStd | 0.42199492     |
| Step_0-AverageReturn    | 243            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 564            |
| Step_0-MinReturn        | 16.5           |
| Step_0-NumTrajs         | 1247           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 107            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42190656     |
| Step_1-AverageReturn    | 241            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 567            |
| Step_1-MinReturn        | 12.7           |
| Step_1-NumTrajs         | 1290           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 107            |
| Time                    | 1.88e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.986          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.006492047    |
| n_timesteps             | 89920000       |
--------------------------------------------

 ---------------- Iteration 281 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 281            |
| ItrTime                 | 70             |
| LossAfter               | -0.006662918   |
| LossBefore              | -5.0981126e-09 |
| MeanKL                  | 0.0073943585   |
| MeanKLBefore            | -7.4480804e-09 |
| Step_0-AverageDiscou... | 115            |
| Step_0-AveragePolicyStd | 0.4225806      |
| Step_0-AverageReturn    | 232            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 575            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 1279           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 102            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.42246103     |
| Step_1-AverageReturn    | 239            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 617            |
| Step_1-MinReturn        | 6.59           |
| Step_1-NumTrajs         | 1310           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 100            |
| Time                    | 1.89e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.97           |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.0066629127   |
| n_timesteps             | 90240000       |
--------------------------------------------

 ---------------- Iteration 282 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 282            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.0070983917  |
| LossBefore              | -2.8795735e-09 |
| MeanKL                  | 0.007335293    |
| MeanKLBefore            | 1.1920294e-08  |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.42233452     |
| Step_0-AverageReturn    | 268            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 646            |
| Step_0-MinReturn        | 18.5           |
| Step_0-NumTrajs         | 1155           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 102            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.4223027      |
| Step_1-AverageReturn    | 276            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 614            |
| Step_1-MinReturn        | 25.2           |
| Step_1-NumTrajs         | 1144           |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 98.2           |
| Time                    | 1.89e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.939          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 58             |
| dLoss                   | 0.007098389    |
| n_timesteps             | 90560000       |
--------------------------------------------

 ---------------- Iteration 283 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 283           |
| ItrTime                 | 71.7          |
| LossAfter               | -0.0069381865 |
| LossBefore              | 1.3302554e-09 |
| MeanKL                  | 0.0077262768  |
| MeanKLBefore            | 1.4899646e-08 |
| Step_0-AverageDiscou... | 120           |
| Step_0-AveragePolicyStd | 0.42146403    |
| Step_0-AverageReturn    | 248           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 529           |
| Step_0-MinReturn        | 9.53          |
| Step_0-NumTrajs         | 1184          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 94.7          |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.42144364    |
| Step_1-AverageReturn    | 256           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 559           |
| Step_1-MinReturn        | 8.72          |
| Step_1-NumTrajs         | 1225          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 99.9          |
| Time                    | 1.9e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.989         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.006938188   |
| n_timesteps             | 90880000      |
-------------------------------------------

 ---------------- Iteration 284 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 284           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.006793432  |
| LossBefore              | -6.250758e-09 |
| MeanKL                  | 0.007393421   |
| MeanKLBefore            | 4.4707256e-09 |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.42113668    |
| Step_0-AverageReturn    | 238           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 576           |
| Step_0-MinReturn        | 13.8          |
| Step_0-NumTrajs         | 1232          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 109           |
| Step_1-AverageDiscou... | 118           |
| Step_1-AveragePolicyStd | 0.42112216    |
| Step_1-AverageReturn    | 242           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 571           |
| Step_1-MinReturn        | 11.7          |
| Step_1-NumTrajs         | 1274          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 112           |
| Time                    | 1.91e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.951         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.006793426   |
| n_timesteps             | 91200000      |
-------------------------------------------

 ---------------- Iteration 285 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 285           |
| ItrTime                 | 72.2          |
| LossAfter               | -0.006874977  |
| LossBefore              | 1.2445757e-09 |
| MeanKL                  | 0.006866079   |
| MeanKLBefore            | 5.960861e-09  |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.42098492    |
| Step_0-AverageReturn    | 246           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 632           |
| Step_0-MinReturn        | 17.2          |
| Step_0-NumTrajs         | 1189          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 110           |
| Step_1-AverageDiscou... | 118           |
| Step_1-AveragePolicyStd | 0.420896      |
| Step_1-AverageReturn    | 246           |
| Step_1-EnvExecTime      | 24.4          |
| Step_1-MaxReturn        | 592           |
| Step_1-MinReturn        | 8.68          |
| Step_1-NumTrajs         | 1201          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 101           |
| Time                    | 1.92e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.924         |
| Time-Sampling           | 57.4          |
| Time-TotalInner         | 58.5          |
| dLoss                   | 0.0068749785  |
| n_timesteps             | 91520000      |
-------------------------------------------

 ---------------- Iteration 286 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 286           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0072428496 |
| LossBefore              | 1.7211443e-08 |
| MeanKL                  | 0.0077121137  |
| MeanKLBefore            | 8.940469e-09  |
| Step_0-AverageDiscou... | 128           |
| Step_0-AveragePolicyStd | 0.4215139     |
| Step_0-AverageReturn    | 269           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 634           |
| Step_0-MinReturn        | 11.6          |
| Step_0-NumTrajs         | 1165          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 100           |
| Step_1-AverageDiscou... | 128           |
| Step_1-AveragePolicyStd | 0.42140105    |
| Step_1-AverageReturn    | 264           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 658           |
| Step_1-MinReturn        | 19.4          |
| Step_1-NumTrajs         | 1245          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 94.5          |
| Time                    | 1.92e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.922         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.007242867   |
| n_timesteps             | 91840000      |
-------------------------------------------

 ---------------- Iteration 287 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 287            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0063898554  |
| LossBefore              | -3.6339443e-09 |
| MeanKL                  | 0.007919943    |
| MeanKLBefore            | 1.4906826e-09  |
| Step_0-AverageDiscou... | 125            |
| Step_0-AveragePolicyStd | 0.4207843      |
| Step_0-AverageReturn    | 258            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 593            |
| Step_0-MinReturn        | 9.34           |
| Step_0-NumTrajs         | 1218           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 92.5           |
| Step_1-AverageDiscou... | 124            |
| Step_1-AveragePolicyStd | 0.4207615      |
| Step_1-AverageReturn    | 255            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 605            |
| Step_1-MinReturn        | 15.2           |
| Step_1-NumTrajs         | 1240           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 94.7           |
| Time                    | 1.93e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.942          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.0063898517   |
| n_timesteps             | 92160000       |
--------------------------------------------

 ---------------- Iteration 288 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 288            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.007167374   |
| LossBefore              | -6.4636013e-10 |
| MeanKL                  | 0.0077755945   |
| MeanKLBefore            | -2.9798013e-09 |
| Step_0-AverageDiscou... | 125            |
| Step_0-AveragePolicyStd | 0.42075348     |
| Step_0-AverageReturn    | 266            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 623            |
| Step_0-MinReturn        | 21.1           |
| Step_0-NumTrajs         | 1141           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 92.6           |
| Step_1-AverageDiscou... | 129            |
| Step_1-AveragePolicyStd | 0.420623       |
| Step_1-AverageReturn    | 271            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 566            |
| Step_1-MinReturn        | 13.1           |
| Step_1-NumTrajs         | 1188           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 93.2           |
| Time                    | 1.94e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.912          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0071673733   |
| n_timesteps             | 92480000       |
--------------------------------------------

 ---------------- Iteration 289 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 289            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.006987959   |
| LossBefore              | -1.1479362e-09 |
| MeanKL                  | 0.007817493    |
| MeanKLBefore            | 2.9798082e-08  |
| Step_0-AverageDiscou... | 128            |
| Step_0-AveragePolicyStd | 0.41970614     |
| Step_0-AverageReturn    | 269            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 545            |
| Step_0-MinReturn        | 24.5           |
| Step_0-NumTrajs         | 1176           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 90.1           |
| Step_1-AverageDiscou... | 129            |
| Step_1-AveragePolicyStd | 0.419712       |
| Step_1-AverageReturn    | 272            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 560            |
| Step_1-MinReturn        | 8.45           |
| Step_1-NumTrajs         | 1183           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 92.1           |
| Time                    | 1.94e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.893          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.006987958    |
| n_timesteps             | 92800000       |
--------------------------------------------

 ---------------- Iteration 290 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 290           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007027765  |
| LossBefore              | 5.447821e-09  |
| MeanKL                  | 0.007546422   |
| MeanKLBefore            | 1.0430027e-08 |
| Step_0-AverageDiscou... | 121           |
| Step_0-AveragePolicyStd | 0.41923335    |
| Step_0-AverageReturn    | 254           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 537           |
| Step_0-MinReturn        | 10.8          |
| Step_0-NumTrajs         | 1130          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 96.8          |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.41919598    |
| Step_1-AverageReturn    | 258           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 651           |
| Step_1-MinReturn        | 16.7          |
| Step_1-NumTrajs         | 1181          |
| Step_1-PolicyExecTime   | 4.48          |
| Step_1-StdReturn        | 98.6          |
| Time                    | 1.95e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.908         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0070277704  |
| n_timesteps             | 93120000      |
-------------------------------------------

 ---------------- Iteration 291 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 291            |
| ItrTime                 | 72.4           |
| LossAfter               | -0.006800699   |
| LossBefore              | -2.8133018e-09 |
| MeanKL                  | 0.0071590655   |
| MeanKLBefore            | -7.44859e-09   |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.4194995      |
| Step_0-AverageReturn    | 266            |
| Step_0-EnvExecTime      | 24.6           |
| Step_0-MaxReturn        | 556            |
| Step_0-MinReturn        | 16.6           |
| Step_0-NumTrajs         | 1138           |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 98.1           |
| Step_1-AverageDiscou... | 124            |
| Step_1-AveragePolicyStd | 0.41949144     |
| Step_1-AverageReturn    | 264            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 597            |
| Step_1-MinReturn        | 18.4           |
| Step_1-NumTrajs         | 1178           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 97.2           |
| Time                    | 1.96e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.935          |
| Time-Sampling           | 57.5           |
| Time-TotalInner         | 58.6           |
| dLoss                   | 0.0068006963   |
| n_timesteps             | 93440000       |
--------------------------------------------

 ---------------- Iteration 292 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 292            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0067815855  |
| LossBefore              | -4.8795004e-09 |
| MeanKL                  | 0.00743896     |
| MeanKLBefore            | 4.4694835e-09  |
| Step_0-AverageDiscou... | 109            |
| Step_0-AveragePolicyStd | 0.41903496     |
| Step_0-AverageReturn    | 226            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 509            |
| Step_0-MinReturn        | 7.49           |
| Step_0-NumTrajs         | 1235           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 114            |
| Step_1-AverageDiscou... | 115            |
| Step_1-AveragePolicyStd | 0.41898078     |
| Step_1-AverageReturn    | 238            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 642            |
| Step_1-MinReturn        | 7.43           |
| Step_1-NumTrajs         | 1259           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 110            |
| Time                    | 1.97e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.949          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.006781581    |
| n_timesteps             | 93760000       |
--------------------------------------------

 ---------------- Iteration 293 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 293            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0067867027  |
| LossBefore              | -1.8640505e-09 |
| MeanKL                  | 0.0075700344   |
| MeanKLBefore            | 7.4464026e-09  |
| Step_0-AverageDiscou... | 119            |
| Step_0-AveragePolicyStd | 0.41856894     |
| Step_0-AverageReturn    | 247            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 567            |
| Step_0-MinReturn        | 17.6           |
| Step_0-NumTrajs         | 1134           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 98.3           |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.41847157     |
| Step_1-AverageReturn    | 246            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 589            |
| Step_1-MinReturn        | 6.28           |
| Step_1-NumTrajs         | 1204           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 103            |
| Time                    | 1.97e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.904          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.006786701    |
| n_timesteps             | 94080000       |
--------------------------------------------

 ---------------- Iteration 294 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 294            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0070358827  |
| LossBefore              | -1.3032481e-08 |
| MeanKL                  | 0.0074386434   |
| MeanKLBefore            | 1.4890812e-09  |
| Step_0-AverageDiscou... | 117            |
| Step_0-AveragePolicyStd | 0.4177242      |
| Step_0-AverageReturn    | 247            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 635            |
| Step_0-MinReturn        | 12.3           |
| Step_0-NumTrajs         | 1171           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 104            |
| Step_1-AverageDiscou... | 123            |
| Step_1-AveragePolicyStd | 0.4176345      |
| Step_1-AverageReturn    | 261            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 597            |
| Step_1-MinReturn        | 16.5           |
| Step_1-NumTrajs         | 1189           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 97             |
| Time                    | 1.98e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.976          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0070358696   |
| n_timesteps             | 94400000       |
--------------------------------------------

 ---------------- Iteration 295 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 295           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.0069932058 |
| LossBefore              | -7.990719e-09 |
| MeanKL                  | 0.008010623   |
| MeanKLBefore            | -8.938311e-09 |
| Step_0-AverageDiscou... | 119           |
| Step_0-AveragePolicyStd | 0.41744345    |
| Step_0-AverageReturn    | 252           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 567           |
| Step_0-MinReturn        | 10.1          |
| Step_0-NumTrajs         | 1157          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 103           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.41735998    |
| Step_1-AverageReturn    | 263           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 684           |
| Step_1-MinReturn        | 17            |
| Step_1-NumTrajs         | 1175          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 104           |
| Time                    | 1.99e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.891         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.006993198   |
| n_timesteps             | 94720000      |
-------------------------------------------

 ---------------- Iteration 296 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 296            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0074192835  |
| LossBefore              | -1.8128642e-09 |
| MeanKL                  | 0.0077375607   |
| MeanKLBefore            | 1.6388233e-08  |
| Step_0-AverageDiscou... | 114            |
| Step_0-AveragePolicyStd | 0.41834518     |
| Step_0-AverageReturn    | 236            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 620            |
| Step_0-MinReturn        | 6.71           |
| Step_0-NumTrajs         | 1212           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 110            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.41832048     |
| Step_1-AverageReturn    | 245            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 687            |
| Step_1-MinReturn        | 7.13           |
| Step_1-NumTrajs         | 1265           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 111            |
| Time                    | 1.99e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.969          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0074192816   |
| n_timesteps             | 95040000       |
--------------------------------------------

 ---------------- Iteration 297 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 297            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0071694613  |
| LossBefore              | -1.8350832e-09 |
| MeanKL                  | 0.0076501206   |
| MeanKLBefore            | 2.9795484e-09  |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.41788986     |
| Step_0-AverageReturn    | 244            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 591            |
| Step_0-MinReturn        | 16.4           |
| Step_0-NumTrajs         | 1229           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 93.6           |
| Step_1-AverageDiscou... | 121            |
| Step_1-AveragePolicyStd | 0.41774493     |
| Step_1-AverageReturn    | 245            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 644            |
| Step_1-MinReturn        | 18.9           |
| Step_1-NumTrajs         | 1273           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 98.4           |
| Time                    | 2e+04          |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 1.01           |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0071694595   |
| n_timesteps             | 95360000       |
--------------------------------------------

 ---------------- Iteration 298 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 298           |
| ItrTime                 | 73            |
| LossAfter               | -0.006829422  |
| LossBefore              | 2.7665128e-09 |
| MeanKL                  | 0.007618463   |
| MeanKLBefore            | 2.5329484e-08 |
| Step_0-AverageDiscou... | 119           |
| Step_0-AveragePolicyStd | 0.41756257    |
| Step_0-AverageReturn    | 262           |
| Step_0-EnvExecTime      | 25            |
| Step_0-MaxReturn        | 589           |
| Step_0-MinReturn        | 10.1          |
| Step_0-NumTrajs         | 1085          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 120           |
| Step_1-AveragePolicyStd | 0.417416      |
| Step_1-AverageReturn    | 259           |
| Step_1-EnvExecTime      | 24.4          |
| Step_1-MaxReturn        | 571           |
| Step_1-MinReturn        | 6.44          |
| Step_1-NumTrajs         | 1153          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 99.5          |
| Time                    | 2.01e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.899         |
| Time-Sampling           | 58.2          |
| Time-TotalInner         | 59.3          |
| dLoss                   | 0.0068294248  |
| n_timesteps             | 95680000      |
-------------------------------------------

 ---------------- Iteration 299 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 299           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.007227196  |
| LossBefore              | -7.96793e-09  |
| MeanKL                  | 0.0077141165  |
| MeanKLBefore            | 1.4923514e-09 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.41757673    |
| Step_0-AverageReturn    | 254           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 558           |
| Step_0-MinReturn        | 16.6          |
| Step_0-NumTrajs         | 1175          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 99            |
| Step_1-AverageDiscou... | 125           |
| Step_1-AveragePolicyStd | 0.41748646    |
| Step_1-AverageReturn    | 263           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 630           |
| Step_1-MinReturn        | 12.5          |
| Step_1-NumTrajs         | 1184          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 101           |
| Time                    | 2.01e+04      |
| Time-InnerStep          | 0.168         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.905         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007227188   |
| n_timesteps             | 96000000      |
-------------------------------------------

 ---------------- Iteration 300 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 300            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.006414293   |
| LossBefore              | -7.520315e-09  |
| MeanKL                  | 0.007125114    |
| MeanKLBefore            | -2.4058977e-12 |
| Step_0-AverageDiscou... | 120            |
| Step_0-AveragePolicyStd | 0.41722706     |
| Step_0-AverageReturn    | 259            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 551            |
| Step_0-MinReturn        | 10.5           |
| Step_0-NumTrajs         | 1099           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 96.4           |
| Step_1-AverageDiscou... | 125            |
| Step_1-AveragePolicyStd | 0.41715425     |
| Step_1-AverageReturn    | 264            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 605            |
| Step_1-MinReturn        | 13.8           |
| Step_1-NumTrajs         | 1219           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 101            |
| Time                    | 2.02e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.9            |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0064142854   |
| n_timesteps             | 96320000       |
--------------------------------------------

 ---------------- Iteration 301 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 301           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.006693393  |
| LossBefore              | 1.2936419e-09 |
| MeanKL                  | 0.0070882156  |
| MeanKLBefore            | 2.9798841e-09 |
| Step_0-AverageDiscou... | 120           |
| Step_0-AveragePolicyStd | 0.41784123    |
| Step_0-AverageReturn    | 247           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 582           |
| Step_0-MinReturn        | 16.4          |
| Step_0-NumTrajs         | 1221          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 104           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.41772237    |
| Step_1-AverageReturn    | 246           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 605           |
| Step_1-MinReturn        | 16.5          |
| Step_1-NumTrajs         | 1315          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 101           |
| Time                    | 2.03e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.963         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0066933944  |
| n_timesteps             | 96640000      |
-------------------------------------------

 ---------------- Iteration 302 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 302           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0065923096 |
| LossBefore              | 6.9567874e-10 |
| MeanKL                  | 0.007600597   |
| MeanKLBefore            | 4.470582e-09  |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.41695532    |
| Step_0-AverageReturn    | 271           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 623           |
| Step_0-MinReturn        | 9.99          |
| Step_0-NumTrajs         | 1082          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 108           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.41697735    |
| Step_1-AverageReturn    | 270           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 646           |
| Step_1-MinReturn        | 10.8          |
| Step_1-NumTrajs         | 1187          |
| Step_1-PolicyExecTime   | 4.05          |
| Step_1-StdReturn        | 104           |
| Time                    | 2.04e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.912         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.00659231    |
| n_timesteps             | 96960000      |
-------------------------------------------

 ---------------- Iteration 303 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 303            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.006943263   |
| LossBefore              | -8.530416e-09  |
| MeanKL                  | 0.007307194    |
| MeanKLBefore            | -4.4701642e-09 |
| Step_0-AverageDiscou... | 113            |
| Step_0-AveragePolicyStd | 0.41673407     |
| Step_0-AverageReturn    | 230            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 573            |
| Step_0-MinReturn        | 13.6           |
| Step_0-NumTrajs         | 1236           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 106            |
| Step_1-AverageDiscou... | 116            |
| Step_1-AveragePolicyStd | 0.41666874     |
| Step_1-AverageReturn    | 236            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 567            |
| Step_1-MinReturn        | 19             |
| Step_1-NumTrajs         | 1272           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 102            |
| Time                    | 2.04e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.951          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0069432547   |
| n_timesteps             | 97280000       |
--------------------------------------------

 ---------------- Iteration 304 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 304           |
| ItrTime                 | 72.3          |
| LossAfter               | -0.006659546  |
| LossBefore              | 4.253959e-09  |
| MeanKL                  | 0.0074930885  |
| MeanKLBefore            | 5.9595457e-09 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.416015      |
| Step_0-AverageReturn    | 264           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 664           |
| Step_0-MinReturn        | 16.4          |
| Step_0-NumTrajs         | 1100          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.41598225    |
| Step_1-AverageReturn    | 267           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 637           |
| Step_1-MinReturn        | 11.3          |
| Step_1-NumTrajs         | 1134          |
| Step_1-PolicyExecTime   | 4.56          |
| Step_1-StdReturn        | 112           |
| Time                    | 2.05e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.9           |
| Time-Sampling           | 57.5          |
| Time-TotalInner         | 58.6          |
| dLoss                   | 0.00665955    |
| n_timesteps             | 97600000      |
-------------------------------------------

 ---------------- Iteration 305 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 305            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.006776941   |
| LossBefore              | -9.365507e-10  |
| MeanKL                  | 0.0073709935   |
| MeanKLBefore            | -7.4454434e-09 |
| Step_0-AverageDiscou... | 122            |
| Step_0-AveragePolicyStd | 0.4161842      |
| Step_0-AverageReturn    | 256            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 621            |
| Step_0-MinReturn        | 12.9           |
| Step_0-NumTrajs         | 1182           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 122            |
| Step_1-AveragePolicyStd | 0.41604504     |
| Step_1-AverageReturn    | 258            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 524            |
| Step_1-MinReturn        | 6.87           |
| Step_1-NumTrajs         | 1189           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 107            |
| Time                    | 2.06e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.934          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.00677694     |
| n_timesteps             | 97920000       |
--------------------------------------------

 ---------------- Iteration 306 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 306            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0063567436  |
| LossBefore              | -1.2106931e-09 |
| MeanKL                  | 0.007196094    |
| MeanKLBefore            | -5.9602607e-09 |
| Step_0-AverageDiscou... | 112            |
| Step_0-AveragePolicyStd | 0.41615435     |
| Step_0-AverageReturn    | 229            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 536            |
| Step_0-MinReturn        | 14.8           |
| Step_0-NumTrajs         | 1265           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 108            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.4159934      |
| Step_1-AverageReturn    | 252            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 602            |
| Step_1-MinReturn        | -8.72          |
| Step_1-NumTrajs         | 1211           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 109            |
| Time                    | 2.06e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.937          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0063567422   |
| n_timesteps             | 98240000       |
--------------------------------------------

 ---------------- Iteration 307 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 307           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0067486563 |
| LossBefore              | 3.6890757e-10 |
| MeanKL                  | 0.007272944   |
| MeanKLBefore            | 1.9371063e-08 |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.4157056     |
| Step_0-AverageReturn    | 249           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 584           |
| Step_0-MinReturn        | 13.9          |
| Step_0-NumTrajs         | 1183          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 107           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.41564363    |
| Step_1-AverageReturn    | 252           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 647           |
| Step_1-MinReturn        | 16.2          |
| Step_1-NumTrajs         | 1243          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 98.5          |
| Time                    | 2.07e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.93          |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.0067486567  |
| n_timesteps             | 98560000      |
-------------------------------------------

 ---------------- Iteration 308 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 308           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.0064064367 |
| LossBefore              | 5.2280287e-09 |
| MeanKL                  | 0.006995138   |
| MeanKLBefore            | 5.958258e-09  |
| Step_0-AverageDiscou... | 121           |
| Step_0-AveragePolicyStd | 0.41555572    |
| Step_0-AverageReturn    | 253           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 607           |
| Step_0-MinReturn        | 11.6          |
| Step_0-NumTrajs         | 1195          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 103           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.4156299     |
| Step_1-AverageReturn    | 251           |
| Step_1-EnvExecTime      | 22.7          |
| Step_1-MaxReturn        | 558           |
| Step_1-MinReturn        | 11.2          |
| Step_1-NumTrajs         | 1270          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 97.3          |
| Time                    | 2.08e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.934         |
| Time-Sampling           | 54.1          |
| Time-TotalInner         | 55.2          |
| dLoss                   | 0.006406442   |
| n_timesteps             | 98880000      |
-------------------------------------------

 ---------------- Iteration 309 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 309           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.006358309  |
| LossBefore              | -4.529539e-09 |
| MeanKL                  | 0.007793828   |
| MeanKLBefore            | -1.489299e-09 |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.41599703    |
| Step_0-AverageReturn    | 267           |
| Step_0-EnvExecTime      | 24.4          |
| Step_0-MaxReturn        | 617           |
| Step_0-MinReturn        | 16.3          |
| Step_0-NumTrajs         | 1101          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 115           |
| Step_1-AverageDiscou... | 126           |
| Step_1-AveragePolicyStd | 0.4159421     |
| Step_1-AverageReturn    | 261           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 628           |
| Step_1-MinReturn        | 17.6          |
| Step_1-NumTrajs         | 1248          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 105           |
| Time                    | 2.09e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.937         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.0063583045  |
| n_timesteps             | 99200000      |
-------------------------------------------

 ---------------- Iteration 310 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 310            |
| ItrTime                 | 72.1           |
| LossAfter               | -0.0069133355  |
| LossBefore              | -7.958468e-09  |
| MeanKL                  | 0.0069703      |
| MeanKLBefore            | -1.1921211e-08 |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.41556546     |
| Step_0-AverageReturn    | 286            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 601            |
| Step_0-MinReturn        | 18.8           |
| Step_0-NumTrajs         | 1048           |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 93.7           |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.41550544     |
| Step_1-AverageReturn    | 279            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 600            |
| Step_1-MinReturn        | 21.2           |
| Step_1-NumTrajs         | 1138           |
| Step_1-PolicyExecTime   | 4.23           |
| Step_1-StdReturn        | 106            |
| Time                    | 2.09e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.906          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.4           |
| dLoss                   | 0.0069133276   |
| n_timesteps             | 99520000       |
--------------------------------------------

 ---------------- Iteration 311 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 311            |
| ItrTime                 | 72             |
| LossAfter               | -0.0072284555  |
| LossBefore              | 2.4554783e-09  |
| MeanKL                  | 0.008063509    |
| MeanKLBefore            | -1.0430032e-08 |
| Step_0-AverageDiscou... | 125            |
| Step_0-AveragePolicyStd | 0.4149428      |
| Step_0-AverageReturn    | 271            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 628            |
| Step_0-MinReturn        | 17.9           |
| Step_0-NumTrajs         | 1103           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 105            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.41482764     |
| Step_1-AverageReturn    | 274            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 677            |
| Step_1-MinReturn        | 18.3           |
| Step_1-NumTrajs         | 1163           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 105            |
| Time                    | 2.1e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.901          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.4           |
| dLoss                   | 0.007228458    |
| n_timesteps             | 99840000       |
--------------------------------------------

 ---------------- Iteration 312 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 312            |
| ItrTime                 | 71.9           |
| LossAfter               | -0.0068586506  |
| LossBefore              | -9.288049e-09  |
| MeanKL                  | 0.007600674    |
| MeanKLBefore            | -2.9823064e-09 |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.41381997     |
| Step_0-AverageReturn    | 257            |
| Step_0-EnvExecTime      | 24.4           |
| Step_0-MaxReturn        | 632            |
| Step_0-MinReturn        | 11.3           |
| Step_0-NumTrajs         | 1068           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 128            |
| Step_1-AverageDiscou... | 120            |
| Step_1-AveragePolicyStd | 0.41376996     |
| Step_1-AverageReturn    | 253            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 603            |
| Step_1-MinReturn        | 11.7           |
| Step_1-NumTrajs         | 1212           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 115            |
| Time                    | 2.11e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.903          |
| Time-Sampling           | 57.1           |
| Time-TotalInner         | 58.2           |
| dLoss                   | 0.0068586413   |
| n_timesteps             | 100160000      |
--------------------------------------------

 ---------------- Iteration 313 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 313           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.007030642  |
| LossBefore              | 1.3679509e-09 |
| MeanKL                  | 0.007109145   |
| MeanKLBefore            | 1.4908897e-09 |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.4133075     |
| Step_0-AverageReturn    | 253           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 582           |
| Step_0-MinReturn        | 17.2          |
| Step_0-NumTrajs         | 1093          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 105           |
| Step_1-AverageDiscou... | 126           |
| Step_1-AveragePolicyStd | 0.41323274    |
| Step_1-AverageReturn    | 272           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 711           |
| Step_1-MinReturn        | 18.7          |
| Step_1-NumTrajs         | 1140          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 109           |
| Time                    | 2.11e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.905         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.0070306435  |
| n_timesteps             | 100480000     |
-------------------------------------------

 ---------------- Iteration 314 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 314            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0064843656  |
| LossBefore              | -1.5968304e-09 |
| MeanKL                  | 0.0074337525   |
| MeanKLBefore            | 2.0859673e-08  |
| Step_0-AverageDiscou... | 111            |
| Step_0-AveragePolicyStd | 0.41313684     |
| Step_0-AverageReturn    | 235            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 609            |
| Step_0-MinReturn        | 4.48           |
| Step_0-NumTrajs         | 1174           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 114            |
| Step_1-AveragePolicyStd | 0.41304097     |
| Step_1-AverageReturn    | 237            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 584            |
| Step_1-MinReturn        | 13.6           |
| Step_1-NumTrajs         | 1229           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 116            |
| Time                    | 2.12e+04       |
| Time-InnerStep          | 0.177          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.943          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.006484364    |
| n_timesteps             | 100800000      |
--------------------------------------------

 ---------------- Iteration 315 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 315            |
| ItrTime                 | 72.6           |
| LossAfter               | -0.00735704    |
| LossBefore              | -1.0600992e-09 |
| MeanKL                  | 0.0076385876   |
| MeanKLBefore            | 2.9806575e-09  |
| Step_0-AverageDiscou... | 127            |
| Step_0-AveragePolicyStd | 0.41377127     |
| Step_0-AverageReturn    | 284            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 604            |
| Step_0-MinReturn        | 27             |
| Step_0-NumTrajs         | 1069           |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 107            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.41375476     |
| Step_1-AverageReturn    | 282            |
| Step_1-EnvExecTime      | 24.3           |
| Step_1-MaxReturn        | 600            |
| Step_1-MinReturn        | 19.7           |
| Step_1-NumTrajs         | 1123           |
| Step_1-PolicyExecTime   | 4.18           |
| Step_1-StdReturn        | 109            |
| Time                    | 2.13e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.897          |
| Time-Sampling           | 57.8           |
| Time-TotalInner         | 58.9           |
| dLoss                   | 0.007357039    |
| n_timesteps             | 101120000      |
--------------------------------------------

 ---------------- Iteration 316 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 316           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.0068059354 |
| LossBefore              | 6.4538535e-09 |
| MeanKL                  | 0.007404033   |
| MeanKLBefore            | 5.7784886e-13 |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.41330528    |
| Step_0-AverageReturn    | 251           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 644           |
| Step_0-MinReturn        | 15.6          |
| Step_0-NumTrajs         | 1153          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 105           |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.41326424    |
| Step_1-AverageReturn    | 270           |
| Step_1-EnvExecTime      | 24.5          |
| Step_1-MaxReturn        | 614           |
| Step_1-MinReturn        | 19.9          |
| Step_1-NumTrajs         | 1114          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 104           |
| Time                    | 2.14e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.904         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.006805942   |
| n_timesteps             | 101440000     |
-------------------------------------------

 ---------------- Iteration 317 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 317            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0067101     |
| LossBefore              | 5.8943277e-09  |
| MeanKL                  | 0.0071089654   |
| MeanKLBefore            | -4.4669126e-09 |
| Step_0-AverageDiscou... | 119            |
| Step_0-AveragePolicyStd | 0.41339853     |
| Step_0-AverageReturn    | 251            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 579            |
| Step_0-MinReturn        | 8.12           |
| Step_0-NumTrajs         | 1199           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 117            |
| Step_1-AveragePolicyStd | 0.41333356     |
| Step_1-AverageReturn    | 247            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 636            |
| Step_1-MinReturn        | 8.63           |
| Step_1-NumTrajs         | 1208           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 112            |
| Time                    | 2.14e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.925          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.006710106    |
| n_timesteps             | 101760000      |
--------------------------------------------

 ---------------- Iteration 318 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 318           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0070002303 |
| LossBefore              | -6.370325e-10 |
| MeanKL                  | 0.007676117   |
| MeanKLBefore            | 7.44882e-09   |
| Step_0-AverageDiscou... | 121           |
| Step_0-AveragePolicyStd | 0.4144098     |
| Step_0-AverageReturn    | 259           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 618           |
| Step_0-MinReturn        | 10.8          |
| Step_0-NumTrajs         | 1140          |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 118           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.41440886    |
| Step_1-AverageReturn    | 249           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 597           |
| Step_1-MinReturn        | 11.1          |
| Step_1-NumTrajs         | 1282          |
| Step_1-PolicyExecTime   | 4.37          |
| Step_1-StdReturn        | 106           |
| Time                    | 2.15e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.944         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.00700023    |
| n_timesteps             | 102080000     |
-------------------------------------------

 ---------------- Iteration 319 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 319           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.006388156  |
| LossBefore              | 5.7276797e-09 |
| MeanKL                  | 0.007896398   |
| MeanKLBefore            | 7.4502857e-09 |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.41491187    |
| Step_0-AverageReturn    | 252           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 576           |
| Step_0-MinReturn        | 18.2          |
| Step_0-NumTrajs         | 1149          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 112           |
| Step_1-AverageDiscou... | 120           |
| Step_1-AveragePolicyStd | 0.4149297     |
| Step_1-AverageReturn    | 248           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 595           |
| Step_1-MinReturn        | 9.93          |
| Step_1-NumTrajs         | 1251          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 105           |
| Time                    | 2.16e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.956         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.006388162   |
| n_timesteps             | 102400000     |
-------------------------------------------

 ---------------- Iteration 320 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 320           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0063639185 |
| LossBefore              | 1.729342e-10  |
| MeanKL                  | 0.0076189437  |
| MeanKLBefore            | 5.9589764e-09 |
| Step_0-AverageDiscou... | 125           |
| Step_0-AveragePolicyStd | 0.41524678    |
| Step_0-AverageReturn    | 272           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 739           |
| Step_0-MinReturn        | 13.8          |
| Step_0-NumTrajs         | 1092          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.4151952     |
| Step_1-AverageReturn    | 267           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 642           |
| Step_1-MinReturn        | 16.8          |
| Step_1-NumTrajs         | 1183          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 103           |
| Time                    | 2.16e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.928         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.0063639185  |
| n_timesteps             | 102720000     |
-------------------------------------------

 ---------------- Iteration 321 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 321            |
| ItrTime                 | 72.4           |
| LossAfter               | -0.007128251   |
| LossBefore              | -4.7128914e-09 |
| MeanKL                  | 0.0082719475   |
| MeanKLBefore            | 1.4886382e-09  |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.4148394      |
| Step_0-AverageReturn    | 271            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 591            |
| Step_0-MinReturn        | 22.3           |
| Step_0-NumTrajs         | 1103           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 106            |
| Step_1-AverageDiscou... | 124            |
| Step_1-AveragePolicyStd | 0.41475055     |
| Step_1-AverageReturn    | 264            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 650            |
| Step_1-MinReturn        | 16.5           |
| Step_1-NumTrajs         | 1184           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 101            |
| Time                    | 2.17e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.931          |
| Time-Sampling           | 57.5           |
| Time-TotalInner         | 58.7           |
| dLoss                   | 0.007128246    |
| n_timesteps             | 103040000      |
--------------------------------------------

 ---------------- Iteration 322 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 322            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.006721516   |
| LossBefore              | 3.333998e-09   |
| MeanKL                  | 0.007644389    |
| MeanKLBefore            | -5.9593646e-09 |
| Step_0-AverageDiscou... | 118            |
| Step_0-AveragePolicyStd | 0.41455305     |
| Step_0-AverageReturn    | 248            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 588            |
| Step_0-MinReturn        | 15             |
| Step_0-NumTrajs         | 1160           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 117            |
| Step_1-AverageDiscou... | 126            |
| Step_1-AveragePolicyStd | 0.41459718     |
| Step_1-AverageReturn    | 259            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 629            |
| Step_1-MinReturn        | 19.9           |
| Step_1-NumTrajs         | 1266           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 108            |
| Time                    | 2.18e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.925          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0067215194   |
| n_timesteps             | 103360000      |
--------------------------------------------

 ---------------- Iteration 323 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 323            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007077475   |
| LossBefore              | -3.4255003e-09 |
| MeanKL                  | 0.008236753    |
| MeanKLBefore            | -1.4897585e-08 |
| Step_0-AverageDiscou... | 117            |
| Step_0-AveragePolicyStd | 0.4144203      |
| Step_0-AverageReturn    | 244            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 723            |
| Step_0-MinReturn        | 19.5           |
| Step_0-NumTrajs         | 1147           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.41440168     |
| Step_1-AverageReturn    | 268            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 685            |
| Step_1-MinReturn        | 22.4           |
| Step_1-NumTrajs         | 1214           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 110            |
| Time                    | 2.19e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.948          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.007077472    |
| n_timesteps             | 103680000      |
--------------------------------------------

 ---------------- Iteration 324 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 324            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0069669075  |
| LossBefore              | -1.9851867e-09 |
| MeanKL                  | 0.008079911    |
| MeanKLBefore            | 1.0428579e-08  |
| Step_0-AverageDiscou... | 122            |
| Step_0-AveragePolicyStd | 0.41414043     |
| Step_0-AverageReturn    | 266            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 581            |
| Step_0-MinReturn        | 15.6           |
| Step_0-NumTrajs         | 1093           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 125            |
| Step_1-AveragePolicyStd | 0.41421166     |
| Step_1-AverageReturn    | 265            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 663            |
| Step_1-MinReturn        | 22             |
| Step_1-NumTrajs         | 1204           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 110            |
| Time                    | 2.19e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.898          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0069669057   |
| n_timesteps             | 104000000      |
--------------------------------------------

 ---------------- Iteration 325 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 325           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0069557605 |
| LossBefore              | 4.7800013e-09 |
| MeanKL                  | 0.0075814067  |
| MeanKLBefore            | 1.4899509e-09 |
| Step_0-AverageDiscou... | 126           |
| Step_0-AveragePolicyStd | 0.41489407    |
| Step_0-AverageReturn    | 279           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 607           |
| Step_0-MinReturn        | 17.8          |
| Step_0-NumTrajs         | 1038          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 102           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.41483635    |
| Step_1-AverageReturn    | 273           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 620           |
| Step_1-MinReturn        | 15.4          |
| Step_1-NumTrajs         | 1150          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 103           |
| Time                    | 2.2e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.931         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.006955765   |
| n_timesteps             | 104320000     |
-------------------------------------------

 ---------------- Iteration 326 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 326           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0068762177 |
| LossBefore              | -1.650505e-09 |
| MeanKL                  | 0.0071371347  |
| MeanKLBefore            | -4.46936e-09  |
| Step_0-AverageDiscou... | 121           |
| Step_0-AveragePolicyStd | 0.41444546    |
| Step_0-AverageReturn    | 259           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 608           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 1102          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.41444734    |
| Step_1-AverageReturn    | 262           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 622           |
| Step_1-MinReturn        | 19.4          |
| Step_1-NumTrajs         | 1179          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 110           |
| Time                    | 2.21e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.888         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.006876216   |
| n_timesteps             | 104640000     |
-------------------------------------------

 ---------------- Iteration 327 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 327           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.006515174  |
| LossBefore              | -2.188831e-09 |
| MeanKL                  | 0.0083089145  |
| MeanKLBefore            | -7.447615e-09 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.41452798    |
| Step_0-AverageReturn    | 263           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 558           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1100          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 104           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.41450748    |
| Step_1-AverageReturn    | 272           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 660           |
| Step_1-MinReturn        | 17.8          |
| Step_1-NumTrajs         | 1182          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 113           |
| Time                    | 2.21e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.91          |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.006515172   |
| n_timesteps             | 104960000     |
-------------------------------------------

 ---------------- Iteration 328 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 328           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0068081147 |
| LossBefore              | 7.4312454e-09 |
| MeanKL                  | 0.007720052   |
| MeanKLBefore            | 2.981332e-09  |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.41515687    |
| Step_0-AverageReturn    | 266           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 629           |
| Step_0-MinReturn        | 13.7          |
| Step_0-NumTrajs         | 1091          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 110           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.4150455     |
| Step_1-AverageReturn    | 272           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 606           |
| Step_1-MinReturn        | 16.4          |
| Step_1-NumTrajs         | 1166          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 115           |
| Time                    | 2.22e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.902         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.006808122   |
| n_timesteps             | 105280000     |
-------------------------------------------

 ---------------- Iteration 329 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 329           |
| ItrTime                 | 71            |
| LossAfter               | -0.006449341  |
| LossBefore              | -5.707339e-09 |
| MeanKL                  | 0.007172142   |
| MeanKLBefore            | 8.940485e-09  |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.41500103    |
| Step_0-AverageReturn    | 264           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 569           |
| Step_0-MinReturn        | -26.9         |
| Step_0-NumTrajs         | 1113          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 104           |
| Step_1-AverageDiscou... | 125           |
| Step_1-AveragePolicyStd | 0.41502234    |
| Step_1-AverageReturn    | 260           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 629           |
| Step_1-MinReturn        | -2.61         |
| Step_1-NumTrajs         | 1205          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 109           |
| Time                    | 2.23e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.924         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0064493353  |
| n_timesteps             | 105600000     |
-------------------------------------------

 ---------------- Iteration 330 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 330           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0068458347 |
| LossBefore              | -3.253612e-09 |
| MeanKL                  | 0.007523208   |
| MeanKLBefore            | 1.4887254e-09 |
| Step_0-AverageDiscou... | 115           |
| Step_0-AveragePolicyStd | 0.41510344    |
| Step_0-AverageReturn    | 243           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 604           |
| Step_0-MinReturn        | 8.25          |
| Step_0-NumTrajs         | 1149          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 120           |
| Step_1-AveragePolicyStd | 0.41504878    |
| Step_1-AverageReturn    | 247           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 651           |
| Step_1-MinReturn        | 11.4          |
| Step_1-NumTrajs         | 1239          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 111           |
| Time                    | 2.24e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.922         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.0068458314  |
| n_timesteps             | 105920000     |
-------------------------------------------

 ---------------- Iteration 331 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 331           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0067297937 |
| LossBefore              | 7.568336e-09  |
| MeanKL                  | 0.007970311   |
| MeanKLBefore            | 1.4897367e-08 |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.41549933    |
| Step_0-AverageReturn    | 259           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 587           |
| Step_0-MinReturn        | 5.88          |
| Step_0-NumTrajs         | 1117          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 92.4          |
| Step_1-AverageDiscou... | 129           |
| Step_1-AveragePolicyStd | 0.4154527     |
| Step_1-AverageReturn    | 275           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 641           |
| Step_1-MinReturn        | 9.35          |
| Step_1-NumTrajs         | 1165          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 103           |
| Time                    | 2.24e+04      |
| Time-InnerStep          | 0.177         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.944         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.006729801   |
| n_timesteps             | 106240000     |
-------------------------------------------

 ---------------- Iteration 332 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 332           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.006636031  |
| LossBefore              | -3.891972e-09 |
| MeanKL                  | 0.00784237    |
| MeanKLBefore            | -2.981265e-09 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.41502145    |
| Step_0-AverageReturn    | 263           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 590           |
| Step_0-MinReturn        | 23.3          |
| Step_0-NumTrajs         | 1096          |
| Step_0-PolicyExecTime   | 1.77          |
| Step_0-StdReturn        | 115           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.4149087     |
| Step_1-AverageReturn    | 250           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 633           |
| Step_1-MinReturn        | 19.9          |
| Step_1-NumTrajs         | 1272          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 112           |
| Time                    | 2.25e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.938         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.0066360272  |
| n_timesteps             | 106560000     |
-------------------------------------------

 ---------------- Iteration 333 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 333            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.006881904   |
| LossBefore              | 1.8131478e-10  |
| MeanKL                  | 0.0076825963   |
| MeanKLBefore            | -4.4687107e-09 |
| Step_0-AverageDiscou... | 123            |
| Step_0-AveragePolicyStd | 0.41515815     |
| Step_0-AverageReturn    | 271            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 603            |
| Step_0-MinReturn        | 3.91           |
| Step_0-NumTrajs         | 1044           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 98.9           |
| Step_1-AverageDiscou... | 129            |
| Step_1-AveragePolicyStd | 0.41507214     |
| Step_1-AverageReturn    | 285            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 634            |
| Step_1-MinReturn        | 23             |
| Step_1-NumTrajs         | 1101           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 102            |
| Time                    | 2.26e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.873          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.006881904    |
| n_timesteps             | 106880000      |
--------------------------------------------

 ---------------- Iteration 334 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 334           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.007101479  |
| LossBefore              | -9.904948e-11 |
| MeanKL                  | 0.008489419   |
| MeanKLBefore            | 1.1921371e-08 |
| Step_0-AverageDiscou... | 125           |
| Step_0-AveragePolicyStd | 0.41521758    |
| Step_0-AverageReturn    | 278           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 603           |
| Step_0-MinReturn        | 12.2          |
| Step_0-NumTrajs         | 1069          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 108           |
| Step_1-AverageDiscou... | 127           |
| Step_1-AveragePolicyStd | 0.41517067    |
| Step_1-AverageReturn    | 281           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 623           |
| Step_1-MinReturn        | 9.74          |
| Step_1-NumTrajs         | 1125          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 109           |
| Time                    | 2.26e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.914         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.007101479   |
| n_timesteps             | 107200000     |
-------------------------------------------

 ---------------- Iteration 335 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 335            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0067125903  |
| LossBefore              | -8.9403007e-10 |
| MeanKL                  | 0.0076825814   |
| MeanKLBefore            | 4.470055e-09   |
| Step_0-AverageDiscou... | 116            |
| Step_0-AveragePolicyStd | 0.41550326     |
| Step_0-AverageReturn    | 248            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 534            |
| Step_0-MinReturn        | 11.8           |
| Step_0-NumTrajs         | 1102           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 119            |
| Step_1-AveragePolicyStd | 0.4155513      |
| Step_1-AverageReturn    | 255            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 638            |
| Step_1-MinReturn        | 16.2           |
| Step_1-NumTrajs         | 1136           |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 112            |
| Time                    | 2.27e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.946          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0067125894   |
| n_timesteps             | 107520000      |
--------------------------------------------

 ---------------- Iteration 336 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 336           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0071800007 |
| LossBefore              | 2.748583e-09  |
| MeanKL                  | 0.008087145   |
| MeanKLBefore            | -7.448448e-09 |
| Step_0-AverageDiscou... | 117           |
| Step_0-AveragePolicyStd | 0.4150536     |
| Step_0-AverageReturn    | 249           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 572           |
| Step_0-MinReturn        | 4.73          |
| Step_0-NumTrajs         | 1138          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 120           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.4150188     |
| Step_1-AverageReturn    | 262           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 636           |
| Step_1-MinReturn        | 20.2          |
| Step_1-NumTrajs         | 1162          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 114           |
| Time                    | 2.28e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.929         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.0071800034  |
| n_timesteps             | 107840000     |
-------------------------------------------

 ---------------- Iteration 337 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 337            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.007231222   |
| LossBefore              | -7.5993025e-09 |
| MeanKL                  | 0.0073996084   |
| MeanKLBefore            | -8.940073e-09  |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.41402826     |
| Step_0-AverageReturn    | 277            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 619            |
| Step_0-MinReturn        | 13.3           |
| Step_0-NumTrajs         | 1040           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 98.3           |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.41396567     |
| Step_1-AverageReturn    | 281            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 691            |
| Step_1-MinReturn        | 9.63           |
| Step_1-NumTrajs         | 1185           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 112            |
| Time                    | 2.29e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.898          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0072312145   |
| n_timesteps             | 108160000      |
--------------------------------------------

 ---------------- Iteration 338 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 338            |
| ItrTime                 | 71             |
| LossAfter               | -0.0069514127  |
| LossBefore              | -6.8716255e-09 |
| MeanKL                  | 0.007662383    |
| MeanKLBefore            | -2.2349386e-08 |
| Step_0-AverageDiscou... | 121            |
| Step_0-AveragePolicyStd | 0.41313413     |
| Step_0-AverageReturn    | 263            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 533            |
| Step_0-MinReturn        | 6.75           |
| Step_0-NumTrajs         | 1103           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 123            |
| Step_1-AveragePolicyStd | 0.41311246     |
| Step_1-AverageReturn    | 262            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 677            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1186           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 118            |
| Time                    | 2.29e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.903          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0069514057   |
| n_timesteps             | 108480000      |
--------------------------------------------

 ---------------- Iteration 339 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 339            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0070264363  |
| LossBefore              | -6.6845196e-09 |
| MeanKL                  | 0.0077471733   |
| MeanKLBefore            | -1.3408325e-08 |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.41172367     |
| Step_0-AverageReturn    | 280            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 556            |
| Step_0-MinReturn        | 30.8           |
| Step_0-NumTrajs         | 1074           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 93             |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.41165376     |
| Step_1-AverageReturn    | 279            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 622            |
| Step_1-MinReturn        | 30.7           |
| Step_1-NumTrajs         | 1179           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 100            |
| Time                    | 2.3e+04        |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.947          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0070264298   |
| n_timesteps             | 108800000      |
--------------------------------------------

 ---------------- Iteration 340 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 340            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0065849186  |
| LossBefore              | -7.0340405e-09 |
| MeanKL                  | 0.00748927     |
| MeanKLBefore            | 1.4919813e-09  |
| Step_0-AverageDiscou... | 128            |
| Step_0-AveragePolicyStd | 0.41148934     |
| Step_0-AverageReturn    | 281            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 642            |
| Step_0-MinReturn        | 39.8           |
| Step_0-NumTrajs         | 1077           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 97.3           |
| Step_1-AverageDiscou... | 132            |
| Step_1-AveragePolicyStd | 0.41140202     |
| Step_1-AverageReturn    | 290            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 642            |
| Step_1-MinReturn        | 33.5           |
| Step_1-NumTrajs         | 1129           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 99.2           |
| Time                    | 2.31e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.911          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.0065849116   |
| n_timesteps             | 109120000      |
--------------------------------------------

 ---------------- Iteration 341 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 341           |
| ItrTime                 | 72.1          |
| LossAfter               | -0.006700393  |
| LossBefore              | -1.283024e-08 |
| MeanKL                  | 0.0077978224  |
| MeanKLBefore            | 2.9814486e-09 |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.4116011     |
| Step_0-AverageReturn    | 253           |
| Step_0-EnvExecTime      | 24.6          |
| Step_0-MaxReturn        | 638           |
| Step_0-MinReturn        | 9.54          |
| Step_0-NumTrajs         | 1113          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 107           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.41148385    |
| Step_1-AverageReturn    | 262           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 650           |
| Step_1-MinReturn        | 9.14          |
| Step_1-NumTrajs         | 1153          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 111           |
| Time                    | 2.31e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.916         |
| Time-Sampling           | 57.3          |
| Time-TotalInner         | 58.4          |
| dLoss                   | 0.00670038    |
| n_timesteps             | 109440000     |
-------------------------------------------

 ---------------- Iteration 342 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 342           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0068092    |
| LossBefore              | 4.3359077e-10 |
| MeanKL                  | 0.007940496   |
| MeanKLBefore            | -4.902745e-13 |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.41145495    |
| Step_0-AverageReturn    | 265           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 594           |
| Step_0-MinReturn        | 10.8          |
| Step_0-NumTrajs         | 1172          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 114           |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.41136038    |
| Step_1-AverageReturn    | 257           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 615           |
| Step_1-MinReturn        | 12.7          |
| Step_1-NumTrajs         | 1269          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 115           |
| Time                    | 2.32e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.934         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0068092006  |
| n_timesteps             | 109760000     |
-------------------------------------------

 ---------------- Iteration 343 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 343           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.007044642  |
| LossBefore              | 4.284752e-10  |
| MeanKL                  | 0.007591625   |
| MeanKLBefore            | -6.316725e-13 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.41096526    |
| Step_0-AverageReturn    | 259           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 608           |
| Step_0-MinReturn        | 15.7          |
| Step_0-NumTrajs         | 1179          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 112           |
| Step_1-AverageDiscou... | 126           |
| Step_1-AveragePolicyStd | 0.41091603    |
| Step_1-AverageReturn    | 269           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 610           |
| Step_1-MinReturn        | 11.6          |
| Step_1-NumTrajs         | 1184          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 111           |
| Time                    | 2.33e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.936         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0070446427  |
| n_timesteps             | 110080000     |
-------------------------------------------

 ---------------- Iteration 344 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 344            |
| ItrTime                 | 71.9           |
| LossAfter               | -0.00696325    |
| LossBefore              | -3.8775633e-10 |
| MeanKL                  | 0.008691962    |
| MeanKLBefore            | -2.1245228e-13 |
| Step_0-AverageDiscou... | 123            |
| Step_0-AveragePolicyStd | 0.41071755     |
| Step_0-AverageReturn    | 275            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 581            |
| Step_0-MinReturn        | 10.1           |
| Step_0-NumTrajs         | 1035           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 105            |
| Step_1-AverageDiscou... | 125            |
| Step_1-AveragePolicyStd | 0.41064826     |
| Step_1-AverageReturn    | 268            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 587            |
| Step_1-MinReturn        | 10.3           |
| Step_1-NumTrajs         | 1172           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 112            |
| Time                    | 2.33e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.887          |
| Time-Sampling           | 57.1           |
| Time-TotalInner         | 58.1           |
| dLoss                   | 0.0069632498   |
| n_timesteps             | 110400000      |
--------------------------------------------

 ---------------- Iteration 345 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 345           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.006976667  |
| LossBefore              | 7.3124298e-09 |
| MeanKL                  | 0.008007002   |
| MeanKLBefore            | 2.9818576e-09 |
| Step_0-AverageDiscou... | 118           |
| Step_0-AveragePolicyStd | 0.41067943    |
| Step_0-AverageReturn    | 255           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 571           |
| Step_0-MinReturn        | 9.04          |
| Step_0-NumTrajs         | 1140          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 131           |
| Step_1-AverageDiscou... | 122           |
| Step_1-AveragePolicyStd | 0.41050118    |
| Step_1-AverageReturn    | 256           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 593           |
| Step_1-MinReturn        | 15.8          |
| Step_1-NumTrajs         | 1211          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 120           |
| Time                    | 2.34e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.936         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.0069766743  |
| n_timesteps             | 110720000     |
-------------------------------------------

 ---------------- Iteration 346 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 346           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.0065742983 |
| LossBefore              | 3.2232457e-09 |
| MeanKL                  | 0.0074502626  |
| MeanKLBefore            | 2.9799263e-09 |
| Step_0-AverageDiscou... | 116           |
| Step_0-AveragePolicyStd | 0.4100915     |
| Step_0-AverageReturn    | 248           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 616           |
| Step_0-MinReturn        | 5.13          |
| Step_0-NumTrajs         | 1135          |
| Step_0-PolicyExecTime   | 1.71          |
| Step_0-StdReturn        | 125           |
| Step_1-AverageDiscou... | 121           |
| Step_1-AveragePolicyStd | 0.40995982    |
| Step_1-AverageReturn    | 250           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 640           |
| Step_1-MinReturn        | 6.89          |
| Step_1-NumTrajs         | 1260          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 128           |
| Time                    | 2.35e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.941         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0065743015  |
| n_timesteps             | 111040000     |
-------------------------------------------

 ---------------- Iteration 347 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 347           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.006663342  |
| LossBefore              | -6.583195e-09 |
| MeanKL                  | 0.008017977   |
| MeanKLBefore            | 2.980994e-09  |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.40958095    |
| Step_0-AverageReturn    | 259           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 583           |
| Step_0-MinReturn        | 13.1          |
| Step_0-NumTrajs         | 1147          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 108           |
| Step_1-AverageDiscou... | 126           |
| Step_1-AveragePolicyStd | 0.40950456    |
| Step_1-AverageReturn    | 268           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 613           |
| Step_1-MinReturn        | 14.5          |
| Step_1-NumTrajs         | 1199          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 109           |
| Time                    | 2.36e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.945         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0066633355  |
| n_timesteps             | 111360000     |
-------------------------------------------

 ---------------- Iteration 348 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 348            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.006720043   |
| LossBefore              | -2.7984721e-09 |
| MeanKL                  | 0.0068428097   |
| MeanKLBefore            | 5.9608687e-09  |
| Step_0-AverageDiscou... | 135            |
| Step_0-AveragePolicyStd | 0.41125065     |
| Step_0-AverageReturn    | 303            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 637            |
| Step_0-MinReturn        | 15.4           |
| Step_0-NumTrajs         | 1045           |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 103            |
| Step_1-AverageDiscou... | 133            |
| Step_1-AveragePolicyStd | 0.41118667     |
| Step_1-AverageReturn    | 292            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 680            |
| Step_1-MinReturn        | 13.7           |
| Step_1-NumTrajs         | 1130           |
| Step_1-PolicyExecTime   | 4.18           |
| Step_1-StdReturn        | 109            |
| Time                    | 2.36e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.892          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.00672004     |
| n_timesteps             | 111680000      |
--------------------------------------------

 ---------------- Iteration 349 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 349            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.007042085   |
| LossBefore              | -7.3320516e-10 |
| MeanKL                  | 0.0075484933   |
| MeanKLBefore            | -8.939281e-09  |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.41174942     |
| Step_0-AverageReturn    | 270            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 646            |
| Step_0-MinReturn        | 6.1            |
| Step_0-NumTrajs         | 1138           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 107            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.4117215      |
| Step_1-AverageReturn    | 268            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 601            |
| Step_1-MinReturn        | 8.63           |
| Step_1-NumTrajs         | 1205           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 104            |
| Time                    | 2.37e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.905          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007042084    |
| n_timesteps             | 112000000      |
--------------------------------------------

 ---------------- Iteration 350 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 350             |
| ItrTime                 | 71              |
| LossAfter               | -0.0068259165   |
| LossBefore              | -4.735452e-09   |
| MeanKL                  | 0.008395249     |
| MeanKLBefore            | -1.48984585e-08 |
| Step_0-AverageDiscou... | 118             |
| Step_0-AveragePolicyStd | 0.41134763      |
| Step_0-AverageReturn    | 258             |
| Step_0-EnvExecTime      | 23.8            |
| Step_0-MaxReturn        | 611             |
| Step_0-MinReturn        | 13.4            |
| Step_0-NumTrajs         | 1145            |
| Step_0-PolicyExecTime   | 1.34            |
| Step_0-StdReturn        | 132             |
| Step_1-AverageDiscou... | 126             |
| Step_1-AveragePolicyStd | 0.41137382      |
| Step_1-AverageReturn    | 268             |
| Step_1-EnvExecTime      | 23.5            |
| Step_1-MaxReturn        | 661             |
| Step_1-MinReturn        | 15.2            |
| Step_1-NumTrajs         | 1223            |
| Step_1-PolicyExecTime   | 4.12            |
| Step_1-StdReturn        | 123             |
| Time                    | 2.38e+04        |
| Time-InnerStep          | 0.167           |
| Time-MAMLSteps          | 13.7            |
| Time-OuterStep          | 13.7            |
| Time-SampleProc         | 0.913           |
| Time-Sampling           | 56.1            |
| Time-TotalInner         | 57.3            |
| dLoss                   | 0.006825912     |
| n_timesteps             | 112320000       |
---------------------------------------------

 ---------------- Iteration 351 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 351            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.006297618   |
| LossBefore              | -8.465899e-09  |
| MeanKL                  | 0.007538332    |
| MeanKLBefore            | 1.19210855e-08 |
| Step_0-AverageDiscou... | 122            |
| Step_0-AveragePolicyStd | 0.41106597     |
| Step_0-AverageReturn    | 257            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 567            |
| Step_0-MinReturn        | 20.5           |
| Step_0-NumTrajs         | 1184           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 101            |
| Step_1-AverageDiscou... | 123            |
| Step_1-AveragePolicyStd | 0.41111118     |
| Step_1-AverageReturn    | 259            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 602            |
| Step_1-MinReturn        | 19.5           |
| Step_1-NumTrajs         | 1228           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 116            |
| Time                    | 2.38e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.94           |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.00629761     |
| n_timesteps             | 112640000      |
--------------------------------------------

 ---------------- Iteration 352 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 352            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.006609743   |
| LossBefore              | -9.673826e-11  |
| MeanKL                  | 0.0074286563   |
| MeanKLBefore            | -1.3409979e-08 |
| Step_0-AverageDiscou... | 122            |
| Step_0-AveragePolicyStd | 0.41063422     |
| Step_0-AverageReturn    | 261            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 581            |
| Step_0-MinReturn        | 20.8           |
| Step_0-NumTrajs         | 1143           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 126            |
| Step_1-AveragePolicyStd | 0.4106391      |
| Step_1-AverageReturn    | 271            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 612            |
| Step_1-MinReturn        | 17.9           |
| Step_1-NumTrajs         | 1180           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 117            |
| Time                    | 2.39e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.929          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.006609743    |
| n_timesteps             | 112960000      |
--------------------------------------------

 ---------------- Iteration 353 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 353            |
| ItrTime                 | 72             |
| LossAfter               | -0.006826195   |
| LossBefore              | -1.1015585e-08 |
| MeanKL                  | 0.008330119    |
| MeanKLBefore            | -1.4899854e-09 |
| Step_0-AverageDiscou... | 125            |
| Step_0-AveragePolicyStd | 0.41040266     |
| Step_0-AverageReturn    | 275            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 602            |
| Step_0-MinReturn        | 17.5           |
| Step_0-NumTrajs         | 1072           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 105            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.4103156      |
| Step_1-AverageReturn    | 282            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 604            |
| Step_1-MinReturn        | 26.4           |
| Step_1-NumTrajs         | 1121           |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 112            |
| Time                    | 2.4e+04        |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.897          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.4           |
| dLoss                   | 0.0068261838   |
| n_timesteps             | 113280000      |
--------------------------------------------

 ---------------- Iteration 354 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 354            |
| ItrTime                 | 72.9           |
| LossAfter               | -0.006864631   |
| LossBefore              | -2.3862765e-09 |
| MeanKL                  | 0.008129083    |
| MeanKLBefore            | -5.958976e-09  |
| Step_0-AverageDiscou... | 119            |
| Step_0-AveragePolicyStd | 0.41015497     |
| Step_0-AverageReturn    | 263            |
| Step_0-EnvExecTime      | 24.6           |
| Step_0-MaxReturn        | 614            |
| Step_0-MinReturn        | 12.1           |
| Step_0-NumTrajs         | 1076           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 120            |
| Step_1-AverageDiscou... | 125            |
| Step_1-AveragePolicyStd | 0.41001412     |
| Step_1-AverageReturn    | 275            |
| Step_1-EnvExecTime      | 24.7           |
| Step_1-MaxReturn        | 656            |
| Step_1-MinReturn        | 16             |
| Step_1-NumTrajs         | 1116           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 120            |
| Time                    | 2.41e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.89           |
| Time-Sampling           | 58.1           |
| Time-TotalInner         | 59.2           |
| dLoss                   | 0.0068646288   |
| n_timesteps             | 113600000      |
--------------------------------------------

 ---------------- Iteration 355 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 355            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0067577683  |
| LossBefore              | -4.1560537e-09 |
| MeanKL                  | 0.0073743565   |
| MeanKLBefore            | -7.4472526e-09 |
| Step_0-AverageDiscou... | 119            |
| Step_0-AveragePolicyStd | 0.4107308      |
| Step_0-AverageReturn    | 255            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 607            |
| Step_0-MinReturn        | 7.59           |
| Step_0-NumTrajs         | 1145           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 122            |
| Step_1-AverageDiscou... | 125            |
| Step_1-AveragePolicyStd | 0.4105655      |
| Step_1-AverageReturn    | 267            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 749            |
| Step_1-MinReturn        | 15.1           |
| Step_1-NumTrajs         | 1174           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 126            |
| Time                    | 2.41e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.911          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.006757764    |
| n_timesteps             | 113920000      |
--------------------------------------------

 ---------------- Iteration 356 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 356           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.007008061  |
| LossBefore              | 7.3913577e-09 |
| MeanKL                  | 0.007822701   |
| MeanKLBefore            | 1.3408323e-08 |
| Step_0-AverageDiscou... | 132           |
| Step_0-AveragePolicyStd | 0.41103518    |
| Step_0-AverageReturn    | 289           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 674           |
| Step_0-MinReturn        | 20.3          |
| Step_0-NumTrajs         | 1100          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 103           |
| Step_1-AverageDiscou... | 132           |
| Step_1-AveragePolicyStd | 0.41097304    |
| Step_1-AverageReturn    | 276           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 608           |
| Step_1-MinReturn        | 25.2          |
| Step_1-NumTrajs         | 1213          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 103           |
| Time                    | 2.42e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.894         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0070080683  |
| n_timesteps             | 114240000     |
-------------------------------------------

 ---------------- Iteration 357 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 357           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.006652458  |
| LossBefore              | 1.8023822e-09 |
| MeanKL                  | 0.006803182   |
| MeanKLBefore            | 1.0429574e-08 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.4113651     |
| Step_0-AverageReturn    | 258           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 608           |
| Step_0-MinReturn        | 12.8          |
| Step_0-NumTrajs         | 1182          |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 120           |
| Step_1-AverageDiscou... | 120           |
| Step_1-AveragePolicyStd | 0.41127717    |
| Step_1-AverageReturn    | 251           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 675           |
| Step_1-MinReturn        | 15.1          |
| Step_1-NumTrajs         | 1215          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 114           |
| Time                    | 2.43e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.928         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 58            |
| dLoss                   | 0.00665246    |
| n_timesteps             | 114560000     |
-------------------------------------------

 ---------------- Iteration 358 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 358            |
| ItrTime                 | 71             |
| LossAfter               | -0.0066969665  |
| LossBefore              | -3.1105654e-09 |
| MeanKL                  | 0.0069862967   |
| MeanKLBefore            | -8.93805e-09   |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.4113574      |
| Step_0-AverageReturn    | 273            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 559            |
| Step_0-MinReturn        | 19.1           |
| Step_0-NumTrajs         | 1105           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 112            |
| Step_1-AverageDiscou... | 126            |
| Step_1-AveragePolicyStd | 0.41135633     |
| Step_1-AverageReturn    | 275            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 667            |
| Step_1-MinReturn        | 20.5           |
| Step_1-NumTrajs         | 1154           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 118            |
| Time                    | 2.43e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.912          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.006696963    |
| n_timesteps             | 114880000      |
--------------------------------------------

 ---------------- Iteration 359 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 359            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.006961528   |
| LossBefore              | -3.0853773e-09 |
| MeanKL                  | 0.007769659    |
| MeanKLBefore            | -1.3409759e-08 |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.41123408     |
| Step_0-AverageReturn    | 265            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 603            |
| Step_0-MinReturn        | 7.01           |
| Step_0-NumTrajs         | 1172           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 110            |
| Step_1-AverageDiscou... | 127            |
| Step_1-AveragePolicyStd | 0.41120413     |
| Step_1-AverageReturn    | 273            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 632            |
| Step_1-MinReturn        | 16.9           |
| Step_1-NumTrajs         | 1175           |
| Step_1-PolicyExecTime   | 4.48           |
| Step_1-StdReturn        | 111            |
| Time                    | 2.44e+04       |
| Time-InnerStep          | 0.171          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.91           |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.006961525    |
| n_timesteps             | 115200000      |
--------------------------------------------

 ---------------- Iteration 360 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 360            |
| ItrTime                 | 72             |
| LossAfter               | -0.00682922    |
| LossBefore              | -4.3759166e-10 |
| MeanKL                  | 0.0070217727   |
| MeanKLBefore            | 1.0427769e-08  |
| Step_0-AverageDiscou... | 131            |
| Step_0-AveragePolicyStd | 0.41129467     |
| Step_0-AverageReturn    | 298            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 564            |
| Step_0-MinReturn        | 15.4           |
| Step_0-NumTrajs         | 1016           |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 111            |
| Step_1-AverageDiscou... | 135            |
| Step_1-AveragePolicyStd | 0.41121605     |
| Step_1-AverageReturn    | 301            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 622            |
| Step_1-MinReturn        | 23.7           |
| Step_1-NumTrajs         | 1090           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 120            |
| Time                    | 2.45e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.873          |
| Time-Sampling           | 57.2           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.0068292194   |
| n_timesteps             | 115520000      |
--------------------------------------------

 ---------------- Iteration 361 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 361            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0070493133  |
| LossBefore              | 2.614154e-09   |
| MeanKL                  | 0.007887324    |
| MeanKLBefore            | -1.4889014e-09 |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.4112047      |
| Step_0-AverageReturn    | 265            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 635            |
| Step_0-MinReturn        | 7.09           |
| Step_0-NumTrajs         | 1138           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 128            |
| Step_1-AveragePolicyStd | 0.41121197     |
| Step_1-AverageReturn    | 277            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 603            |
| Step_1-MinReturn        | 16.7           |
| Step_1-NumTrajs         | 1149           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 115            |
| Time                    | 2.46e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.895          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007049316    |
| n_timesteps             | 115840000      |
--------------------------------------------

 ---------------- Iteration 362 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 362            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.006909719   |
| LossBefore              | -1.8388256e-09 |
| MeanKL                  | 0.007568753    |
| MeanKLBefore            | 2.9791498e-09  |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.41050437     |
| Step_0-AverageReturn    | 274            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 661            |
| Step_0-MinReturn        | 5.72           |
| Step_0-NumTrajs         | 1136           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 129            |
| Step_1-AverageDiscou... | 129            |
| Step_1-AveragePolicyStd | 0.41039744     |
| Step_1-AverageReturn    | 279            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 621            |
| Step_1-MinReturn        | 6.97           |
| Step_1-NumTrajs         | 1164           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 125            |
| Time                    | 2.46e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.888          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56.1           |
| dLoss                   | 0.0069097173   |
| n_timesteps             | 116160000      |
--------------------------------------------

 ---------------- Iteration 363 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 363           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.007062321  |
| LossBefore              | 2.121112e-09  |
| MeanKL                  | 0.0075049764  |
| MeanKLBefore            | -8.940779e-09 |
| Step_0-AverageDiscou... | 128           |
| Step_0-AveragePolicyStd | 0.40988222    |
| Step_0-AverageReturn    | 286           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 619           |
| Step_0-MinReturn        | 9.56          |
| Step_0-NumTrajs         | 1055          |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 129           |
| Step_1-AveragePolicyStd | 0.4097933     |
| Step_1-AverageReturn    | 280           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 614           |
| Step_1-MinReturn        | 10.6          |
| Step_1-NumTrajs         | 1151          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 117           |
| Time                    | 2.47e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.883         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0070623234  |
| n_timesteps             | 116480000     |
-------------------------------------------

 ---------------- Iteration 364 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 364           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.0059926324 |
| LossBefore              | 5.323663e-10  |
| MeanKL                  | 0.006434972   |
| MeanKLBefore            | -1.192093e-08 |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.40942624    |
| Step_0-AverageReturn    | 258           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 556           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1199          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 126           |
| Step_1-AveragePolicyStd | 0.4094058     |
| Step_1-AverageReturn    | 265           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 613           |
| Step_1-MinReturn        | 9.86          |
| Step_1-NumTrajs         | 1235          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 116           |
| Time                    | 2.48e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 14            |
| Time-OuterStep          | 14            |
| Time-SampleProc         | 0.902         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.005992633   |
| n_timesteps             | 116800000     |
-------------------------------------------

 ---------------- Iteration 365 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 365           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0069626705 |
| LossBefore              | 3.242062e-09  |
| MeanKL                  | 0.0077347755  |
| MeanKLBefore            | 8.939288e-09  |
| Step_0-AverageDiscou... | 134           |
| Step_0-AveragePolicyStd | 0.40958834    |
| Step_0-AverageReturn    | 290           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 613           |
| Step_0-MinReturn        | 17.6          |
| Step_0-NumTrajs         | 1147          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 95.2          |
| Step_1-AverageDiscou... | 131           |
| Step_1-AveragePolicyStd | 0.40959156    |
| Step_1-AverageReturn    | 281           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 739           |
| Step_1-MinReturn        | 19.9          |
| Step_1-NumTrajs         | 1180          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 104           |
| Time                    | 2.48e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.937         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.0069626737  |
| n_timesteps             | 117120000     |
-------------------------------------------

 ---------------- Iteration 366 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 366           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.0070177778 |
| LossBefore              | -2.04233e-08  |
| MeanKL                  | 0.0072756596  |
| MeanKLBefore            | -8.938895e-09 |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.40935683    |
| Step_0-AverageReturn    | 263           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 570           |
| Step_0-MinReturn        | 18.7          |
| Step_0-NumTrajs         | 1148          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 125           |
| Step_1-AveragePolicyStd | 0.4092865     |
| Step_1-AverageReturn    | 267           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 655           |
| Step_1-MinReturn        | 3.39          |
| Step_1-NumTrajs         | 1191          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 120           |
| Time                    | 2.49e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.904         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0070177573  |
| n_timesteps             | 117440000     |
-------------------------------------------

 ---------------- Iteration 367 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 367            |
| ItrTime                 | 70             |
| LossAfter               | -0.007441149   |
| LossBefore              | 6.2842163e-09  |
| MeanKL                  | 0.007763043    |
| MeanKLBefore            | -2.9777063e-09 |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.4099962      |
| Step_0-AverageReturn    | 269            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 602            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 1143           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 109            |
| Step_1-AverageDiscou... | 127            |
| Step_1-AveragePolicyStd | 0.40990233     |
| Step_1-AverageReturn    | 265            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 586            |
| Step_1-MinReturn        | 9.4            |
| Step_1-NumTrajs         | 1208           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 110            |
| Time                    | 2.5e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.93           |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.007441155    |
| n_timesteps             | 117760000      |
--------------------------------------------

 ---------------- Iteration 368 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 368            |
| ItrTime                 | 72.5           |
| LossAfter               | -0.0073292307  |
| LossBefore              | 6.1514505e-09  |
| MeanKL                  | 0.008373333    |
| MeanKLBefore            | -1.1816326e-12 |
| Step_0-AverageDiscou... | 124            |
| Step_0-AveragePolicyStd | 0.40906695     |
| Step_0-AverageReturn    | 285            |
| Step_0-EnvExecTime      | 24.6           |
| Step_0-MaxReturn        | 600            |
| Step_0-MinReturn        | 16.3           |
| Step_0-NumTrajs         | 1031           |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 138            |
| Step_1-AverageDiscou... | 127            |
| Step_1-AveragePolicyStd | 0.4090119      |
| Step_1-AverageReturn    | 280            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 718            |
| Step_1-MinReturn        | 14.6           |
| Step_1-NumTrajs         | 1136           |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 133            |
| Time                    | 2.51e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.89           |
| Time-Sampling           | 57.6           |
| Time-TotalInner         | 58.7           |
| dLoss                   | 0.0073292367   |
| n_timesteps             | 118080000      |
--------------------------------------------

 ---------------- Iteration 369 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 369            |
| ItrTime                 | 72             |
| LossAfter               | -0.0071382634  |
| LossBefore              | -3.2527905e-09 |
| MeanKL                  | 0.00754111     |
| MeanKLBefore            | 4.469235e-09   |
| Step_0-AverageDiscou... | 133            |
| Step_0-AveragePolicyStd | 0.40853333     |
| Step_0-AverageReturn    | 302            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 675            |
| Step_0-MinReturn        | 9.49           |
| Step_0-NumTrajs         | 1046           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 120            |
| Step_1-AverageDiscou... | 134            |
| Step_1-AveragePolicyStd | 0.40856627     |
| Step_1-AverageReturn    | 304            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 648            |
| Step_1-MinReturn        | 23.2           |
| Step_1-NumTrajs         | 1079           |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 120            |
| Time                    | 2.51e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.872          |
| Time-Sampling           | 57.2           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.00713826     |
| n_timesteps             | 118400000      |
--------------------------------------------

 ---------------- Iteration 370 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 370            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0068504484  |
| LossBefore              | -9.3875085e-09 |
| MeanKL                  | 0.008105503    |
| MeanKLBefore            | -8.939944e-09  |
| Step_0-AverageDiscou... | 131            |
| Step_0-AveragePolicyStd | 0.40983954     |
| Step_0-AverageReturn    | 281            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 589            |
| Step_0-MinReturn        | 15.5           |
| Step_0-NumTrajs         | 1146           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 114            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.40984192     |
| Step_1-AverageReturn    | 277            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 583            |
| Step_1-MinReturn        | 17.3           |
| Step_1-NumTrajs         | 1210           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 111            |
| Time                    | 2.52e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.927          |
| Time-Sampling           | 54.3           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.006850439    |
| n_timesteps             | 118720000      |
--------------------------------------------

 ---------------- Iteration 371 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 371           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.006611116  |
| LossBefore              | 6.74448e-09   |
| MeanKL                  | 0.0074319644  |
| MeanKLBefore            | 1.4902447e-09 |
| Step_0-AverageDiscou... | 128           |
| Step_0-AveragePolicyStd | 0.40972042    |
| Step_0-AverageReturn    | 264           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 601           |
| Step_0-MinReturn        | 18.2          |
| Step_0-NumTrajs         | 1254          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 112           |
| Step_1-AverageDiscou... | 130           |
| Step_1-AveragePolicyStd | 0.40966904    |
| Step_1-AverageReturn    | 271           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 611           |
| Step_1-MinReturn        | 12.6          |
| Step_1-NumTrajs         | 1230          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 114           |
| Time                    | 2.53e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.943         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0066111227  |
| n_timesteps             | 119040000     |
-------------------------------------------

 ---------------- Iteration 372 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 372           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.0065289573 |
| LossBefore              | 1.2087684e-08 |
| MeanKL                  | 0.0070454725  |
| MeanKLBefore            | 1.9368871e-08 |
| Step_0-AverageDiscou... | 120           |
| Step_0-AveragePolicyStd | 0.40869787    |
| Step_0-AverageReturn    | 257           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 600           |
| Step_0-MinReturn        | 4.18          |
| Step_0-NumTrajs         | 1135          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 121           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.40874448    |
| Step_1-AverageReturn    | 266           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 598           |
| Step_1-MinReturn        | 21.6          |
| Step_1-NumTrajs         | 1169          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 122           |
| Time                    | 2.53e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.902         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.0065289694  |
| n_timesteps             | 119360000     |
-------------------------------------------

 ---------------- Iteration 373 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 373           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.007055171  |
| LossBefore              | 9.699291e-09  |
| MeanKL                  | 0.007849696   |
| MeanKLBefore            | 1.7880318e-08 |
| Step_0-AverageDiscou... | 123           |
| Step_0-AveragePolicyStd | 0.40861082    |
| Step_0-AverageReturn    | 260           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 596           |
| Step_0-MinReturn        | 10.6          |
| Step_0-NumTrajs         | 1213          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 124           |
| Step_1-AveragePolicyStd | 0.40847784    |
| Step_1-AverageReturn    | 262           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 598           |
| Step_1-MinReturn        | 10.6          |
| Step_1-NumTrajs         | 1256          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 117           |
| Time                    | 2.54e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.935         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.0070551806  |
| n_timesteps             | 119680000     |
-------------------------------------------

 ---------------- Iteration 374 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 374            |
| ItrTime                 | 73             |
| LossAfter               | -0.007032217   |
| LossBefore              | -1.6808247e-09 |
| MeanKL                  | 0.007936196    |
| MeanKLBefore            | 1.1917955e-08  |
| Step_0-AverageDiscou... | 126            |
| Step_0-AveragePolicyStd | 0.40858424     |
| Step_0-AverageReturn    | 290            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 600            |
| Step_0-MinReturn        | 21.3           |
| Step_0-NumTrajs         | 998            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 121            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.40852305     |
| Step_1-AverageReturn    | 294            |
| Step_1-EnvExecTime      | 24.8           |
| Step_1-MaxReturn        | 655            |
| Step_1-MinReturn        | 29.6           |
| Step_1-NumTrajs         | 1082           |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 129            |
| Time                    | 2.55e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.852          |
| Time-Sampling           | 58.3           |
| Time-TotalInner         | 59.3           |
| dLoss                   | 0.007032215    |
| n_timesteps             | 120000000      |
--------------------------------------------

 ---------------- Iteration 375 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 375           |
| ItrTime                 | 70            |
| LossAfter               | -0.006691888  |
| LossBefore              | 1.7614838e-08 |
| MeanKL                  | 0.008064585   |
| MeanKLBefore            | -7.451427e-09 |
| Step_0-AverageDiscou... | 130           |
| Step_0-AveragePolicyStd | 0.40817764    |
| Step_0-AverageReturn    | 279           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 622           |
| Step_0-MinReturn        | 12.7          |
| Step_0-NumTrajs         | 1165          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 110           |
| Step_1-AverageDiscou... | 129           |
| Step_1-AveragePolicyStd | 0.40804207    |
| Step_1-AverageReturn    | 276           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 643           |
| Step_1-MinReturn        | 2.53          |
| Step_1-NumTrajs         | 1207          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 116           |
| Time                    | 2.55e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.956         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0066919057  |
| n_timesteps             | 120320000     |
-------------------------------------------

 ---------------- Iteration 376 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 376           |
| ItrTime                 | 72.3          |
| LossAfter               | -0.0071667596 |
| LossBefore              | 1.8799589e-09 |
| MeanKL                  | 0.0069742696  |
| MeanKLBefore            | -4.468783e-09 |
| Step_0-AverageDiscou... | 124           |
| Step_0-AveragePolicyStd | 0.40749785    |
| Step_0-AverageReturn    | 281           |
| Step_0-EnvExecTime      | 24.5          |
| Step_0-MaxReturn        | 601           |
| Step_0-MinReturn        | 17.3          |
| Step_0-NumTrajs         | 1048          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 117           |
| Step_1-AverageDiscou... | 129           |
| Step_1-AveragePolicyStd | 0.4075093     |
| Step_1-AverageReturn    | 284           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 614           |
| Step_1-MinReturn        | 15.5          |
| Step_1-NumTrajs         | 1141          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 120           |
| Time                    | 2.56e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.893         |
| Time-Sampling           | 57.5          |
| Time-TotalInner         | 58.6          |
| dLoss                   | 0.0071667614  |
| n_timesteps             | 120640000     |
-------------------------------------------

 ---------------- Iteration 377 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 377          |
| ItrTime                 | 71.3         |
| LossAfter               | -0.00691624  |
| LossBefore              | 3.949364e-09 |
| MeanKL                  | 0.007686411  |
| MeanKLBefore            | 4.467236e-09 |
| Step_0-AverageDiscou... | 126          |
| Step_0-AveragePolicyStd | 0.40768945   |
| Step_0-AverageReturn    | 270          |
| Step_0-EnvExecTime      | 24           |
| Step_0-MaxReturn        | 625          |
| Step_0-MinReturn        | 13.2         |
| Step_0-NumTrajs         | 1138         |
| Step_0-PolicyExecTime   | 1.35         |
| Step_0-StdReturn        | 118          |
| Step_1-AverageDiscou... | 128          |
| Step_1-AveragePolicyStd | 0.407733     |
| Step_1-AverageReturn    | 274          |
| Step_1-EnvExecTime      | 23.7         |
| Step_1-MaxReturn        | 625          |
| Step_1-MinReturn        | 14.7         |
| Step_1-NumTrajs         | 1185         |
| Step_1-PolicyExecTime   | 4.07         |
| Step_1-StdReturn        | 120          |
| Time                    | 2.57e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.7         |
| Time-OuterStep          | 13.7         |
| Time-SampleProc         | 0.927        |
| Time-Sampling           | 56.5         |
| Time-TotalInner         | 57.6         |
| dLoss                   | 0.0069162436 |
| n_timesteps             | 120960000    |
------------------------------------------

 ---------------- Iteration 378 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 378           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.006913299  |
| LossBefore              | 6.0447347e-10 |
| MeanKL                  | 0.0074372604  |
| MeanKLBefore            | 2.9805691e-09 |
| Step_0-AverageDiscou... | 136           |
| Step_0-AveragePolicyStd | 0.4078281     |
| Step_0-AverageReturn    | 310           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 675           |
| Step_0-MinReturn        | 17.3          |
| Step_0-NumTrajs         | 1025          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 109           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.40772986    |
| Step_1-AverageReturn    | 303           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 683           |
| Step_1-MinReturn        | 21.4          |
| Step_1-NumTrajs         | 1132          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 112           |
| Time                    | 2.58e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.886         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0069132997  |
| n_timesteps             | 121280000     |
-------------------------------------------

 ---------------- Iteration 379 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 379            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.006682222   |
| LossBefore              | -3.2441766e-10 |
| MeanKL                  | 0.00789922     |
| MeanKLBefore            | -1.3408732e-08 |
| Step_0-AverageDiscou... | 132            |
| Step_0-AveragePolicyStd | 0.4074897      |
| Step_0-AverageReturn    | 302            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 584            |
| Step_0-MinReturn        | 17.4           |
| Step_0-NumTrajs         | 1015           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 118            |
| Step_1-AverageDiscou... | 135            |
| Step_1-AveragePolicyStd | 0.40755665     |
| Step_1-AverageReturn    | 295            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 620            |
| Step_1-MinReturn        | 19.2           |
| Step_1-NumTrajs         | 1114           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 113            |
| Time                    | 2.58e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.885          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0066822213   |
| n_timesteps             | 121600000      |
--------------------------------------------

 ---------------- Iteration 380 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 380           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0071342103 |
| LossBefore              | -6.238686e-10 |
| MeanKL                  | 0.008084408   |
| MeanKLBefore            | 1.4900591e-09 |
| Step_0-AverageDiscou... | 127           |
| Step_0-AveragePolicyStd | 0.4065568     |
| Step_0-AverageReturn    | 276           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 607           |
| Step_0-MinReturn        | 21.5          |
| Step_0-NumTrajs         | 1101          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 109           |
| Step_1-AverageDiscou... | 130           |
| Step_1-AveragePolicyStd | 0.4065858     |
| Step_1-AverageReturn    | 277           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 617           |
| Step_1-MinReturn        | 21.9          |
| Step_1-NumTrajs         | 1202          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 115           |
| Time                    | 2.59e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.919         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.00713421    |
| n_timesteps             | 121920000     |
-------------------------------------------

 ---------------- Iteration 381 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 381             |
| ItrTime                 | 69.8            |
| LossAfter               | -0.0069396333   |
| LossBefore              | 1.1788721e-10   |
| MeanKL                  | 0.0077432394    |
| MeanKLBefore            | -1.19206645e-08 |
| Step_0-AverageDiscou... | 132             |
| Step_0-AveragePolicyStd | 0.40664804      |
| Step_0-AverageReturn    | 287             |
| Step_0-EnvExecTime      | 23.2            |
| Step_0-MaxReturn        | 634             |
| Step_0-MinReturn        | 6.75            |
| Step_0-NumTrajs         | 1122            |
| Step_0-PolicyExecTime   | 1.36            |
| Step_0-StdReturn        | 111             |
| Step_1-AverageDiscou... | 132             |
| Step_1-AveragePolicyStd | 0.40653694      |
| Step_1-AverageReturn    | 283             |
| Step_1-EnvExecTime      | 23.1            |
| Step_1-MaxReturn        | 655             |
| Step_1-MinReturn        | 19.8            |
| Step_1-NumTrajs         | 1188            |
| Step_1-PolicyExecTime   | 4.08            |
| Step_1-StdReturn        | 114             |
| Time                    | 2.6e+04         |
| Time-InnerStep          | 0.164           |
| Time-MAMLSteps          | 13.7            |
| Time-OuterStep          | 13.7            |
| Time-SampleProc         | 0.935           |
| Time-Sampling           | 55              |
| Time-TotalInner         | 56.1            |
| dLoss                   | 0.0069396333    |
| n_timesteps             | 122240000       |
---------------------------------------------

 ---------------- Iteration 382 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 382           |
| ItrTime                 | 71.7          |
| LossAfter               | -0.0066175265 |
| LossBefore              | -3.967616e-09 |
| MeanKL                  | 0.0076036765  |
| MeanKLBefore            | 4.470357e-09  |
| Step_0-AverageDiscou... | 125           |
| Step_0-AveragePolicyStd | 0.40659797    |
| Step_0-AverageReturn    | 278           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 649           |
| Step_0-MinReturn        | 7.38          |
| Step_0-NumTrajs         | 1092          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 128           |
| Step_1-AverageDiscou... | 130           |
| Step_1-AveragePolicyStd | 0.4065681     |
| Step_1-AverageReturn    | 288           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 657           |
| Step_1-MinReturn        | 9.48          |
| Step_1-NumTrajs         | 1128          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 124           |
| Time                    | 2.6e+04       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.87          |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.0066175223  |
| n_timesteps             | 122560000     |
-------------------------------------------

 ---------------- Iteration 383 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 383            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0064612767  |
| LossBefore              | -1.5528716e-08 |
| MeanKL                  | 0.0069112084   |
| MeanKLBefore            | 2.9792897e-09  |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.4062902      |
| Step_0-AverageReturn    | 294            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 644            |
| Step_0-MinReturn        | 19.7           |
| Step_0-NumTrajs         | 1026           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 132            |
| Step_1-AveragePolicyStd | 0.40628463     |
| Step_1-AverageReturn    | 293            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 610            |
| Step_1-MinReturn        | 9.97           |
| Step_1-NumTrajs         | 1107           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 119            |
| Time                    | 2.61e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.863          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0064612613   |
| n_timesteps             | 122880000      |
--------------------------------------------

 ---------------- Iteration 384 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 384            |
| ItrTime                 | 70             |
| LossAfter               | -0.006986332   |
| LossBefore              | -1.5685389e-09 |
| MeanKL                  | 0.008356305    |
| MeanKLBefore            | -1.4900557e-09 |
| Step_0-AverageDiscou... | 130            |
| Step_0-AveragePolicyStd | 0.40730175     |
| Step_0-AverageReturn    | 291            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 617            |
| Step_0-MinReturn        | 18.3           |
| Step_0-NumTrajs         | 1063           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 126            |
| Step_1-AverageDiscou... | 134            |
| Step_1-AveragePolicyStd | 0.40735626     |
| Step_1-AverageReturn    | 292            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 657            |
| Step_1-MinReturn        | 14.8           |
| Step_1-NumTrajs         | 1172           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 126            |
| Time                    | 2.62e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.867          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0069863307   |
| n_timesteps             | 123200000      |
--------------------------------------------

 ---------------- Iteration 385 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 385            |
| ItrTime                 | 69.3           |
| LossAfter               | -0.00624516    |
| LossBefore              | -1.1513423e-08 |
| MeanKL                  | 0.0076223304   |
| MeanKLBefore            | 1.7879032e-08  |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.40668654     |
| Step_0-AverageReturn    | 279            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 555            |
| Step_0-MinReturn        | 30.7           |
| Step_0-NumTrajs         | 1114           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 101            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.40665257     |
| Step_1-AverageReturn    | 283            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 655            |
| Step_1-MinReturn        | 38.1           |
| Step_1-NumTrajs         | 1147           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 109            |
| Time                    | 2.63e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.899          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.0062451484   |
| n_timesteps             | 123520000      |
--------------------------------------------

 ---------------- Iteration 386 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 386            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.0065892925  |
| LossBefore              | -1.5376072e-09 |
| MeanKL                  | 0.0071063703   |
| MeanKLBefore            | 5.9593495e-09  |
| Step_0-AverageDiscou... | 127            |
| Step_0-AveragePolicyStd | 0.40644875     |
| Step_0-AverageReturn    | 274            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 632            |
| Step_0-MinReturn        | 22.4           |
| Step_0-NumTrajs         | 1146           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 107            |
| Step_1-AverageDiscou... | 132            |
| Step_1-AveragePolicyStd | 0.40630725     |
| Step_1-AverageReturn    | 282            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 615            |
| Step_1-MinReturn        | 23.2           |
| Step_1-NumTrajs         | 1191           |
| Step_1-PolicyExecTime   | 4.28           |
| Step_1-StdReturn        | 105            |
| Time                    | 2.63e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.882          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.006589291    |
| n_timesteps             | 123840000      |
--------------------------------------------

 ---------------- Iteration 387 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 387           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.006908341  |
| LossBefore              | 5.0554996e-09 |
| MeanKL                  | 0.007808988   |
| MeanKLBefore            | 5.958415e-09  |
| Step_0-AverageDiscou... | 135           |
| Step_0-AveragePolicyStd | 0.40608644    |
| Step_0-AverageReturn    | 317           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 613           |
| Step_0-MinReturn        | 15.7          |
| Step_0-NumTrajs         | 975           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 113           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.40596235    |
| Step_1-AverageReturn    | 311           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 621           |
| Step_1-MinReturn        | 27            |
| Step_1-NumTrajs         | 1072          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 120           |
| Time                    | 2.64e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.818         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.006908346   |
| n_timesteps             | 124160000     |
-------------------------------------------

 ---------------- Iteration 388 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 388           |
| ItrTime                 | 71            |
| LossAfter               | -0.00696692   |
| LossBefore              | 4.0102104e-09 |
| MeanKL                  | 0.007378309   |
| MeanKLBefore            | 1.0432136e-08 |
| Step_0-AverageDiscou... | 127           |
| Step_0-AveragePolicyStd | 0.40652227    |
| Step_0-AverageReturn    | 287           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 593           |
| Step_0-MinReturn        | 14.1          |
| Step_0-NumTrajs         | 1065          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 123           |
| Step_1-AverageDiscou... | 131           |
| Step_1-AveragePolicyStd | 0.4064272     |
| Step_1-AverageReturn    | 288           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 746           |
| Step_1-MinReturn        | 17.1          |
| Step_1-NumTrajs         | 1152          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 123           |
| Time                    | 2.65e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.872         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0069669243  |
| n_timesteps             | 124480000     |
-------------------------------------------

 ---------------- Iteration 389 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 389           |
| ItrTime                 | 72.2          |
| LossAfter               | -0.0066041267 |
| LossBefore              | -2.816925e-09 |
| MeanKL                  | 0.007008812   |
| MeanKLBefore            | 4.4710964e-09 |
| Step_0-AverageDiscou... | 129           |
| Step_0-AveragePolicyStd | 0.40655407    |
| Step_0-AverageReturn    | 286           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 638           |
| Step_0-MinReturn        | 8.81          |
| Step_0-NumTrajs         | 1061          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 118           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.4065354     |
| Step_1-AverageReturn    | 294           |
| Step_1-EnvExecTime      | 24.4          |
| Step_1-MaxReturn        | 652           |
| Step_1-MinReturn        | 23.4          |
| Step_1-NumTrajs         | 1107          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 119           |
| Time                    | 2.65e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.868         |
| Time-Sampling           | 57.4          |
| Time-TotalInner         | 58.5          |
| dLoss                   | 0.006604124   |
| n_timesteps             | 124800000     |
-------------------------------------------

 ---------------- Iteration 390 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 390           |
| ItrTime                 | 71.9          |
| LossAfter               | -0.006963171  |
| LossBefore              | 4.2500536e-09 |
| MeanKL                  | 0.008749107   |
| MeanKLBefore            | 5.9604055e-09 |
| Step_0-AverageDiscou... | 141           |
| Step_0-AveragePolicyStd | 0.40525523    |
| Step_0-AverageReturn    | 332           |
| Step_0-EnvExecTime      | 24.7          |
| Step_0-MaxReturn        | 596           |
| Step_0-MinReturn        | 26.1          |
| Step_0-NumTrajs         | 955           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 108           |
| Step_1-AverageDiscou... | 142           |
| Step_1-AveragePolicyStd | 0.40528086    |
| Step_1-AverageReturn    | 324           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 645           |
| Step_1-MinReturn        | 27            |
| Step_1-NumTrajs         | 1070          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 118           |
| Time                    | 2.66e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.832         |
| Time-Sampling           | 57.3          |
| Time-TotalInner         | 58.3          |
| dLoss                   | 0.0069631753  |
| n_timesteps             | 125120000     |
-------------------------------------------

 ---------------- Iteration 391 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 391            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0071131303  |
| LossBefore              | -2.1226427e-09 |
| MeanKL                  | 0.007899137    |
| MeanKLBefore            | 1.0430817e-08  |
| Step_0-AverageDiscou... | 136            |
| Step_0-AveragePolicyStd | 0.40517175     |
| Step_0-AverageReturn    | 315            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 596            |
| Step_0-MinReturn        | 14.5           |
| Step_0-NumTrajs         | 1002           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 106            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.40520895     |
| Step_1-AverageReturn    | 303            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 618            |
| Step_1-MinReturn        | 23.3           |
| Step_1-NumTrajs         | 1112           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 115            |
| Time                    | 2.67e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.884          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007113128    |
| n_timesteps             | 125440000      |
--------------------------------------------

 ---------------- Iteration 392 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 392          |
| ItrTime                 | 71.3         |
| LossAfter               | -0.006883091 |
| LossBefore              | 4.270496e-09 |
| MeanKL                  | 0.008580456  |
| MeanKLBefore            | 1.489151e-09 |
| Step_0-AverageDiscou... | 126          |
| Step_0-AveragePolicyStd | 0.4053503    |
| Step_0-AverageReturn    | 283          |
| Step_0-EnvExecTime      | 23.7         |
| Step_0-MaxReturn        | 640          |
| Step_0-MinReturn        | 19.2         |
| Step_0-NumTrajs         | 1076         |
| Step_0-PolicyExecTime   | 1.3          |
| Step_0-StdReturn        | 126          |
| Step_1-AverageDiscou... | 129          |
| Step_1-AveragePolicyStd | 0.4053002    |
| Step_1-AverageReturn    | 282          |
| Step_1-EnvExecTime      | 24           |
| Step_1-MaxReturn        | 626          |
| Step_1-MinReturn        | 15.3         |
| Step_1-NumTrajs         | 1150         |
| Step_1-PolicyExecTime   | 4.16         |
| Step_1-StdReturn        | 122          |
| Time                    | 2.68e+04     |
| Time-InnerStep          | 0.164        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.878        |
| Time-Sampling           | 56.5         |
| Time-TotalInner         | 57.6         |
| dLoss                   | 0.006883095  |
| n_timesteps             | 125760000    |
------------------------------------------

 ---------------- Iteration 393 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 393            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.0067839785  |
| LossBefore              | -3.5875836e-10 |
| MeanKL                  | 0.008191292    |
| MeanKLBefore            | 1.0431336e-08  |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.40547684     |
| Step_0-AverageReturn    | 276            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 626            |
| Step_0-MinReturn        | 15.2           |
| Step_0-NumTrajs         | 1164           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 108            |
| Step_1-AverageDiscou... | 132            |
| Step_1-AveragePolicyStd | 0.40530294     |
| Step_1-AverageReturn    | 284            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 682            |
| Step_1-MinReturn        | 19.8           |
| Step_1-NumTrajs         | 1170           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 116            |
| Time                    | 2.68e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.893          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 56             |
| dLoss                   | 0.006783978    |
| n_timesteps             | 126080000      |
--------------------------------------------

 ---------------- Iteration 394 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 394            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0068730777  |
| LossBefore              | 1.1792702e-09  |
| MeanKL                  | 0.008669702    |
| MeanKLBefore            | -1.4907406e-09 |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.4047937      |
| Step_0-AverageReturn    | 282            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 622            |
| Step_0-MinReturn        | 17.7           |
| Step_0-NumTrajs         | 1126           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 123            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.404781       |
| Step_1-AverageReturn    | 283            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 609            |
| Step_1-MinReturn        | 14.7           |
| Step_1-NumTrajs         | 1169           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 122            |
| Time                    | 2.69e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.888          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.006873079    |
| n_timesteps             | 126400000      |
--------------------------------------------

 ---------------- Iteration 395 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 395           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.006651564  |
| LossBefore              | -3.45655e-09  |
| MeanKL                  | 0.0072923354  |
| MeanKLBefore            | 1.3410384e-08 |
| Step_0-AverageDiscou... | 125           |
| Step_0-AveragePolicyStd | 0.40456146    |
| Step_0-AverageReturn    | 278           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 617           |
| Step_0-MinReturn        | 8.15          |
| Step_0-NumTrajs         | 1091          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 122           |
| Step_1-AverageDiscou... | 132           |
| Step_1-AveragePolicyStd | 0.4045313     |
| Step_1-AverageReturn    | 297           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 650           |
| Step_1-MinReturn        | 13.4          |
| Step_1-NumTrajs         | 1086          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 122           |
| Time                    | 2.7e+04       |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.006651561   |
| n_timesteps             | 126720000     |
-------------------------------------------

 ---------------- Iteration 396 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 396            |
| ItrTime                 | 72.2           |
| LossAfter               | -0.0064788833  |
| LossBefore              | -1.0729023e-08 |
| MeanKL                  | 0.0075307377   |
| MeanKLBefore            | -1.3408571e-08 |
| Step_0-AverageDiscou... | 138            |
| Step_0-AveragePolicyStd | 0.40467924     |
| Step_0-AverageReturn    | 319            |
| Step_0-EnvExecTime      | 24.7           |
| Step_0-MaxReturn        | 619            |
| Step_0-MinReturn        | 31.9           |
| Step_0-NumTrajs         | 1035           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 122            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.40457138     |
| Step_1-AverageReturn    | 301            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 641            |
| Step_1-MinReturn        | 31.5           |
| Step_1-NumTrajs         | 1155           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 115            |
| Time                    | 2.7e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.874          |
| Time-Sampling           | 57.4           |
| Time-TotalInner         | 58.5           |
| dLoss                   | 0.0064788726   |
| n_timesteps             | 127040000      |
--------------------------------------------

 ---------------- Iteration 397 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 397            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.0069253976  |
| LossBefore              | -5.7135243e-09 |
| MeanKL                  | 0.008839341    |
| MeanKLBefore            | 8.938052e-09   |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.40391585     |
| Step_0-AverageReturn    | 278            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 619            |
| Step_0-MinReturn        | 11.8           |
| Step_0-NumTrajs         | 1178           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 127            |
| Step_1-AverageDiscou... | 134            |
| Step_1-AveragePolicyStd | 0.40391904     |
| Step_1-AverageReturn    | 292            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 668            |
| Step_1-MinReturn        | 12.3           |
| Step_1-NumTrajs         | 1166           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 121            |
| Time                    | 2.71e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.877          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.006925392    |
| n_timesteps             | 127360000      |
--------------------------------------------

 ---------------- Iteration 398 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 398            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.006809561   |
| LossBefore              | -7.3592354e-09 |
| MeanKL                  | 0.008266494    |
| MeanKLBefore            | 1.6388638e-08  |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.40391842     |
| Step_0-AverageReturn    | 281            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 576            |
| Step_0-MinReturn        | 14.5           |
| Step_0-NumTrajs         | 1127           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 111            |
| Step_1-AverageDiscou... | 129            |
| Step_1-AveragePolicyStd | 0.4038807      |
| Step_1-AverageReturn    | 276            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 667            |
| Step_1-MinReturn        | 13.7           |
| Step_1-NumTrajs         | 1188           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 120            |
| Time                    | 2.72e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.901          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0068095536   |
| n_timesteps             | 127680000      |
--------------------------------------------

 ---------------- Iteration 399 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 399            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.0073233857  |
| LossBefore              | -7.397336e-10  |
| MeanKL                  | 0.008919271    |
| MeanKLBefore            | -2.9772316e-09 |
| Step_0-AverageDiscou... | 133            |
| Step_0-AveragePolicyStd | 0.40354115     |
| Step_0-AverageReturn    | 304            |
| Step_0-EnvExecTime      | 24.6           |
| Step_0-MaxReturn        | 646            |
| Step_0-MinReturn        | 7.21           |
| Step_0-NumTrajs         | 1030           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 120            |
| Step_1-AverageDiscou... | 130            |
| Step_1-AveragePolicyStd | 0.4034732      |
| Step_1-AverageReturn    | 280            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 674            |
| Step_1-MinReturn        | 26.4           |
| Step_1-NumTrajs         | 1185           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 123            |
| Time                    | 2.72e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.848          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58.1           |
| dLoss                   | 0.0073233848   |
| n_timesteps             | 128000000      |
--------------------------------------------

 ---------------- Iteration 400 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 400           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.007167504  |
| LossBefore              | -8.072128e-09 |
| MeanKL                  | 0.0075555868  |
| MeanKLBefore            | 1.1919951e-08 |
| Step_0-AverageDiscou... | 132           |
| Step_0-AveragePolicyStd | 0.40345418    |
| Step_0-AverageReturn    | 292           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 643           |
| Step_0-MinReturn        | 2.84          |
| Step_0-NumTrajs         | 1062          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 136           |
| Step_1-AveragePolicyStd | 0.4033997     |
| Step_1-AverageReturn    | 297           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 657           |
| Step_1-MinReturn        | 24            |
| Step_1-NumTrajs         | 1131          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 121           |
| Time                    | 2.73e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.885         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0071674963  |
| n_timesteps             | 128320000     |
-------------------------------------------

 ---------------- Iteration 401 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 401           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0071354224 |
| LossBefore              | 2.7013818e-09 |
| MeanKL                  | 0.008359987   |
| MeanKLBefore            | 5.960632e-09  |
| Step_0-AverageDiscou... | 136           |
| Step_0-AveragePolicyStd | 0.4031245     |
| Step_0-AverageReturn    | 306           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 637           |
| Step_0-MinReturn        | 19.3          |
| Step_0-NumTrajs         | 1071          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 120           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.4031545     |
| Step_1-AverageReturn    | 300           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 657           |
| Step_1-MinReturn        | 18.7          |
| Step_1-NumTrajs         | 1160          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 124           |
| Time                    | 2.74e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.891         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007135425   |
| n_timesteps             | 128640000     |
-------------------------------------------

 ---------------- Iteration 402 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 402          |
| ItrTime                 | 70.8         |
| LossAfter               | -0.006701363 |
| LossBefore              | 8.185566e-09 |
| MeanKL                  | 0.008064832  |
| MeanKLBefore            | 1.787939e-08 |
| Step_0-AverageDiscou... | 132          |
| Step_0-AveragePolicyStd | 0.40267378   |
| Step_0-AverageReturn    | 299          |
| Step_0-EnvExecTime      | 23.7         |
| Step_0-MaxReturn        | 694          |
| Step_0-MinReturn        | 21.7         |
| Step_0-NumTrajs         | 1029         |
| Step_0-PolicyExecTime   | 1.32         |
| Step_0-StdReturn        | 113          |
| Step_1-AverageDiscou... | 134          |
| Step_1-AveragePolicyStd | 0.40260324   |
| Step_1-AverageReturn    | 296          |
| Step_1-EnvExecTime      | 23.7         |
| Step_1-MaxReturn        | 655          |
| Step_1-MinReturn        | 14           |
| Step_1-NumTrajs         | 1124         |
| Step_1-PolicyExecTime   | 4.11         |
| Step_1-StdReturn        | 118          |
| Time                    | 2.75e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.864        |
| Time-Sampling           | 56.1         |
| Time-TotalInner         | 57.2         |
| dLoss                   | 0.006701371  |
| n_timesteps             | 128960000    |
------------------------------------------

 ---------------- Iteration 403 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 403           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.006701751  |
| LossBefore              | -9.390691e-09 |
| MeanKL                  | 0.007461734   |
| MeanKLBefore            | 1.4887144e-09 |
| Step_0-AverageDiscou... | 134           |
| Step_0-AveragePolicyStd | 0.4027193     |
| Step_0-AverageReturn    | 305           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 607           |
| Step_0-MinReturn        | 11            |
| Step_0-NumTrajs         | 1062          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 125           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.40257716    |
| Step_1-AverageReturn    | 295           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 721           |
| Step_1-MinReturn        | 12.5          |
| Step_1-NumTrajs         | 1127          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 126           |
| Time                    | 2.75e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.879         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.006701742   |
| n_timesteps             | 129280000     |
-------------------------------------------

 ---------------- Iteration 404 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 404            |
| ItrTime                 | 71             |
| LossAfter               | -0.007397853   |
| LossBefore              | -1.1313771e-09 |
| MeanKL                  | 0.007838525    |
| MeanKLBefore            | 1.3407421e-08  |
| Step_0-AverageDiscou... | 135            |
| Step_0-AveragePolicyStd | 0.40308088     |
| Step_0-AverageReturn    | 306            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 678            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 1016           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 112            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.40305614     |
| Step_1-AverageReturn    | 317            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 711            |
| Step_1-MinReturn        | 23.5           |
| Step_1-NumTrajs         | 1110           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 118            |
| Time                    | 2.76e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.881          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007397852    |
| n_timesteps             | 129600000      |
--------------------------------------------

 ---------------- Iteration 405 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 405           |
| ItrTime                 | 68.5          |
| LossAfter               | -0.00695394   |
| LossBefore              | -3.875691e-10 |
| MeanKL                  | 0.008252074   |
| MeanKLBefore            | -8.938941e-09 |
| Step_0-AverageDiscou... | 119           |
| Step_0-AveragePolicyStd | 0.4033096     |
| Step_0-AverageReturn    | 240           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 628           |
| Step_0-MinReturn        | 17.5          |
| Step_0-NumTrajs         | 1307          |
| Step_0-PolicyExecTime   | 1.25          |
| Step_0-StdReturn        | 130           |
| Step_1-AverageDiscou... | 123           |
| Step_1-AveragePolicyStd | 0.40334633    |
| Step_1-AverageReturn    | 254           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 644           |
| Step_1-MinReturn        | 16.1          |
| Step_1-NumTrajs         | 1272          |
| Step_1-PolicyExecTime   | 3.93          |
| Step_1-StdReturn        | 134           |
| Time                    | 2.77e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.948         |
| Time-Sampling           | 53.7          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0069539393  |
| n_timesteps             | 129920000     |
-------------------------------------------

 ---------------- Iteration 406 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 406            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0071998746  |
| LossBefore              | -1.7760028e-08 |
| MeanKL                  | 0.009487468    |
| MeanKLBefore            | 1.7879287e-08  |
| Step_0-AverageDiscou... | 128            |
| Step_0-AveragePolicyStd | 0.40302575     |
| Step_0-AverageReturn    | 268            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 653            |
| Step_0-MinReturn        | 21.9           |
| Step_0-NumTrajs         | 1223           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 117            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.4029542      |
| Step_1-AverageReturn    | 280            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 676            |
| Step_1-MinReturn        | 6.22           |
| Step_1-NumTrajs         | 1204           |
| Step_1-PolicyExecTime   | 4.01           |
| Step_1-StdReturn        | 121            |
| Time                    | 2.77e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.913          |
| Time-Sampling           | 54.3           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.007199857    |
| n_timesteps             | 130240000      |
--------------------------------------------

 ---------------- Iteration 407 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 407            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.0076854723  |
| LossBefore              | -2.1371804e-09 |
| MeanKL                  | 0.008742021    |
| MeanKLBefore            | 2.5327825e-08  |
| Step_0-AverageDiscou... | 134            |
| Step_0-AveragePolicyStd | 0.40287113     |
| Step_0-AverageReturn    | 306            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 623            |
| Step_0-MinReturn        | 21.7           |
| Step_0-NumTrajs         | 1041           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 119            |
| Step_1-AverageDiscou... | 135            |
| Step_1-AveragePolicyStd | 0.4028188      |
| Step_1-AverageReturn    | 299            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 627            |
| Step_1-MinReturn        | 27.9           |
| Step_1-NumTrajs         | 1129           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 124            |
| Time                    | 2.78e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.87           |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.00768547     |
| n_timesteps             | 130560000      |
--------------------------------------------

 ---------------- Iteration 408 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 408            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.007251656   |
| LossBefore              | 1.8604929e-09  |
| MeanKL                  | 0.0076339184   |
| MeanKLBefore            | -5.9582703e-09 |
| Step_0-AverageDiscou... | 134            |
| Step_0-AveragePolicyStd | 0.40327823     |
| Step_0-AverageReturn    | 293            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 639            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 1136           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 124            |
| Step_1-AverageDiscou... | 136            |
| Step_1-AveragePolicyStd | 0.40324512     |
| Step_1-AverageReturn    | 300            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 708            |
| Step_1-MinReturn        | 17.1           |
| Step_1-NumTrajs         | 1158           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 128            |
| Time                    | 2.79e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.933          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007251658    |
| n_timesteps             | 130880000      |
--------------------------------------------

 ---------------- Iteration 409 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 409           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.0066893785 |
| LossBefore              | 3.6808168e-09 |
| MeanKL                  | 0.008096778   |
| MeanKLBefore            | 2.0858062e-08 |
| Step_0-AverageDiscou... | 136           |
| Step_0-AveragePolicyStd | 0.40324485    |
| Step_0-AverageReturn    | 298           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 669           |
| Step_0-MinReturn        | 24.2          |
| Step_0-NumTrajs         | 1093          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 107           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.40321055    |
| Step_1-AverageReturn    | 295           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 646           |
| Step_1-MinReturn        | 6.9           |
| Step_1-NumTrajs         | 1190          |
| Step_1-PolicyExecTime   | 4             |
| Step_1-StdReturn        | 117           |
| Time                    | 2.79e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.881         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0066893823  |
| n_timesteps             | 131200000     |
-------------------------------------------

 ---------------- Iteration 410 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 410            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0069033606  |
| LossBefore              | -1.7051844e-09 |
| MeanKL                  | 0.0074773603   |
| MeanKLBefore            | -5.9601746e-09 |
| Step_0-AverageDiscou... | 133            |
| Step_0-AveragePolicyStd | 0.40251297     |
| Step_0-AverageReturn    | 310            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 681            |
| Step_0-MinReturn        | 2.97           |
| Step_0-NumTrajs         | 1019           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 130            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.4024943      |
| Step_1-AverageReturn    | 314            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 660            |
| Step_1-MinReturn        | 11.8           |
| Step_1-NumTrajs         | 1084           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 124            |
| Time                    | 2.8e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0069033587   |
| n_timesteps             | 131520000      |
--------------------------------------------

 ---------------- Iteration 411 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 411           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.0068878653 |
| LossBefore              | -7.285975e-09 |
| MeanKL                  | 0.007367571   |
| MeanKLBefore            | 1.9849012e-12 |
| Step_0-AverageDiscou... | 134           |
| Step_0-AveragePolicyStd | 0.40128767    |
| Step_0-AverageReturn    | 287           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 591           |
| Step_0-MinReturn        | 29            |
| Step_0-NumTrajs         | 1169          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 114           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.40121692    |
| Step_1-AverageReturn    | 297           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 723           |
| Step_1-MinReturn        | 16.2          |
| Step_1-NumTrajs         | 1185          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 123           |
| Time                    | 2.81e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.901         |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.3          |
| dLoss                   | 0.006887858   |
| n_timesteps             | 131840000     |
-------------------------------------------

 ---------------- Iteration 412 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 412            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007162039   |
| LossBefore              | 1.1580452e-09  |
| MeanKL                  | 0.008593105    |
| MeanKLBefore            | -1.7881861e-08 |
| Step_0-AverageDiscou... | 131            |
| Step_0-AveragePolicyStd | 0.40147442     |
| Step_0-AverageReturn    | 294            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 683            |
| Step_0-MinReturn        | 11.5           |
| Step_0-NumTrajs         | 1090           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 131            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.4015314      |
| Step_1-AverageReturn    | 305            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 652            |
| Step_1-MinReturn        | 8.73           |
| Step_1-NumTrajs         | 1114           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 127            |
| Time                    | 2.82e+04       |
| Time-InnerStep          | 0.183          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.879          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.00716204     |
| n_timesteps             | 132160000      |
--------------------------------------------

 ---------------- Iteration 413 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 413           |
| ItrTime                 | 70            |
| LossAfter               | -0.006434062  |
| LossBefore              | 1.9972743e-09 |
| MeanKL                  | 0.007096379   |
| MeanKLBefore            | 4.4686814e-09 |
| Step_0-AverageDiscou... | 126           |
| Step_0-AveragePolicyStd | 0.4010445     |
| Step_0-AverageReturn    | 270           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 638           |
| Step_0-MinReturn        | 8.17          |
| Step_0-NumTrajs         | 1200          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 136           |
| Step_1-AverageDiscou... | 134           |
| Step_1-AveragePolicyStd | 0.40092954    |
| Step_1-AverageReturn    | 291           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 707           |
| Step_1-MinReturn        | 9.63          |
| Step_1-NumTrajs         | 1179          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 130           |
| Time                    | 2.82e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.959         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.006434064   |
| n_timesteps             | 132480000     |
-------------------------------------------

 ---------------- Iteration 414 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 414           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.0068336762 |
| LossBefore              | -2.239555e-09 |
| MeanKL                  | 0.007850407   |
| MeanKLBefore            | 7.56728e-14   |
| Step_0-AverageDiscou... | 133           |
| Step_0-AveragePolicyStd | 0.40062916    |
| Step_0-AverageReturn    | 297           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 658           |
| Step_0-MinReturn        | 10.7          |
| Step_0-NumTrajs         | 1099          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 129           |
| Step_1-AverageDiscou... | 138           |
| Step_1-AveragePolicyStd | 0.40061775    |
| Step_1-AverageReturn    | 302           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 692           |
| Step_1-MinReturn        | 14.4          |
| Step_1-NumTrajs         | 1164          |
| Step_1-PolicyExecTime   | 4.47          |
| Step_1-StdReturn        | 130           |
| Time                    | 2.83e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.867         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.006833674   |
| n_timesteps             | 132800000     |
-------------------------------------------

 ---------------- Iteration 415 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 415            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.0073638046  |
| LossBefore              | -1.3128795e-08 |
| MeanKL                  | 0.00810768     |
| MeanKLBefore            | -1.3410097e-08 |
| Step_0-AverageDiscou... | 136            |
| Step_0-AveragePolicyStd | 0.40021643     |
| Step_0-AverageReturn    | 304            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 660            |
| Step_0-MinReturn        | 8.68           |
| Step_0-NumTrajs         | 1102           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 132            |
| Step_1-AverageDiscou... | 133            |
| Step_1-AveragePolicyStd | 0.40023193     |
| Step_1-AverageReturn    | 285            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 663            |
| Step_1-MinReturn        | 12             |
| Step_1-NumTrajs         | 1227           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 125            |
| Time                    | 2.84e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.904          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56.1           |
| dLoss                   | 0.0073637916   |
| n_timesteps             | 133120000      |
--------------------------------------------

 ---------------- Iteration 416 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 416           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.006759894  |
| LossBefore              | -7.207702e-09 |
| MeanKL                  | 0.00772378    |
| MeanKLBefore            | 4.4679114e-09 |
| Step_0-AverageDiscou... | 122           |
| Step_0-AveragePolicyStd | 0.3999727     |
| Step_0-AverageReturn    | 267           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 643           |
| Step_0-MinReturn        | 17.4          |
| Step_0-NumTrajs         | 1134          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 131           |
| Step_1-AverageDiscou... | 131           |
| Step_1-AveragePolicyStd | 0.39975983    |
| Step_1-AverageReturn    | 291           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 806           |
| Step_1-MinReturn        | 15            |
| Step_1-NumTrajs         | 1134          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 123           |
| Time                    | 2.84e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.89          |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.006759887   |
| n_timesteps             | 133440000     |
-------------------------------------------

 ---------------- Iteration 417 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 417            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.006522864   |
| LossBefore              | -3.0061245e-09 |
| MeanKL                  | 0.007879416    |
| MeanKLBefore            | -4.468112e-09  |
| Step_0-AverageDiscou... | 129            |
| Step_0-AveragePolicyStd | 0.39921784     |
| Step_0-AverageReturn    | 283            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 611            |
| Step_0-MinReturn        | 23             |
| Step_0-NumTrajs         | 1104           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 124            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.39912125     |
| Step_1-AverageReturn    | 282            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 683            |
| Step_1-MinReturn        | 0.464          |
| Step_1-NumTrajs         | 1189           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 130            |
| Time                    | 2.85e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.894          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0065228613   |
| n_timesteps             | 133760000      |
--------------------------------------------

 ---------------- Iteration 418 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 418           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.0065287813 |
| LossBefore              | 8.168255e-09  |
| MeanKL                  | 0.0076191416  |
| MeanKLBefore            | -7.451101e-09 |
| Step_0-AverageDiscou... | 135           |
| Step_0-AveragePolicyStd | 0.3997667     |
| Step_0-AverageReturn    | 292           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 621           |
| Step_0-MinReturn        | 22.1          |
| Step_0-NumTrajs         | 1158          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 116           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.39978376    |
| Step_1-AverageReturn    | 278           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 622           |
| Step_1-MinReturn        | 18.8          |
| Step_1-NumTrajs         | 1242          |
| Step_1-PolicyExecTime   | 3.94          |
| Step_1-StdReturn        | 114           |
| Time                    | 2.86e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.908         |
| Time-Sampling           | 54.4          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.0065287896  |
| n_timesteps             | 134080000     |
-------------------------------------------

 ---------------- Iteration 419 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 419           |
| ItrTime                 | 68.8          |
| LossAfter               | -0.0072092162 |
| LossBefore              | 3.4170106e-10 |
| MeanKL                  | 0.008604493   |
| MeanKLBefore            | 2.982801e-09  |
| Step_0-AverageDiscou... | 127           |
| Step_0-AveragePolicyStd | 0.39932495    |
| Step_0-AverageReturn    | 275           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 673           |
| Step_0-MinReturn        | 13.3          |
| Step_0-NumTrajs         | 1153          |
| Step_0-PolicyExecTime   | 1.26          |
| Step_0-StdReturn        | 132           |
| Step_1-AverageDiscou... | 128           |
| Step_1-AveragePolicyStd | 0.3992789     |
| Step_1-AverageReturn    | 272           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 690           |
| Step_1-MinReturn        | 13.7          |
| Step_1-NumTrajs         | 1220          |
| Step_1-PolicyExecTime   | 3.96          |
| Step_1-StdReturn        | 131           |
| Time                    | 2.86e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.915         |
| Time-Sampling           | 54            |
| Time-TotalInner         | 55.1          |
| dLoss                   | 0.0072092167  |
| n_timesteps             | 134400000     |
-------------------------------------------

 ---------------- Iteration 420 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 420            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0069249393  |
| LossBefore              | -5.4380678e-09 |
| MeanKL                  | 0.007840581    |
| MeanKLBefore            | -5.9605947e-09 |
| Step_0-AverageDiscou... | 136            |
| Step_0-AveragePolicyStd | 0.39849487     |
| Step_0-AverageReturn    | 312            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 622            |
| Step_0-MinReturn        | 23.1           |
| Step_0-NumTrajs         | 1047           |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 124            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.39839077     |
| Step_1-AverageReturn    | 304            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 683            |
| Step_1-MinReturn        | 13.9           |
| Step_1-NumTrajs         | 1142           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 125            |
| Time                    | 2.87e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.897          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.0069249338   |
| n_timesteps             | 134720000      |
--------------------------------------------

 ---------------- Iteration 421 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 421           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.006994769  |
| LossBefore              | 6.653289e-09  |
| MeanKL                  | 0.007619771   |
| MeanKLBefore            | 5.9612915e-09 |
| Step_0-AverageDiscou... | 133           |
| Step_0-AveragePolicyStd | 0.398039      |
| Step_0-AverageReturn    | 305           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 659           |
| Step_0-MinReturn        | 11.8          |
| Step_0-NumTrajs         | 1067          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 134           |
| Step_1-AverageDiscou... | 136           |
| Step_1-AveragePolicyStd | 0.39801037    |
| Step_1-AverageReturn    | 302           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 650           |
| Step_1-MinReturn        | 16.4          |
| Step_1-NumTrajs         | 1134          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 125           |
| Time                    | 2.88e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.863         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0069947755  |
| n_timesteps             | 135040000     |
-------------------------------------------

 ---------------- Iteration 422 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 422           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0069527077 |
| LossBefore              | 1.4885359e-09 |
| MeanKL                  | 0.008159789   |
| MeanKLBefore            | 1.4899662e-08 |
| Step_0-AverageDiscou... | 136           |
| Step_0-AveragePolicyStd | 0.39809707    |
| Step_0-AverageReturn    | 312           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 673           |
| Step_0-MinReturn        | 17.8          |
| Step_0-NumTrajs         | 1033          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 131           |
| Step_1-AverageDiscou... | 136           |
| Step_1-AveragePolicyStd | 0.3980593     |
| Step_1-AverageReturn    | 306           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 643           |
| Step_1-MinReturn        | 20.4          |
| Step_1-NumTrajs         | 1082          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 126           |
| Time                    | 2.89e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.006952709   |
| n_timesteps             | 135360000     |
-------------------------------------------

 ---------------- Iteration 423 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 423           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.006965364  |
| LossBefore              | 1.1734771e-08 |
| MeanKL                  | 0.007734542   |
| MeanKLBefore            | 5.030643e-13  |
| Step_0-AverageDiscou... | 130           |
| Step_0-AveragePolicyStd | 0.39829823    |
| Step_0-AverageReturn    | 299           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 615           |
| Step_0-MinReturn        | 9.9           |
| Step_0-NumTrajs         | 1028          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 128           |
| Step_1-AverageDiscou... | 136           |
| Step_1-AveragePolicyStd | 0.39826027    |
| Step_1-AverageReturn    | 313           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 679           |
| Step_1-MinReturn        | 17.3          |
| Step_1-NumTrajs         | 1077          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 130           |
| Time                    | 2.89e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0069653755  |
| n_timesteps             | 135680000     |
-------------------------------------------

 ---------------- Iteration 424 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 424           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.006563065  |
| LossBefore              | 2.0600428e-09 |
| MeanKL                  | 0.0075355074  |
| MeanKLBefore            | 1.7880318e-08 |
| Step_0-AverageDiscou... | 138           |
| Step_0-AveragePolicyStd | 0.39812392    |
| Step_0-AverageReturn    | 317           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 670           |
| Step_0-MinReturn        | 16.4          |
| Step_0-NumTrajs         | 1043          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 120           |
| Step_1-AverageDiscou... | 140           |
| Step_1-AveragePolicyStd | 0.3980348     |
| Step_1-AverageReturn    | 318           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 622           |
| Step_1-MinReturn        | 24.6          |
| Step_1-NumTrajs         | 1069          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 120           |
| Time                    | 2.9e+04       |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.006563067   |
| n_timesteps             | 136000000     |
-------------------------------------------

 ---------------- Iteration 425 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 425            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0071204724  |
| LossBefore              | 5.946924e-09   |
| MeanKL                  | 0.0072532655   |
| MeanKLBefore            | -1.1919022e-08 |
| Step_0-AverageDiscou... | 137            |
| Step_0-AveragePolicyStd | 0.39810148     |
| Step_0-AverageReturn    | 310            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 692            |
| Step_0-MinReturn        | 21             |
| Step_0-NumTrajs         | 1087           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 132            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.39801827     |
| Step_1-AverageReturn    | 308            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 696            |
| Step_1-MinReturn        | 21             |
| Step_1-NumTrajs         | 1159           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 132            |
| Time                    | 2.91e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.911          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.0071204784   |
| n_timesteps             | 136320000      |
--------------------------------------------

 ---------------- Iteration 426 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 426           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.0070902766 |
| LossBefore              | -1.262331e-09 |
| MeanKL                  | 0.0074801086  |
| MeanKLBefore            | 7.452738e-09  |
| Step_0-AverageDiscou... | 139           |
| Step_0-AveragePolicyStd | 0.39797166    |
| Step_0-AverageReturn    | 319           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 677           |
| Step_0-MinReturn        | 19.1          |
| Step_0-NumTrajs         | 1034          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 117           |
| Step_1-AverageDiscou... | 140           |
| Step_1-AveragePolicyStd | 0.3978906     |
| Step_1-AverageReturn    | 317           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 665           |
| Step_1-MinReturn        | 12.4          |
| Step_1-NumTrajs         | 1104          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 128           |
| Time                    | 2.91e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.863         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.007090275   |
| n_timesteps             | 136640000     |
-------------------------------------------

 ---------------- Iteration 427 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 427           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007125179  |
| LossBefore              | 9.619575e-09  |
| MeanKL                  | 0.00777594    |
| MeanKLBefore            | 1.4916864e-09 |
| Step_0-AverageDiscou... | 130           |
| Step_0-AveragePolicyStd | 0.39786318    |
| Step_0-AverageReturn    | 293           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 615           |
| Step_0-MinReturn        | 15.5          |
| Step_0-NumTrajs         | 1070          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 129           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.3978188     |
| Step_1-AverageReturn    | 297           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 678           |
| Step_1-MinReturn        | 11.6          |
| Step_1-NumTrajs         | 1139          |
| Step_1-PolicyExecTime   | 3.97          |
| Step_1-StdReturn        | 133           |
| Time                    | 2.92e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.855         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0071251886  |
| n_timesteps             | 136960000     |
-------------------------------------------

 ---------------- Iteration 428 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 428           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0067914026 |
| LossBefore              | 9.024205e-10  |
| MeanKL                  | 0.007979921   |
| MeanKLBefore            | 1.489899e-08  |
| Step_0-AverageDiscou... | 131           |
| Step_0-AveragePolicyStd | 0.39706618    |
| Step_0-AverageReturn    | 296           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 599           |
| Step_0-MinReturn        | 22.7          |
| Step_0-NumTrajs         | 1058          |
| Step_0-PolicyExecTime   | 1.73          |
| Step_0-StdReturn        | 132           |
| Step_1-AverageDiscou... | 128           |
| Step_1-AveragePolicyStd | 0.39706165    |
| Step_1-AverageReturn    | 282           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 702           |
| Step_1-MinReturn        | 5.32          |
| Step_1-NumTrajs         | 1137          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 126           |
| Time                    | 2.93e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.851         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0067914035  |
| n_timesteps             | 137280000     |
-------------------------------------------

 ---------------- Iteration 429 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 429           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0070975684 |
| LossBefore              | 3.8698538e-09 |
| MeanKL                  | 0.0077784434  |
| MeanKLBefore            | -4.472281e-09 |
| Step_0-AverageDiscou... | 130           |
| Step_0-AveragePolicyStd | 0.39634824    |
| Step_0-AverageReturn    | 280           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 602           |
| Step_0-MinReturn        | 22.3          |
| Step_0-NumTrajs         | 1162          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 124           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.39639282    |
| Step_1-AverageReturn    | 281           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 690           |
| Step_1-MinReturn        | 19.1          |
| Step_1-NumTrajs         | 1227          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 121           |
| Time                    | 2.94e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.89          |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007097572   |
| n_timesteps             | 137600000     |
-------------------------------------------

 ---------------- Iteration 430 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 430           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.0071617514 |
| LossBefore              | 7.8897744e-10 |
| MeanKL                  | 0.008352208   |
| MeanKLBefore            | 8.940314e-09  |
| Step_0-AverageDiscou... | 130           |
| Step_0-AveragePolicyStd | 0.3961927     |
| Step_0-AverageReturn    | 291           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 715           |
| Step_0-MinReturn        | 8.49          |
| Step_0-NumTrajs         | 1074          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 130           |
| Step_1-AverageDiscou... | 134           |
| Step_1-AveragePolicyStd | 0.39611647    |
| Step_1-AverageReturn    | 293           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 691           |
| Step_1-MinReturn        | 18.2          |
| Step_1-NumTrajs         | 1149          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 132           |
| Time                    | 2.94e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.862         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0071617523  |
| n_timesteps             | 137920000     |
-------------------------------------------

 ---------------- Iteration 431 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 431            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0068464936  |
| LossBefore              | 2.388163e-09   |
| MeanKL                  | 0.0075648623   |
| MeanKLBefore            | 1.34095846e-08 |
| Step_0-AverageDiscou... | 134            |
| Step_0-AveragePolicyStd | 0.3952883      |
| Step_0-AverageReturn    | 304            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 647            |
| Step_0-MinReturn        | 8.11           |
| Step_0-NumTrajs         | 1064           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 123            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.39521912     |
| Step_1-AverageReturn    | 313            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 653            |
| Step_1-MinReturn        | 11.8           |
| Step_1-NumTrajs         | 1073           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 128            |
| Time                    | 2.95e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.861          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.006846496    |
| n_timesteps             | 138240000      |
--------------------------------------------

 ---------------- Iteration 432 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 432            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0071100853  |
| LossBefore              | -3.0107459e-09 |
| MeanKL                  | 0.0078020403   |
| MeanKLBefore            | -7.4490507e-09 |
| Step_0-AverageDiscou... | 123            |
| Step_0-AveragePolicyStd | 0.39543238     |
| Step_0-AverageReturn    | 273            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 601            |
| Step_0-MinReturn        | 15.5           |
| Step_0-NumTrajs         | 1126           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 143            |
| Step_1-AverageDiscou... | 126            |
| Step_1-AveragePolicyStd | 0.39539862     |
| Step_1-AverageReturn    | 273            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 684            |
| Step_1-MinReturn        | 13.6           |
| Step_1-NumTrajs         | 1170           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 133            |
| Time                    | 2.96e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.887          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0071100825   |
| n_timesteps             | 138560000      |
--------------------------------------------

 ---------------- Iteration 433 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 433           |
| ItrTime                 | 68.7          |
| LossAfter               | -0.0070984126 |
| LossBefore              | 1.1163243e-09 |
| MeanKL                  | 0.007684768   |
| MeanKLBefore            | -1.49067e-09  |
| Step_0-AverageDiscou... | 138           |
| Step_0-AveragePolicyStd | 0.39594787    |
| Step_0-AverageReturn    | 312           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 710           |
| Step_0-MinReturn        | 7.18          |
| Step_0-NumTrajs         | 1077          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 122           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.3958248     |
| Step_1-AverageReturn    | 302           |
| Step_1-EnvExecTime      | 22.4          |
| Step_1-MaxReturn        | 664           |
| Step_1-MinReturn        | 8.26          |
| Step_1-NumTrajs         | 1150          |
| Step_1-PolicyExecTime   | 4             |
| Step_1-StdReturn        | 121           |
| Time                    | 2.96e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.863         |
| Time-Sampling           | 54            |
| Time-TotalInner         | 55            |
| dLoss                   | 0.0070984135  |
| n_timesteps             | 138880000     |
-------------------------------------------

 ---------------- Iteration 434 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 434           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0072633536 |
| LossBefore              | 6.025288e-09  |
| MeanKL                  | 0.007811575   |
| MeanKLBefore            | -7.974066e-13 |
| Step_0-AverageDiscou... | 141           |
| Step_0-AveragePolicyStd | 0.39533314    |
| Step_0-AverageReturn    | 325           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 667           |
| Step_0-MinReturn        | 13.1          |
| Step_0-NumTrajs         | 1031          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 121           |
| Step_1-AverageDiscou... | 146           |
| Step_1-AveragePolicyStd | 0.3953106     |
| Step_1-AverageReturn    | 335           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 689           |
| Step_1-MinReturn        | 5.66          |
| Step_1-NumTrajs         | 1084          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 135           |
| Time                    | 2.97e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0072633596  |
| n_timesteps             | 139200000     |
-------------------------------------------

 ---------------- Iteration 435 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 435           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007168895  |
| LossBefore              | 5.5022418e-09 |
| MeanKL                  | 0.007874804   |
| MeanKLBefore            | 2.9787128e-09 |
| Step_0-AverageDiscou... | 141           |
| Step_0-AveragePolicyStd | 0.39560443    |
| Step_0-AverageReturn    | 324           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 633           |
| Step_0-MinReturn        | 17.6          |
| Step_0-NumTrajs         | 1026          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 111           |
| Step_1-AverageDiscou... | 138           |
| Step_1-AveragePolicyStd | 0.39545867    |
| Step_1-AverageReturn    | 303           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 687           |
| Step_1-MinReturn        | 20.5          |
| Step_1-NumTrajs         | 1147          |
| Step_1-PolicyExecTime   | 4             |
| Step_1-StdReturn        | 118           |
| Time                    | 2.98e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.829         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.0071689007  |
| n_timesteps             | 139520000     |
-------------------------------------------

 ---------------- Iteration 436 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 436            |
| ItrTime                 | 69.3           |
| LossAfter               | -0.006903327   |
| LossBefore              | -2.7903014e-12 |
| MeanKL                  | 0.008435088    |
| MeanKLBefore            | 2.2349209e-08  |
| Step_0-AverageDiscou... | 127            |
| Step_0-AveragePolicyStd | 0.39562726     |
| Step_0-AverageReturn    | 275            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 624            |
| Step_0-MinReturn        | 4.93           |
| Step_0-NumTrajs         | 1189           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 137            |
| Step_1-AverageDiscou... | 131            |
| Step_1-AveragePolicyStd | 0.39562142     |
| Step_1-AverageReturn    | 282            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 724            |
| Step_1-MinReturn        | 4.07           |
| Step_1-NumTrajs         | 1209           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 137            |
| Time                    | 2.98e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.92           |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.006903327    |
| n_timesteps             | 139840000      |
--------------------------------------------

 ---------------- Iteration 437 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 437            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0072468584  |
| LossBefore              | -1.1871139e-08 |
| MeanKL                  | 0.008508084    |
| MeanKLBefore            | 7.4491227e-09  |
| Step_0-AverageDiscou... | 141            |
| Step_0-AveragePolicyStd | 0.39556068     |
| Step_0-AverageReturn    | 321            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 648            |
| Step_0-MinReturn        | 15.7           |
| Step_0-NumTrajs         | 1067           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 115            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.39544526     |
| Step_1-AverageReturn    | 313            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 661            |
| Step_1-MinReturn        | 24.3           |
| Step_1-NumTrajs         | 1133           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 120            |
| Time                    | 2.99e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.864          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007246847    |
| n_timesteps             | 140160000      |
--------------------------------------------

 ---------------- Iteration 438 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 438           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007020074  |
| LossBefore              | 6.4259496e-09 |
| MeanKL                  | 0.008079378   |
| MeanKLBefore            | 2.9796037e-09 |
| Step_0-AverageDiscou... | 139           |
| Step_0-AveragePolicyStd | 0.39508218    |
| Step_0-AverageReturn    | 311           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 641           |
| Step_0-MinReturn        | 14.7          |
| Step_0-NumTrajs         | 1072          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 122           |
| Step_1-AverageDiscou... | 142           |
| Step_1-AveragePolicyStd | 0.3949974     |
| Step_1-AverageReturn    | 308           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 759           |
| Step_1-MinReturn        | 21.2          |
| Step_1-NumTrajs         | 1163          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 129           |
| Time                    | 3e+04         |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.862         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0070200805  |
| n_timesteps             | 140480000     |
-------------------------------------------

 ---------------- Iteration 439 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 439           |
| ItrTime                 | 73            |
| LossAfter               | -0.0068955673 |
| LossBefore              | 5.572609e-11  |
| MeanKL                  | 0.0076419385  |
| MeanKLBefore            | -4.471076e-09 |
| Step_0-AverageDiscou... | 133           |
| Step_0-AveragePolicyStd | 0.39560696    |
| Step_0-AverageReturn    | 311           |
| Step_0-EnvExecTime      | 24.5          |
| Step_0-MaxReturn        | 644           |
| Step_0-MinReturn        | 14.5          |
| Step_0-NumTrajs         | 1004          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 127           |
| Step_1-AverageDiscou... | 142           |
| Step_1-AveragePolicyStd | 0.39546886    |
| Step_1-AverageReturn    | 340           |
| Step_1-EnvExecTime      | 25            |
| Step_1-MaxReturn        | 750           |
| Step_1-MinReturn        | 18.2          |
| Step_1-NumTrajs         | 998           |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 132           |
| Time                    | 3.01e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.819         |
| Time-Sampling           | 58.3          |
| Time-TotalInner         | 59.3          |
| dLoss                   | 0.0068955673  |
| n_timesteps             | 140800000     |
-------------------------------------------

 ---------------- Iteration 440 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 440            |
| ItrTime                 | 68.7           |
| LossAfter               | -0.006936426   |
| LossBefore              | -5.888226e-09  |
| MeanKL                  | 0.008287084    |
| MeanKLBefore            | -4.4684385e-09 |
| Step_0-AverageDiscou... | 134            |
| Step_0-AveragePolicyStd | 0.39529184     |
| Step_0-AverageReturn    | 288            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 651            |
| Step_0-MinReturn        | 11.7           |
| Step_0-NumTrajs         | 1179           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 122            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.39531296     |
| Step_1-AverageReturn    | 293            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 712            |
| Step_1-MinReturn        | 12.5           |
| Step_1-NumTrajs         | 1219           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 128            |
| Time                    | 3.01e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.902          |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55.1           |
| dLoss                   | 0.0069364198   |
| n_timesteps             | 141120000      |
--------------------------------------------

 ---------------- Iteration 441 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 441           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0063668936 |
| LossBefore              | 1.4374197e-08 |
| MeanKL                  | 0.0074579576  |
| MeanKLBefore            | 8.939172e-09  |
| Step_0-AverageDiscou... | 136           |
| Step_0-AveragePolicyStd | 0.39525384    |
| Step_0-AverageReturn    | 310           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 663           |
| Step_0-MinReturn        | -2.19         |
| Step_0-NumTrajs         | 1053          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 133           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39520076    |
| Step_1-AverageReturn    | 331           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 707           |
| Step_1-MinReturn        | 23            |
| Step_1-NumTrajs         | 1071          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 129           |
| Time                    | 3.02e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.855         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.006366908   |
| n_timesteps             | 141440000     |
-------------------------------------------

 ---------------- Iteration 442 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 442           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.006935136  |
| LossBefore              | -1.279389e-08 |
| MeanKL                  | 0.008033174   |
| MeanKLBefore            | -5.958541e-09 |
| Step_0-AverageDiscou... | 148           |
| Step_0-AveragePolicyStd | 0.3956102     |
| Step_0-AverageReturn    | 350           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 667           |
| Step_0-MinReturn        | 19.8          |
| Step_0-NumTrajs         | 983           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 113           |
| Step_1-AverageDiscou... | 142           |
| Step_1-AveragePolicyStd | 0.39554048    |
| Step_1-AverageReturn    | 317           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 659           |
| Step_1-MinReturn        | 24.3          |
| Step_1-NumTrajs         | 1141          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 121           |
| Time                    | 3.03e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.828         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0069351234  |
| n_timesteps             | 141760000     |
-------------------------------------------

 ---------------- Iteration 443 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 443            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0072302604  |
| LossBefore              | 2.3025586e-09  |
| MeanKL                  | 0.00836893     |
| MeanKLBefore            | -4.4700856e-09 |
| Step_0-AverageDiscou... | 139            |
| Step_0-AveragePolicyStd | 0.39634067     |
| Step_0-AverageReturn    | 316            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 650            |
| Step_0-MinReturn        | 8.81           |
| Step_0-NumTrajs         | 1075           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 137            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.39622805     |
| Step_1-AverageReturn    | 315            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 757            |
| Step_1-MinReturn        | 19.2           |
| Step_1-NumTrajs         | 1112           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 135            |
| Time                    | 3.03e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.859          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0072302627   |
| n_timesteps             | 142080000      |
--------------------------------------------

 ---------------- Iteration 444 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 444            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.007137291   |
| LossBefore              | -4.768875e-09  |
| MeanKL                  | 0.0076636425   |
| MeanKLBefore            | -1.4890866e-09 |
| Step_0-AverageDiscou... | 144            |
| Step_0-AveragePolicyStd | 0.3975095      |
| Step_0-AverageReturn    | 340            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 670            |
| Step_0-MinReturn        | 6.7            |
| Step_0-NumTrajs         | 990            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 126            |
| Step_1-AverageDiscou... | 142            |
| Step_1-AveragePolicyStd | 0.39746255     |
| Step_1-AverageReturn    | 323            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 727            |
| Step_1-MinReturn        | 11.8           |
| Step_1-NumTrajs         | 1093           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 128            |
| Time                    | 3.04e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.845          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0071372865   |
| n_timesteps             | 142400000      |
--------------------------------------------

 ---------------- Iteration 445 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 445          |
| ItrTime                 | 70.7         |
| LossAfter               | -0.007479976 |
| LossBefore              | 4.096054e-09 |
| MeanKL                  | 0.007762597  |
| MeanKLBefore            | 8.939237e-09 |
| Step_0-AverageDiscou... | 146          |
| Step_0-AveragePolicyStd | 0.3977801    |
| Step_0-AverageReturn    | 342          |
| Step_0-EnvExecTime      | 23.6         |
| Step_0-MaxReturn        | 638          |
| Step_0-MinReturn        | 17.9         |
| Step_0-NumTrajs         | 1010         |
| Step_0-PolicyExecTime   | 1.32         |
| Step_0-StdReturn        | 112          |
| Step_1-AverageDiscou... | 142          |
| Step_1-AveragePolicyStd | 0.3976505    |
| Step_1-AverageReturn    | 325          |
| Step_1-EnvExecTime      | 23.7         |
| Step_1-MaxReturn        | 709          |
| Step_1-MinReturn        | 22.5         |
| Step_1-NumTrajs         | 1078         |
| Step_1-PolicyExecTime   | 4.14         |
| Step_1-StdReturn        | 128          |
| Time                    | 3.05e+04     |
| Time-InnerStep          | 0.165        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.846        |
| Time-Sampling           | 56           |
| Time-TotalInner         | 57           |
| dLoss                   | 0.00747998   |
| n_timesteps             | 142720000    |
------------------------------------------

 ---------------- Iteration 446 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 446           |
| ItrTime                 | 71            |
| LossAfter               | -0.0068703024 |
| LossBefore              | -4.544904e-09 |
| MeanKL                  | 0.007696338   |
| MeanKLBefore            | 5.9617817e-09 |
| Step_0-AverageDiscou... | 141           |
| Step_0-AveragePolicyStd | 0.39827603    |
| Step_0-AverageReturn    | 326           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 693           |
| Step_0-MinReturn        | 11.6          |
| Step_0-NumTrajs         | 1052          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 137           |
| Step_1-AverageDiscou... | 141           |
| Step_1-AveragePolicyStd | 0.39816663    |
| Step_1-AverageReturn    | 321           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 733           |
| Step_1-MinReturn        | 10.4          |
| Step_1-NumTrajs         | 1101          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 142           |
| Time                    | 3.06e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.865         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0068702977  |
| n_timesteps             | 143040000     |
-------------------------------------------

 ---------------- Iteration 447 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 447            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0070328615  |
| LossBefore              | -8.859535e-09  |
| MeanKL                  | 0.007926365    |
| MeanKLBefore            | -1.4881973e-09 |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39777473     |
| Step_0-AverageReturn    | 325            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 630            |
| Step_0-MinReturn        | 16.1           |
| Step_0-NumTrajs         | 1048           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 127            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.39774853     |
| Step_1-AverageReturn    | 320            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 662            |
| Step_1-MinReturn        | 16.3           |
| Step_1-NumTrajs         | 1110           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 131            |
| Time                    | 3.06e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.856          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.0070328526   |
| n_timesteps             | 143360000      |
--------------------------------------------

 ---------------- Iteration 448 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 448           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.0071488097 |
| LossBefore              | 3.2763314e-09 |
| MeanKL                  | 0.009143332   |
| MeanKLBefore            | 1.192169e-08  |
| Step_0-AverageDiscou... | 137           |
| Step_0-AveragePolicyStd | 0.39754656    |
| Step_0-AverageReturn    | 299           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 656           |
| Step_0-MinReturn        | 20.8          |
| Step_0-NumTrajs         | 1154          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 126           |
| Step_1-AverageDiscou... | 138           |
| Step_1-AveragePolicyStd | 0.39748645    |
| Step_1-AverageReturn    | 307           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 628           |
| Step_1-MinReturn        | 18.6          |
| Step_1-NumTrajs         | 1150          |
| Step_1-PolicyExecTime   | 3.96          |
| Step_1-StdReturn        | 129           |
| Time                    | 3.07e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.887         |
| Time-Sampling           | 54.4          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.007148813   |
| n_timesteps             | 143680000     |
-------------------------------------------

 ---------------- Iteration 449 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 449           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0065346286 |
| LossBefore              | 8.08988e-10   |
| MeanKL                  | 0.006968189   |
| MeanKLBefore            | -5.960456e-09 |
| Step_0-AverageDiscou... | 131           |
| Step_0-AveragePolicyStd | 0.3969187     |
| Step_0-AverageReturn    | 296           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 652           |
| Step_0-MinReturn        | 15            |
| Step_0-NumTrajs         | 1119          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 146           |
| Step_1-AverageDiscou... | 133           |
| Step_1-AveragePolicyStd | 0.39674863    |
| Step_1-AverageReturn    | 299           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 675           |
| Step_1-MinReturn        | 18.4          |
| Step_1-NumTrajs         | 1154          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 144           |
| Time                    | 3.08e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0065346295  |
| n_timesteps             | 144000000     |
-------------------------------------------

 ---------------- Iteration 450 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 450           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0070783375 |
| LossBefore              | -8.113153e-09 |
| MeanKL                  | 0.008400323   |
| MeanKLBefore            | -3.112177e-13 |
| Step_0-AverageDiscou... | 140           |
| Step_0-AveragePolicyStd | 0.39686176    |
| Step_0-AverageReturn    | 321           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 725           |
| Step_0-MinReturn        | 14.3          |
| Step_0-NumTrajs         | 1072          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 129           |
| Step_1-AverageDiscou... | 140           |
| Step_1-AveragePolicyStd | 0.3969771     |
| Step_1-AverageReturn    | 313           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 669           |
| Step_1-MinReturn        | 20.8          |
| Step_1-NumTrajs         | 1136          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 131           |
| Time                    | 3.08e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0070783296  |
| n_timesteps             | 144320000     |
-------------------------------------------

 ---------------- Iteration 451 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 451            |
| ItrTime                 | 69.3           |
| LossAfter               | -0.006810969   |
| LossBefore              | -2.1682471e-09 |
| MeanKL                  | 0.007867676    |
| MeanKLBefore            | -1.7880483e-08 |
| Step_0-AverageDiscou... | 137            |
| Step_0-AveragePolicyStd | 0.39628747     |
| Step_0-AverageReturn    | 308            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 643            |
| Step_0-MinReturn        | 9.61           |
| Step_0-NumTrajs         | 1115           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 137            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.3963007      |
| Step_1-AverageReturn    | 302            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 635            |
| Step_1-MinReturn        | 9.02           |
| Step_1-NumTrajs         | 1178           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 131            |
| Time                    | 3.09e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.869          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.006810967    |
| n_timesteps             | 144640000      |
--------------------------------------------

 ---------------- Iteration 452 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 452           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.006986457  |
| LossBefore              | 2.2058742e-09 |
| MeanKL                  | 0.008399504   |
| MeanKLBefore            | -2.979489e-09 |
| Step_0-AverageDiscou... | 132           |
| Step_0-AveragePolicyStd | 0.39663008    |
| Step_0-AverageReturn    | 309           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 657           |
| Step_0-MinReturn        | 2.35          |
| Step_0-NumTrajs         | 1022          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 143           |
| Step_1-AverageDiscou... | 138           |
| Step_1-AveragePolicyStd | 0.39661056    |
| Step_1-AverageReturn    | 311           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 770           |
| Step_1-MinReturn        | 5.32          |
| Step_1-NumTrajs         | 1111          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 140           |
| Time                    | 3.1e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.84          |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0069864593  |
| n_timesteps             | 144960000     |
-------------------------------------------

 ---------------- Iteration 453 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 453            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0067575127  |
| LossBefore              | -7.0144224e-09 |
| MeanKL                  | 0.00887474     |
| MeanKLBefore            | -1.1920884e-08 |
| Step_0-AverageDiscou... | 144            |
| Step_0-AveragePolicyStd | 0.39591298     |
| Step_0-AverageReturn    | 323            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 678            |
| Step_0-MinReturn        | 7.02           |
| Step_0-NumTrajs         | 1107           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 128            |
| Step_1-AverageDiscou... | 143            |
| Step_1-AveragePolicyStd | 0.39583117     |
| Step_1-AverageReturn    | 317            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 667            |
| Step_1-MinReturn        | 12.9           |
| Step_1-NumTrajs         | 1162           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 133            |
| Time                    | 3.1e+04        |
| Time-InnerStep          | 0.182          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.878          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.0067575057   |
| n_timesteps             | 145280000      |
--------------------------------------------

 ---------------- Iteration 454 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 454            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0075970516  |
| LossBefore              | 2.4952707e-09  |
| MeanKL                  | 0.008133363    |
| MeanKLBefore            | -2.9797036e-09 |
| Step_0-AverageDiscou... | 145            |
| Step_0-AveragePolicyStd | 0.39567325     |
| Step_0-AverageReturn    | 335            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 672            |
| Step_0-MinReturn        | 23.5           |
| Step_0-NumTrajs         | 1053           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 134            |
| Step_1-AverageDiscou... | 142            |
| Step_1-AveragePolicyStd | 0.39561927     |
| Step_1-AverageReturn    | 317            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 713            |
| Step_1-MinReturn        | 25.1           |
| Step_1-NumTrajs         | 1124           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 130            |
| Time                    | 3.11e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.894          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007597054    |
| n_timesteps             | 145600000      |
--------------------------------------------

 ---------------- Iteration 455 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 455            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0066080266  |
| LossBefore              | -2.2909674e-10 |
| MeanKL                  | 0.007615438    |
| MeanKLBefore            | 5.960782e-09   |
| Step_0-AverageDiscou... | 142            |
| Step_0-AveragePolicyStd | 0.3946008      |
| Step_0-AverageReturn    | 324            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 676            |
| Step_0-MinReturn        | 35.6           |
| Step_0-NumTrajs         | 1079           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 127            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.3944632      |
| Step_1-AverageReturn    | 315            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 631            |
| Step_1-MinReturn        | 3.63           |
| Step_1-NumTrajs         | 1126           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 124            |
| Time                    | 3.12e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.857          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0066080266   |
| n_timesteps             | 145920000      |
--------------------------------------------

 ---------------- Iteration 456 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 456            |
| ItrTime                 | 69             |
| LossAfter               | -0.0069081187  |
| LossBefore              | 5.6481184e-09  |
| MeanKL                  | 0.0076391725   |
| MeanKLBefore            | -2.9791134e-09 |
| Step_0-AverageDiscou... | 144            |
| Step_0-AveragePolicyStd | 0.39475715     |
| Step_0-AverageReturn    | 328            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 675            |
| Step_0-MinReturn        | 9.35           |
| Step_0-NumTrajs         | 1059           |
| Step_0-PolicyExecTime   | 1.67           |
| Step_0-StdReturn        | 123            |
| Step_1-AverageDiscou... | 145            |
| Step_1-AveragePolicyStd | 0.39467943     |
| Step_1-AverageReturn    | 326            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 693            |
| Step_1-MinReturn        | 7.03           |
| Step_1-NumTrajs         | 1128           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 134            |
| Time                    | 3.13e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.861          |
| Time-Sampling           | 54.3           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.0069081243   |
| n_timesteps             | 146240000      |
--------------------------------------------

 ---------------- Iteration 457 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 457           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0072904034 |
| LossBefore              | 3.2066794e-10 |
| MeanKL                  | 0.007981983   |
| MeanKLBefore            | 1.1918613e-08 |
| Step_0-AverageDiscou... | 150           |
| Step_0-AveragePolicyStd | 0.39493448    |
| Step_0-AverageReturn    | 351           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 655           |
| Step_0-MinReturn        | 18.8          |
| Step_0-NumTrajs         | 1017          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 123           |
| Step_1-AverageDiscou... | 148           |
| Step_1-AveragePolicyStd | 0.39484408    |
| Step_1-AverageReturn    | 333           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 699           |
| Step_1-MinReturn        | 17.9          |
| Step_1-NumTrajs         | 1122          |
| Step_1-PolicyExecTime   | 4             |
| Step_1-StdReturn        | 137           |
| Time                    | 3.13e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007290404   |
| n_timesteps             | 146560000     |
-------------------------------------------

 ---------------- Iteration 458 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 458          |
| ItrTime                 | 70.2         |
| LossAfter               | -0.007481399 |
| LossBefore              | 1.889139e-09 |
| MeanKL                  | 0.008149041  |
| MeanKLBefore            | 1.63906e-08  |
| Step_0-AverageDiscou... | 141          |
| Step_0-AveragePolicyStd | 0.39486763   |
| Step_0-AverageReturn    | 322          |
| Step_0-EnvExecTime      | 23.4         |
| Step_0-MaxReturn        | 659          |
| Step_0-MinReturn        | 18.8         |
| Step_0-NumTrajs         | 1072         |
| Step_0-PolicyExecTime   | 1.31         |
| Step_0-StdReturn        | 131          |
| Step_1-AverageDiscou... | 146          |
| Step_1-AveragePolicyStd | 0.3946109    |
| Step_1-AverageReturn    | 332          |
| Step_1-EnvExecTime      | 23.4         |
| Step_1-MaxReturn        | 663          |
| Step_1-MinReturn        | 13.3         |
| Step_1-NumTrajs         | 1099         |
| Step_1-PolicyExecTime   | 4.1          |
| Step_1-StdReturn        | 127          |
| Time                    | 3.14e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.881        |
| Time-Sampling           | 55.5         |
| Time-TotalInner         | 56.6         |
| dLoss                   | 0.007481401  |
| n_timesteps             | 146880000    |
------------------------------------------

 ---------------- Iteration 459 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 459            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007678737   |
| LossBefore              | -2.4527018e-09 |
| MeanKL                  | 0.008746512    |
| MeanKLBefore            | 8.939273e-09   |
| Step_0-AverageDiscou... | 138            |
| Step_0-AveragePolicyStd | 0.39468452     |
| Step_0-AverageReturn    | 321            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 669            |
| Step_0-MinReturn        | 7.19           |
| Step_0-NumTrajs         | 1046           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 135            |
| Step_1-AverageDiscou... | 137            |
| Step_1-AveragePolicyStd | 0.39462504     |
| Step_1-AverageReturn    | 310            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 646            |
| Step_1-MinReturn        | 12.4           |
| Step_1-NumTrajs         | 1112           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 137            |
| Time                    | 3.15e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.856          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0076787346   |
| n_timesteps             | 147200000      |
--------------------------------------------

 ---------------- Iteration 460 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 460            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007095431   |
| LossBefore              | -6.5714447e-09 |
| MeanKL                  | 0.00745513     |
| MeanKLBefore            | 1.4893133e-09  |
| Step_0-AverageDiscou... | 127            |
| Step_0-AveragePolicyStd | 0.39442405     |
| Step_0-AverageReturn    | 284            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 625            |
| Step_0-MinReturn        | 13.7           |
| Step_0-NumTrajs         | 1114           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 142            |
| Step_1-AverageDiscou... | 135            |
| Step_1-AveragePolicyStd | 0.3943499      |
| Step_1-AverageReturn    | 304            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 653            |
| Step_1-MinReturn        | 15.4           |
| Step_1-NumTrajs         | 1148           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 145            |
| Time                    | 3.15e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.885          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0070954245   |
| n_timesteps             | 147520000      |
--------------------------------------------

 ---------------- Iteration 461 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 461            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.007577376   |
| LossBefore              | 5.9793166e-09  |
| MeanKL                  | 0.008192653    |
| MeanKLBefore            | -4.4696606e-09 |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39399794     |
| Step_0-AverageReturn    | 324            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 704            |
| Step_0-MinReturn        | 13.7           |
| Step_0-NumTrajs         | 1045           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 133            |
| Step_1-AverageDiscou... | 146            |
| Step_1-AveragePolicyStd | 0.3938802      |
| Step_1-AverageReturn    | 331            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 715            |
| Step_1-MinReturn        | 15             |
| Step_1-NumTrajs         | 1125           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 136            |
| Time                    | 3.16e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.851          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007577382    |
| n_timesteps             | 147840000      |
--------------------------------------------

 ---------------- Iteration 462 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 462            |
| ItrTime                 | 70             |
| LossAfter               | -0.0072997743  |
| LossBefore              | 6.910951e-09   |
| MeanKL                  | 0.007927599    |
| MeanKLBefore            | -1.3408846e-08 |
| Step_0-AverageDiscou... | 138            |
| Step_0-AveragePolicyStd | 0.39395067     |
| Step_0-AverageReturn    | 316            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 662            |
| Step_0-MinReturn        | 7.24           |
| Step_0-NumTrajs         | 1058           |
| Step_0-PolicyExecTime   | 1.28           |
| Step_0-StdReturn        | 129            |
| Step_1-AverageDiscou... | 144            |
| Step_1-AveragePolicyStd | 0.39382127     |
| Step_1-AverageReturn    | 330            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 661            |
| Step_1-MinReturn        | 19.4           |
| Step_1-NumTrajs         | 1092           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 133            |
| Time                    | 3.17e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.858          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0072997813   |
| n_timesteps             | 148160000      |
--------------------------------------------

 ---------------- Iteration 463 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 463           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.006868869  |
| LossBefore              | -8.096822e-09 |
| MeanKL                  | 0.008478929   |
| MeanKLBefore            | 1.9369338e-08 |
| Step_0-AverageDiscou... | 143           |
| Step_0-AveragePolicyStd | 0.39401802    |
| Step_0-AverageReturn    | 323           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 681           |
| Step_0-MinReturn        | 8.44          |
| Step_0-NumTrajs         | 1080          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 123           |
| Step_1-AverageDiscou... | 147           |
| Step_1-AveragePolicyStd | 0.3939334     |
| Step_1-AverageReturn    | 337           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 689           |
| Step_1-MinReturn        | 12.4          |
| Step_1-NumTrajs         | 1104          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 130           |
| Time                    | 3.17e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.006868861   |
| n_timesteps             | 148480000     |
-------------------------------------------

 ---------------- Iteration 464 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 464            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.006600375   |
| LossBefore              | -6.6584698e-09 |
| MeanKL                  | 0.007522776    |
| MeanKLBefore            | 1.1918486e-08  |
| Step_0-AverageDiscou... | 138            |
| Step_0-AveragePolicyStd | 0.39442706     |
| Step_0-AverageReturn    | 315            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 652            |
| Step_0-MinReturn        | 19.3           |
| Step_0-NumTrajs         | 1050           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 125            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.39444897     |
| Step_1-AverageReturn    | 311            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 776            |
| Step_1-MinReturn        | 14.5           |
| Step_1-NumTrajs         | 1111           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 134            |
| Time                    | 3.18e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.849          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0066003683   |
| n_timesteps             | 148800000      |
--------------------------------------------

 ---------------- Iteration 465 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 465           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0069733365 |
| LossBefore              | 7.9364515e-09 |
| MeanKL                  | 0.007959205   |
| MeanKLBefore            | 8.938496e-09  |
| Step_0-AverageDiscou... | 126           |
| Step_0-AveragePolicyStd | 0.39486423    |
| Step_0-AverageReturn    | 285           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 632           |
| Step_0-MinReturn        | 8.98          |
| Step_0-NumTrajs         | 1125          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 129           |
| Step_1-AveragePolicyStd | 0.3947905     |
| Step_1-AverageReturn    | 287           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 766           |
| Step_1-MinReturn        | 9.14          |
| Step_1-NumTrajs         | 1176          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 154           |
| Time                    | 3.19e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.863         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0069733444  |
| n_timesteps             | 149120000     |
-------------------------------------------

 ---------------- Iteration 466 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 466            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.006960201   |
| LossBefore              | 2.847408e-09   |
| MeanKL                  | 0.007936833    |
| MeanKLBefore            | -1.0867751e-12 |
| Step_0-AverageDiscou... | 142            |
| Step_0-AveragePolicyStd | 0.39505357     |
| Step_0-AverageReturn    | 341            |
| Step_0-EnvExecTime      | 24.4           |
| Step_0-MaxReturn        | 640            |
| Step_0-MinReturn        | 9.03           |
| Step_0-NumTrajs         | 977            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 135            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.3949514      |
| Step_1-AverageReturn    | 321            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 693            |
| Step_1-MinReturn        | 23.7           |
| Step_1-NumTrajs         | 1107           |
| Step_1-PolicyExecTime   | 4.02           |
| Step_1-StdReturn        | 140            |
| Time                    | 3.2e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.832          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.006960204    |
| n_timesteps             | 149440000      |
--------------------------------------------

 ---------------- Iteration 467 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 467           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.007146572  |
| LossBefore              | -6.029049e-09 |
| MeanKL                  | 0.008358067   |
| MeanKLBefore            | 7.4494424e-09 |
| Step_0-AverageDiscou... | 139           |
| Step_0-AveragePolicyStd | 0.39595038    |
| Step_0-AverageReturn    | 312           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 667           |
| Step_0-MinReturn        | 14.5          |
| Step_0-NumTrajs         | 1108          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 132           |
| Step_1-AverageDiscou... | 141           |
| Step_1-AveragePolicyStd | 0.39587706    |
| Step_1-AverageReturn    | 321           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 663           |
| Step_1-MinReturn        | 20            |
| Step_1-NumTrajs         | 1133          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 142           |
| Time                    | 3.2e+04       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.007146566   |
| n_timesteps             | 149760000     |
-------------------------------------------

 ---------------- Iteration 468 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 468            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.0074116243  |
| LossBefore              | -2.2468994e-08 |
| MeanKL                  | 0.008036846    |
| MeanKLBefore            | 5.9580976e-09  |
| Step_0-AverageDiscou... | 137            |
| Step_0-AveragePolicyStd | 0.39563042     |
| Step_0-AverageReturn    | 317            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 721            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 1040           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 143            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.39559636     |
| Step_1-AverageReturn    | 321            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 698            |
| Step_1-MinReturn        | 15.4           |
| Step_1-NumTrajs         | 1094           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 134            |
| Time                    | 3.21e+04       |
| Time-InnerStep          | 0.168          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.007411602    |
| n_timesteps             | 150080000      |
--------------------------------------------

 ---------------- Iteration 469 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 469           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0073915916 |
| LossBefore              | 8.399366e-09  |
| MeanKL                  | 0.008414212   |
| MeanKLBefore            | 8.941325e-09  |
| Step_0-AverageDiscou... | 134           |
| Step_0-AveragePolicyStd | 0.39595795    |
| Step_0-AverageReturn    | 307           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 703           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1088          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 141           |
| Step_1-AveragePolicyStd | 0.3959658     |
| Step_1-AverageReturn    | 317           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 744           |
| Step_1-MinReturn        | 17.1          |
| Step_1-NumTrajs         | 1150          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 144           |
| Time                    | 3.22e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0073916     |
| n_timesteps             | 150400000     |
-------------------------------------------

 ---------------- Iteration 470 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 470            |
| ItrTime                 | 73.1           |
| LossAfter               | -0.0070400657  |
| LossBefore              | -8.9026886e-10 |
| MeanKL                  | 0.00762725     |
| MeanKLBefore            | 5.9587015e-09  |
| Step_0-AverageDiscou... | 139            |
| Step_0-AveragePolicyStd | 0.39515293     |
| Step_0-AverageReturn    | 336            |
| Step_0-EnvExecTime      | 24.7           |
| Step_0-MaxReturn        | 638            |
| Step_0-MinReturn        | 7.93           |
| Step_0-NumTrajs         | 978            |
| Step_0-PolicyExecTime   | 1.72           |
| Step_0-StdReturn        | 143            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.3951009      |
| Step_1-AverageReturn    | 314            |
| Step_1-EnvExecTime      | 24.6           |
| Step_1-MaxReturn        | 657            |
| Step_1-MinReturn        | 15.4           |
| Step_1-NumTrajs         | 1109           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 139            |
| Time                    | 3.22e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.853          |
| Time-Sampling           | 58.5           |
| Time-TotalInner         | 59.5           |
| dLoss                   | 0.007040065    |
| n_timesteps             | 150720000      |
--------------------------------------------

 ---------------- Iteration 471 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 471            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0075434423  |
| LossBefore              | -2.9025058e-09 |
| MeanKL                  | 0.0082754735   |
| MeanKLBefore            | -4.467868e-09  |
| Step_0-AverageDiscou... | 132            |
| Step_0-AveragePolicyStd | 0.3950421      |
| Step_0-AverageReturn    | 316            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 665            |
| Step_0-MinReturn        | 2.64           |
| Step_0-NumTrajs         | 1003           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 158            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.3950391      |
| Step_1-AverageReturn    | 329            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 734            |
| Step_1-MinReturn        | 2.06           |
| Step_1-NumTrajs         | 1045           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 159            |
| Time                    | 3.23e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.842          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0075434395   |
| n_timesteps             | 151040000      |
--------------------------------------------

 ---------------- Iteration 472 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 472            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007003069   |
| LossBefore              | -5.0181383e-09 |
| MeanKL                  | 0.00793117     |
| MeanKLBefore            | 5.9579373e-09  |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39561933     |
| Step_0-AverageReturn    | 329            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 670            |
| Step_0-MinReturn        | 17             |
| Step_0-NumTrajs         | 1021           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 141            |
| Step_1-AveragePolicyStd | 0.39564666     |
| Step_1-AverageReturn    | 318            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 717            |
| Step_1-MinReturn        | 21.1           |
| Step_1-NumTrajs         | 1116           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 138            |
| Time                    | 3.24e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.846          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007003064    |
| n_timesteps             | 151360000      |
--------------------------------------------

 ---------------- Iteration 473 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 473            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.00700193    |
| LossBefore              | 1.1399619e-08  |
| MeanKL                  | 0.00758826     |
| MeanKLBefore            | -2.9789085e-09 |
| Step_0-AverageDiscou... | 128            |
| Step_0-AveragePolicyStd | 0.3957789      |
| Step_0-AverageReturn    | 288            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 691            |
| Step_0-MinReturn        | 9.83           |
| Step_0-NumTrajs         | 1121           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 147            |
| Step_1-AverageDiscou... | 132            |
| Step_1-AveragePolicyStd | 0.3957708      |
| Step_1-AverageReturn    | 294            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 772            |
| Step_1-MinReturn        | 17             |
| Step_1-NumTrajs         | 1167           |
| Step_1-PolicyExecTime   | 3.93           |
| Step_1-StdReturn        | 146            |
| Time                    | 3.25e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.872          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007001941    |
| n_timesteps             | 151680000      |
--------------------------------------------

 ---------------- Iteration 474 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 474           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.0072776573 |
| LossBefore              | -9.236148e-10 |
| MeanKL                  | 0.008411505   |
| MeanKLBefore            | 7.4487843e-09 |
| Step_0-AverageDiscou... | 143           |
| Step_0-AveragePolicyStd | 0.3956069     |
| Step_0-AverageReturn    | 335           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 656           |
| Step_0-MinReturn        | 21.2          |
| Step_0-NumTrajs         | 1024          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 139           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39561486    |
| Step_1-AverageReturn    | 328           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 715           |
| Step_1-MinReturn        | 19.1          |
| Step_1-NumTrajs         | 1116          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 141           |
| Time                    | 3.25e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.841         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.0072776563  |
| n_timesteps             | 152000000     |
-------------------------------------------

 ---------------- Iteration 475 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 475            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.007032376   |
| LossBefore              | -9.1227115e-10 |
| MeanKL                  | 0.007286459    |
| MeanKLBefore            | -7.4476447e-09 |
| Step_0-AverageDiscou... | 137            |
| Step_0-AveragePolicyStd | 0.39492032     |
| Step_0-AverageReturn    | 315            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 638            |
| Step_0-MinReturn        | 17             |
| Step_0-NumTrajs         | 1059           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 138            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.39482272     |
| Step_1-AverageReturn    | 318            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 717            |
| Step_1-MinReturn        | 17.7           |
| Step_1-NumTrajs         | 1101           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 140            |
| Time                    | 3.26e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.855          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007032375    |
| n_timesteps             | 152320000      |
--------------------------------------------

 ---------------- Iteration 476 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 476            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0066406564  |
| LossBefore              | -2.9945266e-09 |
| MeanKL                  | 0.008467792    |
| MeanKLBefore            | 7.449349e-09   |
| Step_0-AverageDiscou... | 139            |
| Step_0-AveragePolicyStd | 0.39445904     |
| Step_0-AverageReturn    | 319            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 640            |
| Step_0-MinReturn        | 18.3           |
| Step_0-NumTrajs         | 1062           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 134            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.39446026     |
| Step_1-AverageReturn    | 312            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 691            |
| Step_1-MinReturn        | 7.25           |
| Step_1-NumTrajs         | 1130           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 143            |
| Time                    | 3.27e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.86           |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0066406536   |
| n_timesteps             | 152640000      |
--------------------------------------------

 ---------------- Iteration 477 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 477            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0068260096  |
| LossBefore              | 4.6331876e-09  |
| MeanKL                  | 0.008348929    |
| MeanKLBefore            | -1.6390135e-08 |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39447838     |
| Step_0-AverageReturn    | 315            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 724            |
| Step_0-MinReturn        | 10.6           |
| Step_0-NumTrajs         | 1085           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 135            |
| Step_1-AverageDiscou... | 143            |
| Step_1-AveragePolicyStd | 0.39436218     |
| Step_1-AverageReturn    | 324            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 767            |
| Step_1-MinReturn        | 27             |
| Step_1-NumTrajs         | 1112           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 137            |
| Time                    | 3.27e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.857          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0068260143   |
| n_timesteps             | 152960000      |
--------------------------------------------

 ---------------- Iteration 478 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 478            |
| ItrTime                 | 70             |
| LossAfter               | -0.0069899177  |
| LossBefore              | -6.6427157e-09 |
| MeanKL                  | 0.008187823    |
| MeanKLBefore            | -2.9794565e-09 |
| Step_0-AverageDiscou... | 144            |
| Step_0-AveragePolicyStd | 0.39405796     |
| Step_0-AverageReturn    | 333            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 661            |
| Step_0-MinReturn        | 16.1           |
| Step_0-NumTrajs         | 1041           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 130            |
| Step_1-AverageDiscou... | 143            |
| Step_1-AveragePolicyStd | 0.39398813     |
| Step_1-AverageReturn    | 320            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 749            |
| Step_1-MinReturn        | 14.6           |
| Step_1-NumTrajs         | 1172           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 137            |
| Time                    | 3.28e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.862          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.006989911    |
| n_timesteps             | 153280000      |
--------------------------------------------

 ---------------- Iteration 479 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 479            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.007505079   |
| LossBefore              | -3.4946972e-09 |
| MeanKL                  | 0.008760857    |
| MeanKLBefore            | -5.9610237e-09 |
| Step_0-AverageDiscou... | 143            |
| Step_0-AveragePolicyStd | 0.39424086     |
| Step_0-AverageReturn    | 323            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 644            |
| Step_0-MinReturn        | 26             |
| Step_0-NumTrajs         | 1096           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 131            |
| Step_1-AverageDiscou... | 142            |
| Step_1-AveragePolicyStd | 0.39422166     |
| Step_1-AverageReturn    | 315            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 704            |
| Step_1-MinReturn        | 13             |
| Step_1-NumTrajs         | 1162           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 134            |
| Time                    | 3.29e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.893          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007505075    |
| n_timesteps             | 153600000      |
--------------------------------------------

 ---------------- Iteration 480 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 480           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0070780157 |
| LossBefore              | -1.01994e-08  |
| MeanKL                  | 0.007983366   |
| MeanKLBefore            | 8.9363885e-09 |
| Step_0-AverageDiscou... | 145           |
| Step_0-AveragePolicyStd | 0.39348075    |
| Step_0-AverageReturn    | 341           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 726           |
| Step_0-MinReturn        | 21            |
| Step_0-NumTrajs         | 1002          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 125           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.39342523    |
| Step_1-AverageReturn    | 354           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 763           |
| Step_1-MinReturn        | 25.8          |
| Step_1-NumTrajs         | 1055          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 133           |
| Time                    | 3.3e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.83          |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0070780055  |
| n_timesteps             | 153920000     |
-------------------------------------------

 ---------------- Iteration 481 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 481            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007974207   |
| LossBefore              | -2.4593638e-09 |
| MeanKL                  | 0.009923611    |
| MeanKLBefore            | -8.9388e-09    |
| Step_0-AverageDiscou... | 135            |
| Step_0-AveragePolicyStd | 0.39278463     |
| Step_0-AverageReturn    | 316            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 696            |
| Step_0-MinReturn        | 3.5            |
| Step_0-NumTrajs         | 1023           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.39277133     |
| Step_1-AverageReturn    | 318            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 629            |
| Step_1-MinReturn        | 14.4           |
| Step_1-NumTrajs         | 1099           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 146            |
| Time                    | 3.3e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.2           |
| Time-OuterStep          | 13.2           |
| Time-SampleProc         | 0.842          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007974205    |
| n_timesteps             | 154240000      |
--------------------------------------------

 ---------------- Iteration 482 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 482           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.007042706  |
| LossBefore              | -5.272892e-09 |
| MeanKL                  | 0.007994728   |
| MeanKLBefore            | -8.940036e-09 |
| Step_0-AverageDiscou... | 148           |
| Step_0-AveragePolicyStd | 0.39301223    |
| Step_0-AverageReturn    | 354           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 694           |
| Step_0-MinReturn        | 12.1          |
| Step_0-NumTrajs         | 998           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 132           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39299053    |
| Step_1-AverageReturn    | 324           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 706           |
| Step_1-MinReturn        | 20.5          |
| Step_1-NumTrajs         | 1142          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 137           |
| Time                    | 3.31e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.848         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.007042701   |
| n_timesteps             | 154560000     |
-------------------------------------------

 ---------------- Iteration 483 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 483           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.006793721  |
| LossBefore              | 1.4330181e-08 |
| MeanKL                  | 0.007709749   |
| MeanKLBefore            | 4.469789e-09  |
| Step_0-AverageDiscou... | 137           |
| Step_0-AveragePolicyStd | 0.39388907    |
| Step_0-AverageReturn    | 321           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 620           |
| Step_0-MinReturn        | 14.1          |
| Step_0-NumTrajs         | 1014          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 130           |
| Step_1-AverageDiscou... | 139           |
| Step_1-AveragePolicyStd | 0.39379725    |
| Step_1-AverageReturn    | 316           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 690           |
| Step_1-MinReturn        | 22.1          |
| Step_1-NumTrajs         | 1099          |
| Step_1-PolicyExecTime   | 4.46          |
| Step_1-StdReturn        | 135           |
| Time                    | 3.32e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.845         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0067937355  |
| n_timesteps             | 154880000     |
-------------------------------------------

 ---------------- Iteration 484 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 484            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.007024958   |
| LossBefore              | -1.3217161e-11 |
| MeanKL                  | 0.0076481192   |
| MeanKLBefore            | -2.9792062e-09 |
| Step_0-AverageDiscou... | 143            |
| Step_0-AveragePolicyStd | 0.39363402     |
| Step_0-AverageReturn    | 330            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 709            |
| Step_0-MinReturn        | 25             |
| Step_0-NumTrajs         | 1036           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 127            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.39357004     |
| Step_1-AverageReturn    | 319            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 634            |
| Step_1-MinReturn        | 3.9            |
| Step_1-NumTrajs         | 1092           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 137            |
| Time                    | 3.32e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.835          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007024958    |
| n_timesteps             | 155200000      |
--------------------------------------------

 ---------------- Iteration 485 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 485           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007534863  |
| LossBefore              | 1.0659818e-08 |
| MeanKL                  | 0.008374045   |
| MeanKLBefore            | -8.93906e-09  |
| Step_0-AverageDiscou... | 143           |
| Step_0-AveragePolicyStd | 0.3938933     |
| Step_0-AverageReturn    | 338           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 670           |
| Step_0-MinReturn        | 11.3          |
| Step_0-NumTrajs         | 1027          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 143           |
| Step_1-AverageDiscou... | 140           |
| Step_1-AveragePolicyStd | 0.39387456    |
| Step_1-AverageReturn    | 328           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 727           |
| Step_1-MinReturn        | 11            |
| Step_1-NumTrajs         | 1063          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 147           |
| Time                    | 3.33e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0075348737  |
| n_timesteps             | 155520000     |
-------------------------------------------

 ---------------- Iteration 486 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 486            |
| ItrTime                 | 71             |
| LossAfter               | -0.007156226   |
| LossBefore              | -2.5988953e-10 |
| MeanKL                  | 0.007947618    |
| MeanKLBefore            | 1.19190675e-08 |
| Step_0-AverageDiscou... | 152            |
| Step_0-AveragePolicyStd | 0.3940635      |
| Step_0-AverageReturn    | 369            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 688            |
| Step_0-MinReturn        | 7.05           |
| Step_0-NumTrajs         | 940            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 125            |
| Step_1-AverageDiscou... | 154            |
| Step_1-AveragePolicyStd | 0.394057       |
| Step_1-AverageReturn    | 359            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 839            |
| Step_1-MinReturn        | 3.17           |
| Step_1-NumTrajs         | 1053           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 138            |
| Time                    | 3.34e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.818          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0071562254   |
| n_timesteps             | 155840000      |
--------------------------------------------

 ---------------- Iteration 487 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 487           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.007179885  |
| LossBefore              | 1.3171979e-08 |
| MeanKL                  | 0.008261771   |
| MeanKLBefore            | 7.450945e-09  |
| Step_0-AverageDiscou... | 144           |
| Step_0-AveragePolicyStd | 0.3929985     |
| Step_0-AverageReturn    | 340           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 690           |
| Step_0-MinReturn        | 13.3          |
| Step_0-NumTrajs         | 1008          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 136           |
| Step_1-AverageDiscou... | 148           |
| Step_1-AveragePolicyStd | 0.3929659     |
| Step_1-AverageReturn    | 342           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 679           |
| Step_1-MinReturn        | 27.6          |
| Step_1-NumTrajs         | 1095          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 147           |
| Time                    | 3.34e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.833         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007179898   |
| n_timesteps             | 156160000     |
-------------------------------------------

 ---------------- Iteration 488 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 488           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.007275331  |
| LossBefore              | 2.2644915e-09 |
| MeanKL                  | 0.008769228   |
| MeanKLBefore            | 2.9801939e-09 |
| Step_0-AverageDiscou... | 144           |
| Step_0-AveragePolicyStd | 0.39268717    |
| Step_0-AverageReturn    | 340           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 656           |
| Step_0-MinReturn        | 11.7          |
| Step_0-NumTrajs         | 1018          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 140           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.3926391     |
| Step_1-AverageReturn    | 335           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 720           |
| Step_1-MinReturn        | 13.4          |
| Step_1-NumTrajs         | 1082          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 146           |
| Time                    | 3.35e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.841         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.007275333   |
| n_timesteps             | 156480000     |
-------------------------------------------

 ---------------- Iteration 489 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 489           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0075233625 |
| LossBefore              | 5.357146e-09  |
| MeanKL                  | 0.009102491   |
| MeanKLBefore            | 4.47002e-09   |
| Step_0-AverageDiscou... | 134           |
| Step_0-AveragePolicyStd | 0.3930928     |
| Step_0-AverageReturn    | 316           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 713           |
| Step_0-MinReturn        | 5.11          |
| Step_0-NumTrajs         | 1028          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 144           |
| Step_1-AverageDiscou... | 141           |
| Step_1-AveragePolicyStd | 0.39302745    |
| Step_1-AverageReturn    | 322           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 721           |
| Step_1-MinReturn        | 14.6          |
| Step_1-NumTrajs         | 1104          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 147           |
| Time                    | 3.36e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.846         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007523368   |
| n_timesteps             | 156800000     |
-------------------------------------------

 ---------------- Iteration 490 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 490            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.006686718   |
| LossBefore              | 3.2227536e-09  |
| MeanKL                  | 0.007916634    |
| MeanKLBefore            | -4.4703947e-09 |
| Step_0-AverageDiscou... | 147            |
| Step_0-AveragePolicyStd | 0.39311296     |
| Step_0-AverageReturn    | 348            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 658            |
| Step_0-MinReturn        | 17.6           |
| Step_0-NumTrajs         | 985            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 126            |
| Step_1-AverageDiscou... | 150            |
| Step_1-AveragePolicyStd | 0.3929798      |
| Step_1-AverageReturn    | 355            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 750            |
| Step_1-MinReturn        | 23.8           |
| Step_1-NumTrajs         | 1026           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 136            |
| Time                    | 3.37e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.819          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0066867215   |
| n_timesteps             | 157120000      |
--------------------------------------------

 ---------------- Iteration 491 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 491           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0068870187 |
| LossBefore              | 6.34829e-09   |
| MeanKL                  | 0.008373973   |
| MeanKLBefore            | 1.4905168e-09 |
| Step_0-AverageDiscou... | 150           |
| Step_0-AveragePolicyStd | 0.39292014    |
| Step_0-AverageReturn    | 356           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 657           |
| Step_0-MinReturn        | 20.2          |
| Step_0-NumTrajs         | 987           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 122           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.39286742    |
| Step_1-AverageReturn    | 354           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 766           |
| Step_1-MinReturn        | 3.4           |
| Step_1-NumTrajs         | 1079          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 144           |
| Time                    | 3.37e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.833         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.006887025   |
| n_timesteps             | 157440000     |
-------------------------------------------

 ---------------- Iteration 492 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 492           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0065716617 |
| LossBefore              | 3.5144487e-09 |
| MeanKL                  | 0.0077577233  |
| MeanKLBefore            | 1.3408419e-08 |
| Step_0-AverageDiscou... | 132           |
| Step_0-AveragePolicyStd | 0.3934004     |
| Step_0-AverageReturn    | 308           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 634           |
| Step_0-MinReturn        | 12            |
| Step_0-NumTrajs         | 1049          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 139           |
| Step_1-AveragePolicyStd | 0.39332303    |
| Step_1-AverageReturn    | 317           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 732           |
| Step_1-MinReturn        | 5.16          |
| Step_1-NumTrajs         | 1110          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 157           |
| Time                    | 3.38e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.0065716654  |
| n_timesteps             | 157760000     |
-------------------------------------------

 ---------------- Iteration 493 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 493           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007586246  |
| LossBefore              | 5.31726e-09   |
| MeanKL                  | 0.008539023   |
| MeanKLBefore            | 1.4899726e-08 |
| Step_0-AverageDiscou... | 139           |
| Step_0-AveragePolicyStd | 0.3937825     |
| Step_0-AverageReturn    | 324           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 700           |
| Step_0-MinReturn        | 21.1          |
| Step_0-NumTrajs         | 1024          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 137           |
| Step_1-AverageDiscou... | 137           |
| Step_1-AveragePolicyStd | 0.393759      |
| Step_1-AverageReturn    | 307           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 713           |
| Step_1-MinReturn        | 26.5          |
| Step_1-NumTrajs         | 1127          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 135           |
| Time                    | 3.39e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.848         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.007586251   |
| n_timesteps             | 158080000     |
-------------------------------------------

 ---------------- Iteration 494 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 494            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0073757083  |
| LossBefore              | 9.877084e-10   |
| MeanKL                  | 0.008259645    |
| MeanKLBefore            | -1.4899205e-08 |
| Step_0-AverageDiscou... | 149            |
| Step_0-AveragePolicyStd | 0.39416242     |
| Step_0-AverageReturn    | 358            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 723            |
| Step_0-MinReturn        | 16.4           |
| Step_0-NumTrajs         | 991            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 147            |
| Step_1-AverageDiscou... | 150            |
| Step_1-AveragePolicyStd | 0.39413264     |
| Step_1-AverageReturn    | 350            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 840            |
| Step_1-MinReturn        | 5.89           |
| Step_1-NumTrajs         | 1071           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 143            |
| Time                    | 3.39e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.818          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0073757092   |
| n_timesteps             | 158400000      |
--------------------------------------------

 ---------------- Iteration 495 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 495            |
| ItrTime                 | 71             |
| LossAfter               | -0.0074057416  |
| LossBefore              | -3.0829722e-10 |
| MeanKL                  | 0.0071287947   |
| MeanKLBefore            | 1.487922e-09   |
| Step_0-AverageDiscou... | 149            |
| Step_0-AveragePolicyStd | 0.39493677     |
| Step_0-AverageReturn    | 368            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 679            |
| Step_0-MinReturn        | 9.17           |
| Step_0-NumTrajs         | 936            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 121            |
| Step_1-AverageDiscou... | 147            |
| Step_1-AveragePolicyStd | 0.39484864     |
| Step_1-AverageReturn    | 345            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 696            |
| Step_1-MinReturn        | 9.61           |
| Step_1-NumTrajs         | 1075           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 136            |
| Time                    | 3.4e+04        |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.812          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.007405741    |
| n_timesteps             | 158720000      |
--------------------------------------------

 ---------------- Iteration 496 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 496             |
| ItrTime                 | 70.5            |
| LossAfter               | -0.006954418    |
| LossBefore              | -1.0715212e-09  |
| MeanKL                  | 0.008162787     |
| MeanKLBefore            | -1.19216965e-08 |
| Step_0-AverageDiscou... | 147             |
| Step_0-AveragePolicyStd | 0.39434266      |
| Step_0-AverageReturn    | 348             |
| Step_0-EnvExecTime      | 23.9            |
| Step_0-MaxReturn        | 687             |
| Step_0-MinReturn        | 9.86            |
| Step_0-NumTrajs         | 1008            |
| Step_0-PolicyExecTime   | 1.34            |
| Step_0-StdReturn        | 130             |
| Step_1-AverageDiscou... | 148             |
| Step_1-AveragePolicyStd | 0.39424416      |
| Step_1-AverageReturn    | 338             |
| Step_1-EnvExecTime      | 23.3            |
| Step_1-MaxReturn        | 708             |
| Step_1-MinReturn        | 16.5            |
| Step_1-NumTrajs         | 1094            |
| Step_1-PolicyExecTime   | 4.01            |
| Step_1-StdReturn        | 130             |
| Time                    | 3.41e+04        |
| Time-InnerStep          | 0.164           |
| Time-MAMLSteps          | 13.6            |
| Time-OuterStep          | 13.6            |
| Time-SampleProc         | 0.831           |
| Time-Sampling           | 55.8            |
| Time-TotalInner         | 56.8            |
| dLoss                   | 0.006954417     |
| n_timesteps             | 159040000       |
---------------------------------------------

 ---------------- Iteration 497 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 497           |
| ItrTime                 | 72.5          |
| LossAfter               | -0.0066936896 |
| LossBefore              | 1.5614539e-09 |
| MeanKL                  | 0.0073680035  |
| MeanKLBefore            | 3.7267967e-13 |
| Step_0-AverageDiscou... | 141           |
| Step_0-AveragePolicyStd | 0.39378747    |
| Step_0-AverageReturn    | 331           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 688           |
| Step_0-MinReturn        | 18.9          |
| Step_0-NumTrajs         | 1032          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 141           |
| Step_1-AverageDiscou... | 143           |
| Step_1-AveragePolicyStd | 0.393859      |
| Step_1-AverageReturn    | 331           |
| Step_1-EnvExecTime      | 24.4          |
| Step_1-MaxReturn        | 706           |
| Step_1-MinReturn        | 18.8          |
| Step_1-NumTrajs         | 1086          |
| Step_1-PolicyExecTime   | 4.52          |
| Step_1-StdReturn        | 149           |
| Time                    | 3.42e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 57.8          |
| Time-TotalInner         | 58.9          |
| dLoss                   | 0.006693691   |
| n_timesteps             | 159360000     |
-------------------------------------------

 ---------------- Iteration 498 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 498            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0070648603  |
| LossBefore              | -6.4620296e-09 |
| MeanKL                  | 0.008298099    |
| MeanKLBefore            | 4.4712403e-09  |
| Step_0-AverageDiscou... | 138            |
| Step_0-AveragePolicyStd | 0.3942977      |
| Step_0-AverageReturn    | 331            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 696            |
| Step_0-MinReturn        | 10.9           |
| Step_0-NumTrajs         | 1006           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 148            |
| Step_1-AverageDiscou... | 146            |
| Step_1-AveragePolicyStd | 0.39424807     |
| Step_1-AverageReturn    | 349            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 755            |
| Step_1-MinReturn        | 12.7           |
| Step_1-NumTrajs         | 1036           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 148            |
| Time                    | 3.42e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.833          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.007064854    |
| n_timesteps             | 159680000      |
--------------------------------------------

 ---------------- Iteration 499 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 499            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.007148127   |
| LossBefore              | 6.791887e-09   |
| MeanKL                  | 0.007497195    |
| MeanKLBefore            | -1.4893402e-09 |
| Step_0-AverageDiscou... | 150            |
| Step_0-AveragePolicyStd | 0.39404064     |
| Step_0-AverageReturn    | 362            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 686            |
| Step_0-MinReturn        | 21             |
| Step_0-NumTrajs         | 967            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 126            |
| Step_1-AverageDiscou... | 153            |
| Step_1-AveragePolicyStd | 0.39395463     |
| Step_1-AverageReturn    | 357            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 715            |
| Step_1-MinReturn        | 13.5           |
| Step_1-NumTrajs         | 1071           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 129            |
| Time                    | 3.43e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.818          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.007148134    |
| n_timesteps             | 160000000      |
--------------------------------------------

 ---------------- Iteration 500 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 500            |
| ItrTime                 | 72.2           |
| LossAfter               | -0.0069042617  |
| LossBefore              | -1.7848748e-09 |
| MeanKL                  | 0.0077374587   |
| MeanKLBefore            | -4.4695163e-09 |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39433292     |
| Step_0-AverageReturn    | 335            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 643            |
| Step_0-MinReturn        | 8.84           |
| Step_0-NumTrajs         | 997            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 136            |
| Step_1-AverageDiscou... | 143            |
| Step_1-AveragePolicyStd | 0.39416596     |
| Step_1-AverageReturn    | 340            |
| Step_1-EnvExecTime      | 24.5           |
| Step_1-MaxReturn        | 695            |
| Step_1-MinReturn        | 12.5           |
| Step_1-NumTrajs         | 1037           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 134            |
| Time                    | 3.44e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.813          |
| Time-Sampling           | 57.6           |
| Time-TotalInner         | 58.6           |
| dLoss                   | 0.00690426     |
| n_timesteps             | 160320000      |
--------------------------------------------

 ---------------- Iteration 501 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 501            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007149966   |
| LossBefore              | -5.9211507e-09 |
| MeanKL                  | 0.007658113    |
| MeanKLBefore            | 1.4903927e-09  |
| Step_0-AverageDiscou... | 145            |
| Step_0-AveragePolicyStd | 0.39442006     |
| Step_0-AverageReturn    | 353            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 623            |
| Step_0-MinReturn        | 15.2           |
| Step_0-NumTrajs         | 983            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 134            |
| Step_1-AverageDiscou... | 146            |
| Step_1-AveragePolicyStd | 0.39434475     |
| Step_1-AverageReturn    | 338            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 699            |
| Step_1-MinReturn        | 11.9           |
| Step_1-NumTrajs         | 1105           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 144            |
| Time                    | 3.44e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.832          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.00714996     |
| n_timesteps             | 160640000      |
--------------------------------------------

 ---------------- Iteration 502 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 502            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.0067461566  |
| LossBefore              | -1.2313073e-09 |
| MeanKL                  | 0.007835041    |
| MeanKLBefore            | 4.4691566e-09  |
| Step_0-AverageDiscou... | 141            |
| Step_0-AveragePolicyStd | 0.3952167      |
| Step_0-AverageReturn    | 339            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 667            |
| Step_0-MinReturn        | 15.3           |
| Step_0-NumTrajs         | 998            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 144            |
| Step_1-AveragePolicyStd | 0.39509782     |
| Step_1-AverageReturn    | 337            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 701            |
| Step_1-MinReturn        | 17.6           |
| Step_1-NumTrajs         | 1064           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 140            |
| Time                    | 3.45e+04       |
| Time-InnerStep          | 0.173          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.844          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.006746155    |
| n_timesteps             | 160960000      |
--------------------------------------------

 ---------------- Iteration 503 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 503           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0077046617 |
| LossBefore              | 3.0125288e-10 |
| MeanKL                  | 0.009316713   |
| MeanKLBefore            | 4.468265e-09  |
| Step_0-AverageDiscou... | 150           |
| Step_0-AveragePolicyStd | 0.39525935    |
| Step_0-AverageReturn    | 364           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 693           |
| Step_0-MinReturn        | 17.9          |
| Step_0-NumTrajs         | 961           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 132           |
| Step_1-AverageDiscou... | 150           |
| Step_1-AveragePolicyStd | 0.39515254    |
| Step_1-AverageReturn    | 358           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 691           |
| Step_1-MinReturn        | 20.6          |
| Step_1-NumTrajs         | 1013          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 146           |
| Time                    | 3.46e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.829         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007704662   |
| n_timesteps             | 161280000     |
-------------------------------------------

 ---------------- Iteration 504 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 504           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.006822016  |
| LossBefore              | 1.1570515e-08 |
| MeanKL                  | 0.007714371   |
| MeanKLBefore            | 1.1918859e-08 |
| Step_0-AverageDiscou... | 147           |
| Step_0-AveragePolicyStd | 0.3946502     |
| Step_0-AverageReturn    | 349           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 669           |
| Step_0-MinReturn        | 22.7          |
| Step_0-NumTrajs         | 1011          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 133           |
| Step_1-AverageDiscou... | 147           |
| Step_1-AveragePolicyStd | 0.39462283    |
| Step_1-AverageReturn    | 339           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 726           |
| Step_1-MinReturn        | 16.6          |
| Step_1-NumTrajs         | 1099          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 136           |
| Time                    | 3.47e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.854         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58            |
| dLoss                   | 0.0068220277  |
| n_timesteps             | 161600000     |
-------------------------------------------

 ---------------- Iteration 505 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 505           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0075486274 |
| LossBefore              | -5.60591e-10  |
| MeanKL                  | 0.008200215   |
| MeanKLBefore            | 2.2349155e-08 |
| Step_0-AverageDiscou... | 140           |
| Step_0-AveragePolicyStd | 0.39422873    |
| Step_0-AverageReturn    | 334           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 676           |
| Step_0-MinReturn        | 18.5          |
| Step_0-NumTrajs         | 983           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 141           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.3942496     |
| Step_1-AverageReturn    | 362           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 704           |
| Step_1-MinReturn        | 16.8          |
| Step_1-NumTrajs         | 1014          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 138           |
| Time                    | 3.47e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.829         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007548627   |
| n_timesteps             | 161920000     |
-------------------------------------------

 ---------------- Iteration 506 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 506            |
| ItrTime                 | 70             |
| LossAfter               | -0.0071493103  |
| LossBefore              | -6.3931167e-09 |
| MeanKL                  | 0.0072801514   |
| MeanKLBefore            | 2.0857737e-08  |
| Step_0-AverageDiscou... | 139            |
| Step_0-AveragePolicyStd | 0.39401037     |
| Step_0-AverageReturn    | 326            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 665            |
| Step_0-MinReturn        | 14.4           |
| Step_0-NumTrajs         | 1081           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 154            |
| Step_1-AverageDiscou... | 138            |
| Step_1-AveragePolicyStd | 0.39394763     |
| Step_1-AverageReturn    | 310            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 675            |
| Step_1-MinReturn        | 12.7           |
| Step_1-NumTrajs         | 1185           |
| Step_1-PolicyExecTime   | 4.01           |
| Step_1-StdReturn        | 148            |
| Time                    | 3.48e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.862          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007149304    |
| n_timesteps             | 162240000      |
--------------------------------------------

 ---------------- Iteration 507 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 507           |
| ItrTime                 | 71            |
| LossAfter               | -0.0069791563 |
| LossBefore              | 3.9637853e-09 |
| MeanKL                  | 0.007970007   |
| MeanKLBefore            | 2.9794365e-09 |
| Step_0-AverageDiscou... | 140           |
| Step_0-AveragePolicyStd | 0.39391497    |
| Step_0-AverageReturn    | 331           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 723           |
| Step_0-MinReturn        | 11.5          |
| Step_0-NumTrajs         | 1021          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 144           |
| Step_1-AverageDiscou... | 139           |
| Step_1-AveragePolicyStd | 0.39386016    |
| Step_1-AverageReturn    | 325           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 709           |
| Step_1-MinReturn        | 12.6          |
| Step_1-NumTrajs         | 1095          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 154           |
| Time                    | 3.49e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.831         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0069791605  |
| n_timesteps             | 162560000     |
-------------------------------------------

 ---------------- Iteration 508 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 508           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.007278204  |
| LossBefore              | -5.801056e-10 |
| MeanKL                  | 0.007912278   |
| MeanKLBefore            | 1.4897495e-08 |
| Step_0-AverageDiscou... | 140           |
| Step_0-AveragePolicyStd | 0.39348078    |
| Step_0-AverageReturn    | 330           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 738           |
| Step_0-MinReturn        | 7.46          |
| Step_0-NumTrajs         | 1063          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 145           |
| Step_1-AveragePolicyStd | 0.3933454     |
| Step_1-AverageReturn    | 340           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 708           |
| Step_1-MinReturn        | 12.3          |
| Step_1-NumTrajs         | 1078          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 141           |
| Time                    | 3.49e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.845         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.0072782035  |
| n_timesteps             | 162880000     |
-------------------------------------------

 ---------------- Iteration 509 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 509           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0070786723 |
| LossBefore              | -8.183335e-09 |
| MeanKL                  | 0.008301024   |
| MeanKLBefore            | -8.940953e-09 |
| Step_0-AverageDiscou... | 147           |
| Step_0-AveragePolicyStd | 0.39298788    |
| Step_0-AverageReturn    | 342           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 730           |
| Step_0-MinReturn        | 29.2          |
| Step_0-NumTrajs         | 1060          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 137           |
| Step_1-AverageDiscou... | 151           |
| Step_1-AveragePolicyStd | 0.392848      |
| Step_1-AverageReturn    | 360           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 719           |
| Step_1-MinReturn        | 26            |
| Step_1-NumTrajs         | 1030          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 138           |
| Time                    | 3.5e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.826         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007078664   |
| n_timesteps             | 163200000     |
-------------------------------------------

 ---------------- Iteration 510 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 510            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.0072378204  |
| LossBefore              | -3.977886e-09  |
| MeanKL                  | 0.008453066    |
| MeanKLBefore            | -4.4691637e-09 |
| Step_0-AverageDiscou... | 140            |
| Step_0-AveragePolicyStd | 0.39398426     |
| Step_0-AverageReturn    | 323            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 764            |
| Step_0-MinReturn        | 6.05           |
| Step_0-NumTrajs         | 1101           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 155            |
| Step_1-AverageDiscou... | 139            |
| Step_1-AveragePolicyStd | 0.39394307     |
| Step_1-AverageReturn    | 317            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 760            |
| Step_1-MinReturn        | 8.19           |
| Step_1-NumTrajs         | 1151           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 161            |
| Time                    | 3.51e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.871          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.007237816    |
| n_timesteps             | 163520000      |
--------------------------------------------

 ---------------- Iteration 511 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 511            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.007002002   |
| LossBefore              | 1.5283362e-09  |
| MeanKL                  | 0.007437351    |
| MeanKLBefore            | -1.4890305e-09 |
| Step_0-AverageDiscou... | 143            |
| Step_0-AveragePolicyStd | 0.3941297      |
| Step_0-AverageReturn    | 334            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 672            |
| Step_0-MinReturn        | 27.7           |
| Step_0-NumTrajs         | 1044           |
| Step_0-PolicyExecTime   | 1.69           |
| Step_0-StdReturn        | 128            |
| Step_1-AverageDiscou... | 148            |
| Step_1-AveragePolicyStd | 0.39410642     |
| Step_1-AverageReturn    | 343            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 701            |
| Step_1-MinReturn        | 28.7           |
| Step_1-NumTrajs         | 1102           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 143            |
| Time                    | 3.52e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.834          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58             |
| dLoss                   | 0.0070020035   |
| n_timesteps             | 163840000      |
--------------------------------------------

 ---------------- Iteration 512 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 512            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0072156796  |
| LossBefore              | -4.2097725e-09 |
| MeanKL                  | 0.008202612    |
| MeanKLBefore            | 4.4687374e-09  |
| Step_0-AverageDiscou... | 151            |
| Step_0-AveragePolicyStd | 0.39363086     |
| Step_0-AverageReturn    | 346            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 676            |
| Step_0-MinReturn        | 23.7           |
| Step_0-NumTrajs         | 1066           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 125            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.39352772     |
| Step_1-AverageReturn    | 354            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 707            |
| Step_1-MinReturn        | 19.8           |
| Step_1-NumTrajs         | 1059           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 133            |
| Time                    | 3.52e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.828          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0072156754   |
| n_timesteps             | 164160000      |
--------------------------------------------

 ---------------- Iteration 513 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 513            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.0068368046  |
| LossBefore              | -3.7961457e-11 |
| MeanKL                  | 0.00741521     |
| MeanKLBefore            | -1.3409621e-08 |
| Step_0-AverageDiscou... | 160            |
| Step_0-AveragePolicyStd | 0.3941955      |
| Step_0-AverageReturn    | 391            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 751            |
| Step_0-MinReturn        | 28.4           |
| Step_0-NumTrajs         | 952            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 123            |
| Step_1-AverageDiscou... | 161            |
| Step_1-AveragePolicyStd | 0.3942024      |
| Step_1-AverageReturn    | 390            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 758            |
| Step_1-MinReturn        | 22.3           |
| Step_1-NumTrajs         | 1016           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 140            |
| Time                    | 3.53e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.817          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58             |
| dLoss                   | 0.0068368046   |
| n_timesteps             | 164480000      |
--------------------------------------------

 ---------------- Iteration 514 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 514           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.007049516  |
| LossBefore              | -1.060048e-09 |
| MeanKL                  | 0.007978597   |
| MeanKLBefore            | -7.448685e-09 |
| Step_0-AverageDiscou... | 152           |
| Step_0-AveragePolicyStd | 0.3937784     |
| Step_0-AverageReturn    | 357           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 818           |
| Step_0-MinReturn        | 35.1          |
| Step_0-NumTrajs         | 1052          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 143           |
| Step_1-AverageDiscou... | 149           |
| Step_1-AveragePolicyStd | 0.3936661     |
| Step_1-AverageReturn    | 345           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 729           |
| Step_1-MinReturn        | 38.6          |
| Step_1-NumTrajs         | 1080          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 140           |
| Time                    | 3.54e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.849         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.007049515   |
| n_timesteps             | 164800000     |
-------------------------------------------

 ---------------- Iteration 515 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 515            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.006984611   |
| LossBefore              | 1.0283003e-08  |
| MeanKL                  | 0.0081894575   |
| MeanKLBefore            | -5.9582455e-09 |
| Step_0-AverageDiscou... | 142            |
| Step_0-AveragePolicyStd | 0.39370006     |
| Step_0-AverageReturn    | 329            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 688            |
| Step_0-MinReturn        | 19             |
| Step_0-NumTrajs         | 1087           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 147            |
| Step_1-AverageDiscou... | 144            |
| Step_1-AveragePolicyStd | 0.3937001      |
| Step_1-AverageReturn    | 336            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 655            |
| Step_1-MinReturn        | 23.4           |
| Step_1-NumTrajs         | 1085           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 142            |
| Time                    | 3.54e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0069846213   |
| n_timesteps             | 165120000      |
--------------------------------------------

 ---------------- Iteration 516 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 516            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0072594946  |
| LossBefore              | -1.0190057e-09 |
| MeanKL                  | 0.007973636    |
| MeanKLBefore            | 4.468948e-09   |
| Step_0-AverageDiscou... | 150            |
| Step_0-AveragePolicyStd | 0.39439568     |
| Step_0-AverageReturn    | 356            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 649            |
| Step_0-MinReturn        | 6.82           |
| Step_0-NumTrajs         | 1022           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 140            |
| Step_1-AverageDiscou... | 148            |
| Step_1-AveragePolicyStd | 0.39433354     |
| Step_1-AverageReturn    | 341            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 746            |
| Step_1-MinReturn        | 15.1           |
| Step_1-NumTrajs         | 1108           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 145            |
| Time                    | 3.55e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.826          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0072594937   |
| n_timesteps             | 165440000      |
--------------------------------------------

 ---------------- Iteration 517 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 517            |
| ItrTime                 | 72.1           |
| LossAfter               | -0.0067247464  |
| LossBefore              | -3.0013054e-09 |
| MeanKL                  | 0.0068434523   |
| MeanKLBefore            | 5.960856e-09   |
| Step_0-AverageDiscou... | 144            |
| Step_0-AveragePolicyStd | 0.3940561      |
| Step_0-AverageReturn    | 349            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 643            |
| Step_0-MinReturn        | 11.7           |
| Step_0-NumTrajs         | 1004           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 148            |
| Step_1-AverageDiscou... | 154            |
| Step_1-AveragePolicyStd | 0.39404967     |
| Step_1-AverageReturn    | 370            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 700            |
| Step_1-MinReturn        | 13.6           |
| Step_1-NumTrajs         | 1015           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 138            |
| Time                    | 3.56e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.828          |
| Time-Sampling           | 57.5           |
| Time-TotalInner         | 58.6           |
| dLoss                   | 0.0067247436   |
| n_timesteps             | 165760000      |
--------------------------------------------

 ---------------- Iteration 518 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 518           |
| ItrTime                 | 72.3          |
| LossAfter               | -0.0073582144 |
| LossBefore              | 2.903518e-09  |
| MeanKL                  | 0.008465666   |
| MeanKLBefore            | 1.7877955e-08 |
| Step_0-AverageDiscou... | 142           |
| Step_0-AveragePolicyStd | 0.39396974    |
| Step_0-AverageReturn    | 336           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 699           |
| Step_0-MinReturn        | 25.1          |
| Step_0-NumTrajs         | 1062          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39388162    |
| Step_1-AverageReturn    | 340           |
| Step_1-EnvExecTime      | 24.4          |
| Step_1-MaxReturn        | 761           |
| Step_1-MinReturn        | 17.8          |
| Step_1-NumTrajs         | 1088          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 155           |
| Time                    | 3.56e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 57.5          |
| Time-TotalInner         | 58.6          |
| dLoss                   | 0.007358217   |
| n_timesteps             | 166080000     |
-------------------------------------------

 ---------------- Iteration 519 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 519            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.0066052214  |
| LossBefore              | -5.9478094e-09 |
| MeanKL                  | 0.0076527223   |
| MeanKLBefore            | 1.1918109e-08  |
| Step_0-AverageDiscou... | 148            |
| Step_0-AveragePolicyStd | 0.39375728     |
| Step_0-AverageReturn    | 355            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 660            |
| Step_0-MinReturn        | 14.6           |
| Step_0-NumTrajs         | 1012           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 148            |
| Step_1-AverageDiscou... | 156            |
| Step_1-AveragePolicyStd | 0.3937097      |
| Step_1-AverageReturn    | 374            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 724            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1038           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 144            |
| Time                    | 3.57e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0066052154   |
| n_timesteps             | 166400000      |
--------------------------------------------

 ---------------- Iteration 520 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 520           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.0068393378 |
| LossBefore              | -9.863562e-09 |
| MeanKL                  | 0.007301563   |
| MeanKLBefore            | 7.449445e-09  |
| Step_0-AverageDiscou... | 147           |
| Step_0-AveragePolicyStd | 0.39301342    |
| Step_0-AverageReturn    | 346           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 708           |
| Step_0-MinReturn        | 15.1          |
| Step_0-NumTrajs         | 1064          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 151           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.3930421     |
| Step_1-AverageReturn    | 327           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 735           |
| Step_1-MinReturn        | 16.8          |
| Step_1-NumTrajs         | 1165          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 161           |
| Time                    | 3.58e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.855         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.006839328   |
| n_timesteps             | 166720000     |
-------------------------------------------

 ---------------- Iteration 521 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 521           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0063748644 |
| LossBefore              | 2.7716637e-10 |
| MeanKL                  | 0.0075674974  |
| MeanKLBefore            | 2.976605e-09  |
| Step_0-AverageDiscou... | 150           |
| Step_0-AveragePolicyStd | 0.39318958    |
| Step_0-AverageReturn    | 356           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 666           |
| Step_0-MinReturn        | 27.1          |
| Step_0-NumTrajs         | 1021          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 139           |
| Step_1-AverageDiscou... | 147           |
| Step_1-AveragePolicyStd | 0.3930189     |
| Step_1-AverageReturn    | 335           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 713           |
| Step_1-MinReturn        | 26.1          |
| Step_1-NumTrajs         | 1128          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 144           |
| Time                    | 3.59e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.855         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.006374865   |
| n_timesteps             | 167040000     |
-------------------------------------------

 ---------------- Iteration 522 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 522           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0075460114 |
| LossBefore              | 1.1378146e-08 |
| MeanKL                  | 0.008798607   |
| MeanKLBefore            | 8.937652e-09  |
| Step_0-AverageDiscou... | 151           |
| Step_0-AveragePolicyStd | 0.3929639     |
| Step_0-AverageReturn    | 357           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 691           |
| Step_0-MinReturn        | 1.37          |
| Step_0-NumTrajs         | 1030          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 136           |
| Step_1-AverageDiscou... | 150           |
| Step_1-AveragePolicyStd | 0.39290145    |
| Step_1-AverageReturn    | 343           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 708           |
| Step_1-MinReturn        | 19.7          |
| Step_1-NumTrajs         | 1117          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 145           |
| Time                    | 3.59e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.867         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0075460225  |
| n_timesteps             | 167360000     |
-------------------------------------------

 ---------------- Iteration 523 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 523           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0069051525 |
| LossBefore              | 8.6575245e-09 |
| MeanKL                  | 0.00779427    |
| MeanKLBefore            | 5.961193e-09  |
| Step_0-AverageDiscou... | 158           |
| Step_0-AveragePolicyStd | 0.39343897    |
| Step_0-AverageReturn    | 389           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 652           |
| Step_0-MinReturn        | 28.4          |
| Step_0-NumTrajs         | 952           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 124           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.39337632    |
| Step_1-AverageReturn    | 360           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 806           |
| Step_1-MinReturn        | 18.7          |
| Step_1-NumTrajs         | 1064          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 147           |
| Time                    | 3.6e+04       |
| Time-InnerStep          | 0.158         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.909         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.0069051613  |
| n_timesteps             | 167680000     |
-------------------------------------------

 ---------------- Iteration 524 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 524            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.00698568    |
| LossBefore              | 9.975598e-09   |
| MeanKL                  | 0.007453391    |
| MeanKLBefore            | -1.0430162e-08 |
| Step_0-AverageDiscou... | 150            |
| Step_0-AveragePolicyStd | 0.3940133      |
| Step_0-AverageReturn    | 362            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 747            |
| Step_0-MinReturn        | 6.38           |
| Step_0-NumTrajs         | 1004           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 150            |
| Step_1-AveragePolicyStd | 0.39389768     |
| Step_1-AverageReturn    | 349            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 747            |
| Step_1-MinReturn        | 13             |
| Step_1-NumTrajs         | 1095           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 154            |
| Time                    | 3.61e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.827          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.00698569     |
| n_timesteps             | 168000000      |
--------------------------------------------

 ---------------- Iteration 525 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 525           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0070344745 |
| LossBefore              | -1.975362e-09 |
| MeanKL                  | 0.007585828   |
| MeanKLBefore            | -5.962076e-09 |
| Step_0-AverageDiscou... | 148           |
| Step_0-AveragePolicyStd | 0.39423317    |
| Step_0-AverageReturn    | 355           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 767           |
| Step_0-MinReturn        | 12.6          |
| Step_0-NumTrajs         | 1013          |
| Step_0-PolicyExecTime   | 1.72          |
| Step_0-StdReturn        | 152           |
| Step_1-AverageDiscou... | 148           |
| Step_1-AveragePolicyStd | 0.39413026    |
| Step_1-AverageReturn    | 345           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 717           |
| Step_1-MinReturn        | 13.8          |
| Step_1-NumTrajs         | 1087          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 149           |
| Time                    | 3.61e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.819         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0070344727  |
| n_timesteps             | 168320000     |
-------------------------------------------

 ---------------- Iteration 526 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 526            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.007034165   |
| LossBefore              | 3.742438e-09   |
| MeanKL                  | 0.007000225    |
| MeanKLBefore            | -1.3409114e-08 |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3942245      |
| Step_0-AverageReturn    | 388            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 699            |
| Step_0-MinReturn        | 18.6           |
| Step_0-NumTrajs         | 964            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 127            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.39412138     |
| Step_1-AverageReturn    | 379            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 728            |
| Step_1-MinReturn        | 25.7           |
| Step_1-NumTrajs         | 1013           |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 139            |
| Time                    | 3.62e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.815          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.0070341686   |
| n_timesteps             | 168640000      |
--------------------------------------------

 ---------------- Iteration 527 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 527            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007328094   |
| LossBefore              | -3.6739922e-09 |
| MeanKL                  | 0.0081904065   |
| MeanKLBefore            | -7.451261e-09  |
| Step_0-AverageDiscou... | 158            |
| Step_0-AveragePolicyStd | 0.39467824     |
| Step_0-AverageReturn    | 383            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 718            |
| Step_0-MinReturn        | 24.6           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 134            |
| Step_1-AverageDiscou... | 160            |
| Step_1-AveragePolicyStd | 0.3946297      |
| Step_1-AverageReturn    | 383            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 752            |
| Step_1-MinReturn        | 39.8           |
| Step_1-NumTrajs         | 1034           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 142            |
| Time                    | 3.63e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.815          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0073280903   |
| n_timesteps             | 168960000      |
--------------------------------------------

 ---------------- Iteration 528 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 528           |
| ItrTime                 | 71.7          |
| LossAfter               | -0.006833625  |
| LossBefore              | -7.186509e-10 |
| MeanKL                  | 0.0075654658  |
| MeanKLBefore            | 3.7249134e-08 |
| Step_0-AverageDiscou... | 143           |
| Step_0-AveragePolicyStd | 0.3943235     |
| Step_0-AverageReturn    | 337           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 674           |
| Step_0-MinReturn        | 17.6          |
| Step_0-NumTrajs         | 1066          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 158           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39425483    |
| Step_1-AverageReturn    | 333           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 763           |
| Step_1-MinReturn        | 0.363         |
| Step_1-NumTrajs         | 1116          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 161           |
| Time                    | 3.64e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.85          |
| Time-Sampling           | 57            |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.006833624   |
| n_timesteps             | 169280000     |
-------------------------------------------

 ---------------- Iteration 529 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 529            |
| ItrTime                 | 71             |
| LossAfter               | -0.0071691624  |
| LossBefore              | 1.4158789e-08  |
| MeanKL                  | 0.007972427    |
| MeanKLBefore            | -1.0429309e-08 |
| Step_0-AverageDiscou... | 158            |
| Step_0-AveragePolicyStd | 0.3946955      |
| Step_0-AverageReturn    | 379            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 696            |
| Step_0-MinReturn        | 25.8           |
| Step_0-NumTrajs         | 1030           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 154            |
| Step_1-AveragePolicyStd | 0.39461932     |
| Step_1-AverageReturn    | 355            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 759            |
| Step_1-MinReturn        | 33.8           |
| Step_1-NumTrajs         | 1115           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 148            |
| Time                    | 3.64e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.857          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0071691764   |
| n_timesteps             | 169600000      |
--------------------------------------------

 ---------------- Iteration 530 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 530           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0064768814 |
| LossBefore              | 1.6267728e-08 |
| MeanKL                  | 0.0072227553  |
| MeanKLBefore            | 8.940273e-09  |
| Step_0-AverageDiscou... | 151           |
| Step_0-AveragePolicyStd | 0.39430246    |
| Step_0-AverageReturn    | 349           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 699           |
| Step_0-MinReturn        | 44.1          |
| Step_0-NumTrajs         | 1083          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 143           |
| Step_1-AverageDiscou... | 151           |
| Step_1-AveragePolicyStd | 0.39420357    |
| Step_1-AverageReturn    | 354           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 759           |
| Step_1-MinReturn        | 33.8          |
| Step_1-NumTrajs         | 1073          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 141           |
| Time                    | 3.65e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.851         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0064768977  |
| n_timesteps             | 169920000     |
-------------------------------------------

 ---------------- Iteration 531 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 531            |
| ItrTime                 | 72.4           |
| LossAfter               | -0.006685114   |
| LossBefore              | -2.1428581e-09 |
| MeanKL                  | 0.0069305464   |
| MeanKLBefore            | 8.938458e-09   |
| Step_0-AverageDiscou... | 146            |
| Step_0-AveragePolicyStd | 0.3940724      |
| Step_0-AverageReturn    | 352            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 737            |
| Step_0-MinReturn        | 30             |
| Step_0-NumTrajs         | 1021           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 146            |
| Step_1-AveragePolicyStd | 0.39407685     |
| Step_1-AverageReturn    | 341            |
| Step_1-EnvExecTime      | 24.7           |
| Step_1-MaxReturn        | 754            |
| Step_1-MinReturn        | 16             |
| Step_1-NumTrajs         | 1099           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 149            |
| Time                    | 3.66e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.839          |
| Time-Sampling           | 57.8           |
| Time-TotalInner         | 58.9           |
| dLoss                   | 0.0066851117   |
| n_timesteps             | 170240000      |
--------------------------------------------

 ---------------- Iteration 532 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 532            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.0072389455  |
| LossBefore              | -1.7866135e-09 |
| MeanKL                  | 0.008014908    |
| MeanKLBefore            | 1.3408629e-08  |
| Step_0-AverageDiscou... | 157            |
| Step_0-AveragePolicyStd | 0.3941876      |
| Step_0-AverageReturn    | 387            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 718            |
| Step_0-MinReturn        | 19.1           |
| Step_0-NumTrajs         | 951            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 131            |
| Step_1-AverageDiscou... | 155            |
| Step_1-AveragePolicyStd | 0.3941133      |
| Step_1-AverageReturn    | 378            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 717            |
| Step_1-MinReturn        | 18.1           |
| Step_1-NumTrajs         | 1017           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 148            |
| Time                    | 3.66e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0072389436   |
| n_timesteps             | 170560000      |
--------------------------------------------

 ---------------- Iteration 533 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 533           |
| ItrTime                 | 72            |
| LossAfter               | -0.0068440214 |
| LossBefore              | 2.7308622e-09 |
| MeanKL                  | 0.007246419   |
| MeanKLBefore            | 2.9790503e-09 |
| Step_0-AverageDiscou... | 146           |
| Step_0-AveragePolicyStd | 0.3947496     |
| Step_0-AverageReturn    | 349           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 699           |
| Step_0-MinReturn        | 9.36          |
| Step_0-NumTrajs         | 1034          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 161           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.39464048    |
| Step_1-AverageReturn    | 339           |
| Step_1-EnvExecTime      | 24.3          |
| Step_1-MaxReturn        | 696           |
| Step_1-MinReturn        | 10            |
| Step_1-NumTrajs         | 1099          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 164           |
| Time                    | 3.67e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.869         |
| Time-Sampling           | 57.3          |
| Time-TotalInner         | 58.4          |
| dLoss                   | 0.006844024   |
| n_timesteps             | 170880000     |
-------------------------------------------

 ---------------- Iteration 534 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 534            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007260531   |
| LossBefore              | -8.434464e-09  |
| MeanKL                  | 0.008001307    |
| MeanKLBefore            | -1.4908041e-09 |
| Step_0-AverageDiscou... | 150            |
| Step_0-AveragePolicyStd | 0.39461517     |
| Step_0-AverageReturn    | 347            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 703            |
| Step_0-MinReturn        | 14.3           |
| Step_0-NumTrajs         | 1105           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 153            |
| Step_1-AverageDiscou... | 150            |
| Step_1-AveragePolicyStd | 0.39449358     |
| Step_1-AverageReturn    | 343            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 704            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1141           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 150            |
| Time                    | 3.68e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.876          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007260523    |
| n_timesteps             | 171200000      |
--------------------------------------------

 ---------------- Iteration 535 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 535           |
| ItrTime                 | 71.9          |
| LossAfter               | -0.0068204775 |
| LossBefore              | 5.9033507e-09 |
| MeanKL                  | 0.006784932   |
| MeanKLBefore            | 2.0857907e-08 |
| Step_0-AverageDiscou... | 159           |
| Step_0-AveragePolicyStd | 0.3942893     |
| Step_0-AverageReturn    | 396           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 722           |
| Step_0-MinReturn        | 24.3          |
| Step_0-NumTrajs         | 940           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 127           |
| Step_1-AverageDiscou... | 155           |
| Step_1-AveragePolicyStd | 0.39417356    |
| Step_1-AverageReturn    | 375           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 800           |
| Step_1-MinReturn        | 24.3          |
| Step_1-NumTrajs         | 1029          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 146           |
| Time                    | 3.69e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 57.2          |
| Time-TotalInner         | 58.2          |
| dLoss                   | 0.0068204836  |
| n_timesteps             | 171520000     |
-------------------------------------------

 ---------------- Iteration 536 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 536           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0076779374 |
| LossBefore              | -1.68676e-08  |
| MeanKL                  | 0.008134553   |
| MeanKLBefore            | -5.959515e-09 |
| Step_0-AverageDiscou... | 146           |
| Step_0-AveragePolicyStd | 0.39463437    |
| Step_0-AverageReturn    | 344           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 671           |
| Step_0-MinReturn        | 14.2          |
| Step_0-NumTrajs         | 1047          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 149           |
| Step_1-AverageDiscou... | 150           |
| Step_1-AveragePolicyStd | 0.39462227    |
| Step_1-AverageReturn    | 353           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 699           |
| Step_1-MinReturn        | 19.4          |
| Step_1-NumTrajs         | 1067          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 152           |
| Time                    | 3.69e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.847         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.0076779206  |
| n_timesteps             | 171840000     |
-------------------------------------------

 ---------------- Iteration 537 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 537           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.007251696  |
| LossBefore              | -7.851807e-09 |
| MeanKL                  | 0.008079471   |
| MeanKLBefore            | 2.9800915e-09 |
| Step_0-AverageDiscou... | 149           |
| Step_0-AveragePolicyStd | 0.39471474    |
| Step_0-AverageReturn    | 352           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 784           |
| Step_0-MinReturn        | 33.8          |
| Step_0-NumTrajs         | 1043          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 151           |
| Step_1-AverageDiscou... | 147           |
| Step_1-AveragePolicyStd | 0.39459163    |
| Step_1-AverageReturn    | 337           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 671           |
| Step_1-MinReturn        | 26.7          |
| Step_1-NumTrajs         | 1101          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 151           |
| Time                    | 3.7e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.869         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.0072516883  |
| n_timesteps             | 172160000     |
-------------------------------------------

 ---------------- Iteration 538 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 538            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.007282751   |
| LossBefore              | -1.8422284e-09 |
| MeanKL                  | 0.008561654    |
| MeanKLBefore            | 1.340751e-08   |
| Step_0-AverageDiscou... | 160            |
| Step_0-AveragePolicyStd | 0.39464748     |
| Step_0-AverageReturn    | 394            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 754            |
| Step_0-MinReturn        | 12.1           |
| Step_0-NumTrajs         | 975            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 130            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.39477044     |
| Step_1-AverageReturn    | 356            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 731            |
| Step_1-MinReturn        | 9.66           |
| Step_1-NumTrajs         | 1096           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 144            |
| Time                    | 3.71e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.832          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.0072827493   |
| n_timesteps             | 172480000      |
--------------------------------------------

 ---------------- Iteration 539 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 539            |
| ItrTime                 | 71             |
| LossAfter               | -0.007254879   |
| LossBefore              | -7.2693362e-09 |
| MeanKL                  | 0.008715522    |
| MeanKLBefore            | 8.9393195e-09  |
| Step_0-AverageDiscou... | 149            |
| Step_0-AveragePolicyStd | 0.39347386     |
| Step_0-AverageReturn    | 348            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 696            |
| Step_0-MinReturn        | 18.8           |
| Step_0-NumTrajs         | 1063           |
| Step_0-PolicyExecTime   | 1.71           |
| Step_0-StdReturn        | 147            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.3934228      |
| Step_1-AverageReturn    | 356            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 712            |
| Step_1-MinReturn        | 21.2           |
| Step_1-NumTrajs         | 1082           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 154            |
| Time                    | 3.71e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.861          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0072548715   |
| n_timesteps             | 172800000      |
--------------------------------------------

 ---------------- Iteration 540 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 540            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.007327731   |
| LossBefore              | -1.0324901e-08 |
| MeanKL                  | 0.007911236    |
| MeanKLBefore            | -5.9605236e-09 |
| Step_0-AverageDiscou... | 150            |
| Step_0-AveragePolicyStd | 0.39312294     |
| Step_0-AverageReturn    | 347            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 698            |
| Step_0-MinReturn        | 12.4           |
| Step_0-NumTrajs         | 1077           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 141            |
| Step_1-AverageDiscou... | 149            |
| Step_1-AveragePolicyStd | 0.3930949      |
| Step_1-AverageReturn    | 339            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 688            |
| Step_1-MinReturn        | 32             |
| Step_1-NumTrajs         | 1126           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 144            |
| Time                    | 3.72e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.842          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.0073277205   |
| n_timesteps             | 173120000      |
--------------------------------------------

 ---------------- Iteration 541 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 541            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0071716434  |
| LossBefore              | -9.7706785e-09 |
| MeanKL                  | 0.007656193    |
| MeanKLBefore            | 5.957772e-09   |
| Step_0-AverageDiscou... | 152            |
| Step_0-AveragePolicyStd | 0.3926359      |
| Step_0-AverageReturn    | 357            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 684            |
| Step_0-MinReturn        | 17             |
| Step_0-NumTrajs         | 1025           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 156            |
| Step_1-AveragePolicyStd | 0.3926393      |
| Step_1-AverageReturn    | 369            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 851            |
| Step_1-MinReturn        | 19.6           |
| Step_1-NumTrajs         | 1082           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 152            |
| Time                    | 3.73e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.865          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0071716337   |
| n_timesteps             | 173440000      |
--------------------------------------------

 ---------------- Iteration 542 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 542          |
| ItrTime                 | 69.3         |
| LossAfter               | -0.007006438 |
| LossBefore              | 9.282244e-09 |
| MeanKL                  | 0.008497331  |
| MeanKLBefore            | 4.469065e-09 |
| Step_0-AverageDiscou... | 140          |
| Step_0-AveragePolicyStd | 0.39167574   |
| Step_0-AverageReturn    | 309          |
| Step_0-EnvExecTime      | 22.7         |
| Step_0-MaxReturn        | 706          |
| Step_0-MinReturn        | 12.2         |
| Step_0-NumTrajs         | 1173         |
| Step_0-PolicyExecTime   | 1.27         |
| Step_0-StdReturn        | 154          |
| Step_1-AverageDiscou... | 148          |
| Step_1-AveragePolicyStd | 0.39162385   |
| Step_1-AverageReturn    | 335          |
| Step_1-EnvExecTime      | 23.4         |
| Step_1-MaxReturn        | 742          |
| Step_1-MinReturn        | 16.7         |
| Step_1-NumTrajs         | 1152         |
| Step_1-PolicyExecTime   | 4.05         |
| Step_1-StdReturn        | 158          |
| Time                    | 3.73e+04     |
| Time-InnerStep          | 0.164        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.884        |
| Time-Sampling           | 54.6         |
| Time-TotalInner         | 55.7         |
| dLoss                   | 0.0070064473 |
| n_timesteps             | 173760000    |
------------------------------------------

 ---------------- Iteration 543 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 543           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.006863594  |
| LossBefore              | 4.1932458e-09 |
| MeanKL                  | 0.007242696   |
| MeanKLBefore            | 1.4909002e-09 |
| Step_0-AverageDiscou... | 157           |
| Step_0-AveragePolicyStd | 0.39156964    |
| Step_0-AverageReturn    | 386           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 766           |
| Step_0-MinReturn        | 14.4          |
| Step_0-NumTrajs         | 945           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 129           |
| Step_1-AverageDiscou... | 163           |
| Step_1-AveragePolicyStd | 0.3914879     |
| Step_1-AverageReturn    | 394           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 713           |
| Step_1-MinReturn        | 29.7          |
| Step_1-NumTrajs         | 1013          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 149           |
| Time                    | 3.74e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.836         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0068635982  |
| n_timesteps             | 174080000     |
-------------------------------------------

 ---------------- Iteration 544 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 544           |
| ItrTime                 | 72.1          |
| LossAfter               | -0.0074901767 |
| LossBefore              | 1.97951e-09   |
| MeanKL                  | 0.008166264   |
| MeanKLBefore            | -7.447992e-09 |
| Step_0-AverageDiscou... | 138           |
| Step_0-AveragePolicyStd | 0.39155698    |
| Step_0-AverageReturn    | 330           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 671           |
| Step_0-MinReturn        | 12.6          |
| Step_0-NumTrajs         | 1024          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 166           |
| Step_1-AverageDiscou... | 142           |
| Step_1-AveragePolicyStd | 0.3914796     |
| Step_1-AverageReturn    | 332           |
| Step_1-EnvExecTime      | 24.5          |
| Step_1-MaxReturn        | 768           |
| Step_1-MinReturn        | 14            |
| Step_1-NumTrajs         | 1100          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 172           |
| Time                    | 3.75e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.845         |
| Time-Sampling           | 57.4          |
| Time-TotalInner         | 58.5          |
| dLoss                   | 0.0074901786  |
| n_timesteps             | 174400000     |
-------------------------------------------

 ---------------- Iteration 545 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 545            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.007051113   |
| LossBefore              | 9.0723065e-09  |
| MeanKL                  | 0.007885548    |
| MeanKLBefore            | -1.4918342e-09 |
| Step_0-AverageDiscou... | 156            |
| Step_0-AveragePolicyStd | 0.3911362      |
| Step_0-AverageReturn    | 361            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 706            |
| Step_0-MinReturn        | 19             |
| Step_0-NumTrajs         | 1067           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 132            |
| Step_1-AverageDiscou... | 155            |
| Step_1-AveragePolicyStd | 0.39115605     |
| Step_1-AverageReturn    | 353            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 739            |
| Step_1-MinReturn        | 34.1           |
| Step_1-NumTrajs         | 1128           |
| Step_1-PolicyExecTime   | 4.01           |
| Step_1-StdReturn        | 141            |
| Time                    | 3.76e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.007051122    |
| n_timesteps             | 174720000      |
--------------------------------------------

 ---------------- Iteration 546 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 546            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007268095   |
| LossBefore              | -2.3478683e-09 |
| MeanKL                  | 0.0079294965   |
| MeanKLBefore            | 1.0429581e-08  |
| Step_0-AverageDiscou... | 151            |
| Step_0-AveragePolicyStd | 0.39120838     |
| Step_0-AverageReturn    | 360            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 686            |
| Step_0-MinReturn        | 14.5           |
| Step_0-NumTrajs         | 1017           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 141            |
| Step_1-AverageDiscou... | 153            |
| Step_1-AveragePolicyStd | 0.39111632     |
| Step_1-AverageReturn    | 359            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 689            |
| Step_1-MinReturn        | 13.5           |
| Step_1-NumTrajs         | 1079           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 154            |
| Time                    | 3.76e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.841          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0072680926   |
| n_timesteps             | 175040000      |
--------------------------------------------

 ---------------- Iteration 547 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 547            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0072659017  |
| LossBefore              | -6.8621917e-09 |
| MeanKL                  | 0.0075991363   |
| MeanKLBefore            | 1.0427146e-08  |
| Step_0-AverageDiscou... | 152            |
| Step_0-AveragePolicyStd | 0.39106107     |
| Step_0-AverageReturn    | 371            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 638            |
| Step_0-MinReturn        | 10.4           |
| Step_0-NumTrajs         | 974            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 138            |
| Step_1-AverageDiscou... | 153            |
| Step_1-AveragePolicyStd | 0.39100534     |
| Step_1-AverageReturn    | 370            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 675            |
| Step_1-MinReturn        | 10.6           |
| Step_1-NumTrajs         | 1012           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 150            |
| Time                    | 3.77e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.814          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0072658947   |
| n_timesteps             | 175360000      |
--------------------------------------------

 ---------------- Iteration 548 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 548           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0065588863 |
| LossBefore              | -1.098745e-08 |
| MeanKL                  | 0.0074952454  |
| MeanKLBefore            | 1.0428474e-08 |
| Step_0-AverageDiscou... | 157           |
| Step_0-AveragePolicyStd | 0.39108703    |
| Step_0-AverageReturn    | 380           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 651           |
| Step_0-MinReturn        | 30.4          |
| Step_0-NumTrajs         | 985           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 138           |
| Step_1-AverageDiscou... | 155           |
| Step_1-AveragePolicyStd | 0.3909318     |
| Step_1-AverageReturn    | 363           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 748           |
| Step_1-MinReturn        | 14.5          |
| Step_1-NumTrajs         | 1078          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 147           |
| Time                    | 3.78e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.817         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.006558875   |
| n_timesteps             | 175680000     |
-------------------------------------------

 ---------------- Iteration 549 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 549            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.007210186   |
| LossBefore              | -1.7568951e-09 |
| MeanKL                  | 0.007922585    |
| MeanKLBefore            | 1.4897925e-09  |
| Step_0-AverageDiscou... | 154            |
| Step_0-AveragePolicyStd | 0.3914209      |
| Step_0-AverageReturn    | 363            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 623            |
| Step_0-MinReturn        | 11.5           |
| Step_0-NumTrajs         | 1030           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 135            |
| Step_1-AverageDiscou... | 155            |
| Step_1-AveragePolicyStd | 0.39137384     |
| Step_1-AverageReturn    | 365            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 707            |
| Step_1-MinReturn        | 27.2           |
| Step_1-NumTrajs         | 1070           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 145            |
| Time                    | 3.78e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.838          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0072101844   |
| n_timesteps             | 176000000      |
--------------------------------------------

 ---------------- Iteration 550 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 550             |
| ItrTime                 | 70.9            |
| LossAfter               | -0.00685326     |
| LossBefore              | -1.03705135e-10 |
| MeanKL                  | 0.007808201     |
| MeanKLBefore            | 7.4489472e-09   |
| Step_0-AverageDiscou... | 156             |
| Step_0-AveragePolicyStd | 0.3909111       |
| Step_0-AverageReturn    | 377             |
| Step_0-EnvExecTime      | 24              |
| Step_0-MaxReturn        | 685             |
| Step_0-MinReturn        | 23.2            |
| Step_0-NumTrajs         | 981             |
| Step_0-PolicyExecTime   | 1.34            |
| Step_0-StdReturn        | 131             |
| Step_1-AverageDiscou... | 150             |
| Step_1-AveragePolicyStd | 0.3908069       |
| Step_1-AverageReturn    | 347             |
| Step_1-EnvExecTime      | 23.5            |
| Step_1-MaxReturn        | 718             |
| Step_1-MinReturn        | 16.6            |
| Step_1-NumTrajs         | 1098            |
| Step_1-PolicyExecTime   | 4.04            |
| Step_1-StdReturn        | 147             |
| Time                    | 3.79e+04        |
| Time-InnerStep          | 0.164           |
| Time-MAMLSteps          | 13.6            |
| Time-OuterStep          | 13.6            |
| Time-SampleProc         | 0.832           |
| Time-Sampling           | 56.2            |
| Time-TotalInner         | 57.3            |
| dLoss                   | 0.00685326      |
| n_timesteps             | 176320000       |
---------------------------------------------

 ---------------- Iteration 551 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 551           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.006781076  |
| LossBefore              | 1.088908e-09  |
| MeanKL                  | 0.00799425    |
| MeanKLBefore            | 1.0431197e-08 |
| Step_0-AverageDiscou... | 140           |
| Step_0-AveragePolicyStd | 0.39076793    |
| Step_0-AverageReturn    | 328           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 641           |
| Step_0-MinReturn        | 0.357         |
| Step_0-NumTrajs         | 1061          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 144           |
| Step_1-AveragePolicyStd | 0.3907163     |
| Step_1-AverageReturn    | 331           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 735           |
| Step_1-MinReturn        | 17.3          |
| Step_1-NumTrajs         | 1130          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 162           |
| Time                    | 3.8e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.848         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.006781077   |
| n_timesteps             | 176640000     |
-------------------------------------------

 ---------------- Iteration 552 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 552            |
| ItrTime                 | 72             |
| LossAfter               | -0.0070844963  |
| LossBefore              | -7.9164355e-09 |
| MeanKL                  | 0.008242679    |
| MeanKLBefore            | 1.3408386e-08  |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3911592      |
| Step_0-AverageReturn    | 394            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 712            |
| Step_0-MinReturn        | 19.7           |
| Step_0-NumTrajs         | 946            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 129            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.3910868      |
| Step_1-AverageReturn    | 382            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 722            |
| Step_1-MinReturn        | 23.5           |
| Step_1-NumTrajs         | 1009           |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 146            |
| Time                    | 3.81e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.848          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.0070844884   |
| n_timesteps             | 176960000      |
--------------------------------------------

 ---------------- Iteration 553 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 553           |
| ItrTime                 | 71            |
| LossAfter               | -0.006830095  |
| LossBefore              | -3.407358e-09 |
| MeanKL                  | 0.0072300406  |
| MeanKLBefore            | 8.939667e-09  |
| Step_0-AverageDiscou... | 154           |
| Step_0-AveragePolicyStd | 0.39054435    |
| Step_0-AverageReturn    | 370           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 709           |
| Step_0-MinReturn        | 2.11          |
| Step_0-NumTrajs         | 1018          |
| Step_0-PolicyExecTime   | 1.7           |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 153           |
| Step_1-AveragePolicyStd | 0.3904424     |
| Step_1-AverageReturn    | 347           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 773           |
| Step_1-MinReturn        | 6.27          |
| Step_1-NumTrajs         | 1147          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 158           |
| Time                    | 3.81e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0068300916  |
| n_timesteps             | 177280000     |
-------------------------------------------

 ---------------- Iteration 554 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 554           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0071683945 |
| LossBefore              | 7.3425426e-09 |
| MeanKL                  | 0.007532213   |
| MeanKLBefore            | 1.1919253e-08 |
| Step_0-AverageDiscou... | 151           |
| Step_0-AveragePolicyStd | 0.39065042    |
| Step_0-AverageReturn    | 365           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 735           |
| Step_0-MinReturn        | 19.2          |
| Step_0-NumTrajs         | 1000          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 152           |
| Step_1-AverageDiscou... | 150           |
| Step_1-AveragePolicyStd | 0.39064443    |
| Step_1-AverageReturn    | 343           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 709           |
| Step_1-MinReturn        | 9.07          |
| Step_1-NumTrajs         | 1124          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 152           |
| Time                    | 3.82e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.854         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.007168402   |
| n_timesteps             | 177600000     |
-------------------------------------------

 ---------------- Iteration 555 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 555            |
| ItrTime                 | 70             |
| LossAfter               | -0.0072897365  |
| LossBefore              | 1.00280815e-08 |
| MeanKL                  | 0.007225598    |
| MeanKLBefore            | -1.0429802e-08 |
| Step_0-AverageDiscou... | 154            |
| Step_0-AveragePolicyStd | 0.39059022     |
| Step_0-AverageReturn    | 361            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 724            |
| Step_0-MinReturn        | 3.52           |
| Step_0-NumTrajs         | 1060           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 156            |
| Step_1-AveragePolicyStd | 0.39056626     |
| Step_1-AverageReturn    | 362            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 754            |
| Step_1-MinReturn        | 16.3           |
| Step_1-NumTrajs         | 1113           |
| Step_1-PolicyExecTime   | 4.01           |
| Step_1-StdReturn        | 157            |
| Time                    | 3.83e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.837          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007289747    |
| n_timesteps             | 177920000      |
--------------------------------------------

 ---------------- Iteration 556 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 556           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0072791865 |
| LossBefore              | 2.4666098e-09 |
| MeanKL                  | 0.008118957   |
| MeanKLBefore            | 2.2348054e-08 |
| Step_0-AverageDiscou... | 159           |
| Step_0-AveragePolicyStd | 0.39067408    |
| Step_0-AverageReturn    | 390           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 688           |
| Step_0-MinReturn        | 17.7          |
| Step_0-NumTrajs         | 970           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 134           |
| Step_1-AverageDiscou... | 158           |
| Step_1-AveragePolicyStd | 0.390556      |
| Step_1-AverageReturn    | 380           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 816           |
| Step_1-MinReturn        | 34.2          |
| Step_1-NumTrajs         | 1044          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 152           |
| Time                    | 3.83e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.815         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.007279189   |
| n_timesteps             | 178240000     |
-------------------------------------------

 ---------------- Iteration 557 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 557            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.007139628   |
| LossBefore              | -1.0371527e-08 |
| MeanKL                  | 0.008620337    |
| MeanKLBefore            | 1.6228796e-12  |
| Step_0-AverageDiscou... | 151            |
| Step_0-AveragePolicyStd | 0.39051437     |
| Step_0-AverageReturn    | 359            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 675            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 1032           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 149            |
| Step_1-AverageDiscou... | 149            |
| Step_1-AveragePolicyStd | 0.39050806     |
| Step_1-AverageReturn    | 341            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 776            |
| Step_1-MinReturn        | 13.1           |
| Step_1-NumTrajs         | 1126           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 147            |
| Time                    | 3.84e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.852          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.0071396176   |
| n_timesteps             | 178560000      |
--------------------------------------------

 ---------------- Iteration 558 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 558            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.007075145   |
| LossBefore              | -1.3970487e-08 |
| MeanKL                  | 0.0082595665   |
| MeanKLBefore            | 2.9802707e-09  |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3909264      |
| Step_0-AverageReturn    | 383            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 727            |
| Step_0-MinReturn        | 12.5           |
| Step_0-NumTrajs         | 995            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 134            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.39085895     |
| Step_1-AverageReturn    | 351            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 758            |
| Step_1-MinReturn        | 21.7           |
| Step_1-NumTrajs         | 1098           |
| Step_1-PolicyExecTime   | 4.03           |
| Step_1-StdReturn        | 149            |
| Time                    | 3.85e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.86           |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.007075131    |
| n_timesteps             | 178880000      |
--------------------------------------------

 ---------------- Iteration 559 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 559           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0066800728 |
| LossBefore              | -8.471763e-09 |
| MeanKL                  | 0.0076389993  |
| MeanKLBefore            | 7.4493487e-09 |
| Step_0-AverageDiscou... | 152           |
| Step_0-AveragePolicyStd | 0.3913456     |
| Step_0-AverageReturn    | 362           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 762           |
| Step_0-MinReturn        | 0.456         |
| Step_0-NumTrajs         | 1042          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 155           |
| Step_1-AverageDiscou... | 153           |
| Step_1-AveragePolicyStd | 0.39125013    |
| Step_1-AverageReturn    | 352           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 764           |
| Step_1-MinReturn        | 6.39          |
| Step_1-NumTrajs         | 1117          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 155           |
| Time                    | 3.86e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.827         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0066800644  |
| n_timesteps             | 179200000     |
-------------------------------------------

 ---------------- Iteration 560 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 560            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.0075967805  |
| LossBefore              | -5.8931193e-09 |
| MeanKL                  | 0.008295552    |
| MeanKLBefore            | 3.8866688e-13  |
| Step_0-AverageDiscou... | 164            |
| Step_0-AveragePolicyStd | 0.39222708     |
| Step_0-AverageReturn    | 420            |
| Step_0-EnvExecTime      | 24.6           |
| Step_0-MaxReturn        | 659            |
| Step_0-MinReturn        | 16.3           |
| Step_0-NumTrajs         | 903            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 125            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.39214802     |
| Step_1-AverageReturn    | 381            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 700            |
| Step_1-MinReturn        | 22.3           |
| Step_1-NumTrajs         | 1037           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 149            |
| Time                    | 3.86e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.0075967745   |
| n_timesteps             | 179520000      |
--------------------------------------------

 ---------------- Iteration 561 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 561           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0074614966 |
| LossBefore              | 4.0812105e-09 |
| MeanKL                  | 0.008130165   |
| MeanKLBefore            | -2.979656e-09 |
| Step_0-AverageDiscou... | 153           |
| Step_0-AveragePolicyStd | 0.3926267     |
| Step_0-AverageReturn    | 362           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 723           |
| Step_0-MinReturn        | 17.9          |
| Step_0-NumTrajs         | 1033          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 145           |
| Step_1-AverageDiscou... | 154           |
| Step_1-AveragePolicyStd | 0.39261925    |
| Step_1-AverageReturn    | 354           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 748           |
| Step_1-MinReturn        | 30.9          |
| Step_1-NumTrajs         | 1124          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 153           |
| Time                    | 3.87e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.822         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007461501   |
| n_timesteps             | 179840000     |
-------------------------------------------

 ---------------- Iteration 562 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 562            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.007542354   |
| LossBefore              | -4.9645417e-09 |
| MeanKL                  | 0.007905966    |
| MeanKLBefore            | 1.0429308e-08  |
| Step_0-AverageDiscou... | 147            |
| Step_0-AveragePolicyStd | 0.39265656     |
| Step_0-AverageReturn    | 360            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 728            |
| Step_0-MinReturn        | 3.96           |
| Step_0-NumTrajs         | 1009           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 165            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.39268553     |
| Step_1-AverageReturn    | 372            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 763            |
| Step_1-MinReturn        | 4.77           |
| Step_1-NumTrajs         | 1032           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 167            |
| Time                    | 3.88e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.815          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.007542349    |
| n_timesteps             | 180160000      |
--------------------------------------------

 ---------------- Iteration 563 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 563           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007205381  |
| LossBefore              | 7.988309e-09  |
| MeanKL                  | 0.008281052   |
| MeanKLBefore            | 1.7880872e-08 |
| Step_0-AverageDiscou... | 153           |
| Step_0-AveragePolicyStd | 0.39265403    |
| Step_0-AverageReturn    | 374           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 729           |
| Step_0-MinReturn        | 15.5          |
| Step_0-NumTrajs         | 983           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 155           |
| Step_1-AveragePolicyStd | 0.39259928    |
| Step_1-AverageReturn    | 367           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 753           |
| Step_1-MinReturn        | 13.4          |
| Step_1-NumTrajs         | 1067          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 156           |
| Time                    | 3.88e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.007205389   |
| n_timesteps             | 180480000     |
-------------------------------------------

 ---------------- Iteration 564 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 564           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0071733966 |
| LossBefore              | 5.9883503e-09 |
| MeanKL                  | 0.0073728724  |
| MeanKLBefore            | 2.3840698e-08 |
| Step_0-AverageDiscou... | 167           |
| Step_0-AveragePolicyStd | 0.39252475    |
| Step_0-AverageReturn    | 418           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 713           |
| Step_0-MinReturn        | 20.8          |
| Step_0-NumTrajs         | 919           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 121           |
| Step_1-AverageDiscou... | 165           |
| Step_1-AveragePolicyStd | 0.39249533    |
| Step_1-AverageReturn    | 397           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 755           |
| Step_1-MinReturn        | 17.9          |
| Step_1-NumTrajs         | 1022          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 143           |
| Time                    | 3.89e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.8           |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0071734027  |
| n_timesteps             | 180800000     |
-------------------------------------------

 ---------------- Iteration 565 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 565            |
| ItrTime                 | 69             |
| LossAfter               | -0.0074289977  |
| LossBefore              | -1.3287933e-09 |
| MeanKL                  | 0.0097146435   |
| MeanKLBefore            | -5.9578893e-09 |
| Step_0-AverageDiscou... | 156            |
| Step_0-AveragePolicyStd | 0.39259797     |
| Step_0-AverageReturn    | 378            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 718            |
| Step_0-MinReturn        | 8.09           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 156            |
| Step_1-AverageDiscou... | 152            |
| Step_1-AveragePolicyStd | 0.39255354     |
| Step_1-AverageReturn    | 345            |
| Step_1-EnvExecTime      | 22.6           |
| Step_1-MaxReturn        | 757            |
| Step_1-MinReturn        | 12.3           |
| Step_1-NumTrajs         | 1158           |
| Step_1-PolicyExecTime   | 3.92           |
| Step_1-StdReturn        | 164            |
| Time                    | 3.9e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.0074289963   |
| n_timesteps             | 181120000      |
--------------------------------------------

 ---------------- Iteration 566 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 566           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007141265  |
| LossBefore              | -9.934333e-09 |
| MeanKL                  | 0.007548404   |
| MeanKLBefore            | 1.1919413e-08 |
| Step_0-AverageDiscou... | 159           |
| Step_0-AveragePolicyStd | 0.39291325    |
| Step_0-AverageReturn    | 399           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 721           |
| Step_0-MinReturn        | 19.9          |
| Step_0-NumTrajs         | 922           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 136           |
| Step_1-AverageDiscou... | 161           |
| Step_1-AveragePolicyStd | 0.39295268    |
| Step_1-AverageReturn    | 399           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 717           |
| Step_1-MinReturn        | 12.5          |
| Step_1-NumTrajs         | 987           |
| Step_1-PolicyExecTime   | 4.5           |
| Step_1-StdReturn        | 155           |
| Time                    | 3.9e+04       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.794         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0071412553  |
| n_timesteps             | 181440000     |
-------------------------------------------

 ---------------- Iteration 567 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 567            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0067609353  |
| LossBefore              | -6.6559724e-10 |
| MeanKL                  | 0.0076544555   |
| MeanKLBefore            | -4.471055e-09  |
| Step_0-AverageDiscou... | 157            |
| Step_0-AveragePolicyStd | 0.39314353     |
| Step_0-AverageReturn    | 392            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 778            |
| Step_0-MinReturn        | 19             |
| Step_0-NumTrajs         | 958            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 149            |
| Step_1-AverageDiscou... | 159            |
| Step_1-AveragePolicyStd | 0.39301473     |
| Step_1-AverageReturn    | 384            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 727            |
| Step_1-MinReturn        | 19.8           |
| Step_1-NumTrajs         | 1035           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 154            |
| Time                    | 3.91e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.808          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.006760935    |
| n_timesteps             | 181760000      |
--------------------------------------------

 ---------------- Iteration 568 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 568           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.0071186377 |
| LossBefore              | 1.2314777e-08 |
| MeanKL                  | 0.0076276227  |
| MeanKLBefore            | 1.4907313e-09 |
| Step_0-AverageDiscou... | 155           |
| Step_0-AveragePolicyStd | 0.392797      |
| Step_0-AverageReturn    | 380           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 739           |
| Step_0-MinReturn        | 16            |
| Step_0-NumTrajs         | 1007          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 160           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.39285097    |
| Step_1-AverageReturn    | 373           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 774           |
| Step_1-MinReturn        | 16.4          |
| Step_1-NumTrajs         | 1055          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 160           |
| Time                    | 3.92e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.852         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.00711865    |
| n_timesteps             | 182080000     |
-------------------------------------------

 ---------------- Iteration 569 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 569            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007205114   |
| LossBefore              | -1.4088313e-08 |
| MeanKL                  | 0.007744997    |
| MeanKLBefore            | 2.2958523e-12  |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.39289862     |
| Step_0-AverageReturn    | 427            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 773            |
| Step_0-MinReturn        | 20.8           |
| Step_0-NumTrajs         | 973            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 164            |
| Step_1-AveragePolicyStd | 0.392857       |
| Step_1-AverageReturn    | 385            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 724            |
| Step_1-MinReturn        | 33.7           |
| Step_1-NumTrajs         | 1068           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 148            |
| Time                    | 3.93e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0072051      |
| n_timesteps             | 182400000      |
--------------------------------------------

 ---------------- Iteration 570 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 570           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.007347028  |
| LossBefore              | -7.293867e-09 |
| MeanKL                  | 0.007844113   |
| MeanKLBefore            | 2.980269e-09  |
| Step_0-AverageDiscou... | 155           |
| Step_0-AveragePolicyStd | 0.39262602    |
| Step_0-AverageReturn    | 369           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 691           |
| Step_0-MinReturn        | 18            |
| Step_0-NumTrajs         | 1016          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 147           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.3925478     |
| Step_1-AverageReturn    | 356           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 719           |
| Step_1-MinReturn        | 12.9          |
| Step_1-NumTrajs         | 1083          |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 154           |
| Time                    | 3.93e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.882         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0073470203  |
| n_timesteps             | 182720000     |
-------------------------------------------

 ---------------- Iteration 571 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 571            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0071371966  |
| LossBefore              | -1.9699803e-08 |
| MeanKL                  | 0.0069862865   |
| MeanKLBefore            | -2.9802454e-09 |
| Step_0-AverageDiscou... | 153            |
| Step_0-AveragePolicyStd | 0.3925056      |
| Step_0-AverageReturn    | 371            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 758            |
| Step_0-MinReturn        | 16.7           |
| Step_0-NumTrajs         | 1003           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 156            |
| Step_1-AverageDiscou... | 156            |
| Step_1-AveragePolicyStd | 0.39242998     |
| Step_1-AverageReturn    | 368            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 773            |
| Step_1-MinReturn        | 19.7           |
| Step_1-NumTrajs         | 1077           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 162            |
| Time                    | 3.94e+04       |
| Time-InnerStep          | 0.157          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.87           |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007137177    |
| n_timesteps             | 183040000      |
--------------------------------------------

 ---------------- Iteration 572 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 572            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0068611773  |
| LossBefore              | -3.0398304e-09 |
| MeanKL                  | 0.007473597    |
| MeanKLBefore            | 4.471245e-09   |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3924265      |
| Step_0-AverageReturn    | 380            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 694            |
| Step_0-MinReturn        | 21.8           |
| Step_0-NumTrajs         | 1002           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 142            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.39243466     |
| Step_1-AverageReturn    | 367            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 780            |
| Step_1-MinReturn        | 7.07           |
| Step_1-NumTrajs         | 1068           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 154            |
| Time                    | 3.95e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.818          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.006861174    |
| n_timesteps             | 183360000      |
--------------------------------------------

 ---------------- Iteration 573 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 573           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.007074962  |
| LossBefore              | 1.2082825e-09 |
| MeanKL                  | 0.007351483   |
| MeanKLBefore            | 1.1921193e-08 |
| Step_0-AverageDiscou... | 155           |
| Step_0-AveragePolicyStd | 0.39260885    |
| Step_0-AverageReturn    | 370           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 735           |
| Step_0-MinReturn        | 19.9          |
| Step_0-NumTrajs         | 1018          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 148           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.39254677    |
| Step_1-AverageReturn    | 373           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 747           |
| Step_1-MinReturn        | 18.8          |
| Step_1-NumTrajs         | 1064          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 163           |
| Time                    | 3.95e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.815         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.0070749633  |
| n_timesteps             | 183680000     |
-------------------------------------------

 ---------------- Iteration 574 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 574            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.006930715   |
| LossBefore              | -3.995109e-09  |
| MeanKL                  | 0.0076094544   |
| MeanKLBefore            | -2.9799465e-09 |
| Step_0-AverageDiscou... | 151            |
| Step_0-AveragePolicyStd | 0.3924453      |
| Step_0-AverageReturn    | 358            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 718            |
| Step_0-MinReturn        | 8.3            |
| Step_0-NumTrajs         | 1054           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 159            |
| Step_1-AverageDiscou... | 150            |
| Step_1-AveragePolicyStd | 0.39238796     |
| Step_1-AverageReturn    | 352            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 761            |
| Step_1-MinReturn        | 5.86           |
| Step_1-NumTrajs         | 1083           |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 158            |
| Time                    | 3.96e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.884          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0069307107   |
| n_timesteps             | 184000000      |
--------------------------------------------

 ---------------- Iteration 575 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 575           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0071102143 |
| LossBefore              | -8.372947e-09 |
| MeanKL                  | 0.0074308543  |
| MeanKLBefore            | 5.9605414e-09 |
| Step_0-AverageDiscou... | 153           |
| Step_0-AveragePolicyStd | 0.3924217     |
| Step_0-AverageReturn    | 358           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 730           |
| Step_0-MinReturn        | 10.1          |
| Step_0-NumTrajs         | 1052          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 149           |
| Step_1-AverageDiscou... | 155           |
| Step_1-AveragePolicyStd | 0.392238      |
| Step_1-AverageReturn    | 359           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 708           |
| Step_1-MinReturn        | 31.5          |
| Step_1-NumTrajs         | 1090          |
| Step_1-PolicyExecTime   | 3.99          |
| Step_1-StdReturn        | 149           |
| Time                    | 3.97e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.838         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.007110206   |
| n_timesteps             | 184320000     |
-------------------------------------------

 ---------------- Iteration 576 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 576           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.007883054  |
| LossBefore              | 6.5940595e-09 |
| MeanKL                  | 0.0087006595  |
| MeanKLBefore            | 5.9593486e-09 |
| Step_0-AverageDiscou... | 155           |
| Step_0-AveragePolicyStd | 0.39191473    |
| Step_0-AverageReturn    | 370           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 682           |
| Step_0-MinReturn        | 27.1          |
| Step_0-NumTrajs         | 1040          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.39189383    |
| Step_1-AverageReturn    | 364           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 728           |
| Step_1-MinReturn        | 19.9          |
| Step_1-NumTrajs         | 1100          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 153           |
| Time                    | 3.98e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.851         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007883061   |
| n_timesteps             | 184640000     |
-------------------------------------------

 ---------------- Iteration 577 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 577            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.007170686   |
| LossBefore              | 7.012275e-10   |
| MeanKL                  | 0.007890679    |
| MeanKLBefore            | -1.4887303e-09 |
| Step_0-AverageDiscou... | 169            |
| Step_0-AveragePolicyStd | 0.39215395     |
| Step_0-AverageReturn    | 426            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 783            |
| Step_0-MinReturn        | 8.38           |
| Step_0-NumTrajs         | 942            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 143            |
| Step_1-AverageDiscou... | 164            |
| Step_1-AveragePolicyStd | 0.39209792     |
| Step_1-AverageReturn    | 397            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 749            |
| Step_1-MinReturn        | 18.1           |
| Step_1-NumTrajs         | 1034           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 154            |
| Time                    | 3.98e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.82           |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.007170687    |
| n_timesteps             | 184960000      |
--------------------------------------------

 ---------------- Iteration 578 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 578           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.007260935  |
| LossBefore              | 2.5527727e-09 |
| MeanKL                  | 0.007771781   |
| MeanKLBefore            | 1.1919225e-08 |
| Step_0-AverageDiscou... | 158           |
| Step_0-AveragePolicyStd | 0.39236024    |
| Step_0-AverageReturn    | 370           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 722           |
| Step_0-MinReturn        | 1.74          |
| Step_0-NumTrajs         | 1041          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 151           |
| Step_1-AverageDiscou... | 158           |
| Step_1-AveragePolicyStd | 0.39231536    |
| Step_1-AverageReturn    | 365           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 759           |
| Step_1-MinReturn        | 19.2          |
| Step_1-NumTrajs         | 1099          |
| Step_1-PolicyExecTime   | 4.06          |
| Step_1-StdReturn        | 151           |
| Time                    | 3.99e+04      |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.801         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0072609372  |
| n_timesteps             | 185280000     |
-------------------------------------------

 ---------------- Iteration 579 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 579            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.007484278   |
| LossBefore              | -2.4248088e-09 |
| MeanKL                  | 0.008524944    |
| MeanKLBefore            | 7.4504714e-09  |
| Step_0-AverageDiscou... | 157            |
| Step_0-AveragePolicyStd | 0.3922597      |
| Step_0-AverageReturn    | 381            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 738            |
| Step_0-MinReturn        | 7.52           |
| Step_0-NumTrajs         | 1019           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.39219394     |
| Step_1-AverageReturn    | 370            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 777            |
| Step_1-MinReturn        | 11.1           |
| Step_1-NumTrajs         | 1065           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 155            |
| Time                    | 4e+04          |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.823          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007484276    |
| n_timesteps             | 185600000      |
--------------------------------------------

 ---------------- Iteration 580 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 580           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.007710302  |
| LossBefore              | 2.3177793e-09 |
| MeanKL                  | 0.0077780574  |
| MeanKLBefore            | 1.3409879e-08 |
| Step_0-AverageDiscou... | 154           |
| Step_0-AveragePolicyStd | 0.3923462     |
| Step_0-AverageReturn    | 366           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 752           |
| Step_0-MinReturn        | 11            |
| Step_0-NumTrajs         | 1029          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.392285      |
| Step_1-AverageReturn    | 364           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 745           |
| Step_1-MinReturn        | 8.13          |
| Step_1-NumTrajs         | 1075          |
| Step_1-PolicyExecTime   | 4.45          |
| Step_1-StdReturn        | 148           |
| Time                    | 4e+04         |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.861         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.007710304   |
| n_timesteps             | 185920000     |
-------------------------------------------

 ---------------- Iteration 581 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 581           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0069746994 |
| LossBefore              | 8.5888345e-09 |
| MeanKL                  | 0.007806559   |
| MeanKLBefore            | 2.9805753e-09 |
| Step_0-AverageDiscou... | 156           |
| Step_0-AveragePolicyStd | 0.39227164    |
| Step_0-AverageReturn    | 389           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 759           |
| Step_0-MinReturn        | 12.6          |
| Step_0-NumTrajs         | 939           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 142           |
| Step_1-AverageDiscou... | 156           |
| Step_1-AveragePolicyStd | 0.39228025    |
| Step_1-AverageReturn    | 378           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 753           |
| Step_1-MinReturn        | 16.3          |
| Step_1-NumTrajs         | 1027          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 160           |
| Time                    | 4.01e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.8           |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.006974708   |
| n_timesteps             | 186240000     |
-------------------------------------------

 ---------------- Iteration 582 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 582            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.0071103675  |
| LossBefore              | -4.982808e-09  |
| MeanKL                  | 0.007745953    |
| MeanKLBefore            | -2.0858888e-08 |
| Step_0-AverageDiscou... | 161            |
| Step_0-AveragePolicyStd | 0.39189997     |
| Step_0-AverageReturn    | 404            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 712            |
| Step_0-MinReturn        | 11.1           |
| Step_0-NumTrajs         | 938            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 138            |
| Step_1-AverageDiscou... | 158            |
| Step_1-AveragePolicyStd | 0.39178792     |
| Step_1-AverageReturn    | 377            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 751            |
| Step_1-MinReturn        | 13.2           |
| Step_1-NumTrajs         | 1053           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 147            |
| Time                    | 4.02e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58             |
| dLoss                   | 0.0071103624   |
| n_timesteps             | 186560000      |
--------------------------------------------

 ---------------- Iteration 583 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 583           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0067121657 |
| LossBefore              | 7.554436e-09  |
| MeanKL                  | 0.00795725    |
| MeanKLBefore            | 1.3409211e-08 |
| Step_0-AverageDiscou... | 160           |
| Step_0-AveragePolicyStd | 0.39207703    |
| Step_0-AverageReturn    | 397           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 743           |
| Step_0-MinReturn        | 11.7          |
| Step_0-NumTrajs         | 968           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 158           |
| Step_1-AverageDiscou... | 159           |
| Step_1-AveragePolicyStd | 0.3920878     |
| Step_1-AverageReturn    | 377           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 812           |
| Step_1-MinReturn        | 9.1           |
| Step_1-NumTrajs         | 1057          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 150           |
| Time                    | 4.02e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.82          |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.006712173   |
| n_timesteps             | 186880000     |
-------------------------------------------

 ---------------- Iteration 584 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 584            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.007178955   |
| LossBefore              | -3.7216228e-09 |
| MeanKL                  | 0.0077253757   |
| MeanKLBefore            | 1.0549784e-12  |
| Step_0-AverageDiscou... | 161            |
| Step_0-AveragePolicyStd | 0.39189753     |
| Step_0-AverageReturn    | 400            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 740            |
| Step_0-MinReturn        | 18.8           |
| Step_0-NumTrajs         | 956            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 144            |
| Step_1-AverageDiscou... | 159            |
| Step_1-AveragePolicyStd | 0.39179215     |
| Step_1-AverageReturn    | 383            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 779            |
| Step_1-MinReturn        | 17.6           |
| Step_1-NumTrajs         | 1047           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 155            |
| Time                    | 4.03e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.795          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.007178951    |
| n_timesteps             | 187200000      |
--------------------------------------------

 ---------------- Iteration 585 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 585            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.007423266   |
| LossBefore              | 5.4194266e-10  |
| MeanKL                  | 0.008473379    |
| MeanKLBefore            | -4.4699444e-09 |
| Step_0-AverageDiscou... | 164            |
| Step_0-AveragePolicyStd | 0.39159018     |
| Step_0-AverageReturn    | 405            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 691            |
| Step_0-MinReturn        | 14.9           |
| Step_0-NumTrajs         | 946            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 133            |
| Step_1-AverageDiscou... | 160            |
| Step_1-AveragePolicyStd | 0.3914814      |
| Step_1-AverageReturn    | 382            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 706            |
| Step_1-MinReturn        | 15.6           |
| Step_1-NumTrajs         | 1026           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 145            |
| Time                    | 4.04e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 56             |
| dLoss                   | 0.0074232663   |
| n_timesteps             | 187520000      |
--------------------------------------------

 ---------------- Iteration 586 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 586            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.007002649   |
| LossBefore              | -1.5560129e-09 |
| MeanKL                  | 0.00777217     |
| MeanKLBefore            | 2.9795273e-09  |
| Step_0-AverageDiscou... | 166            |
| Step_0-AveragePolicyStd | 0.3908049      |
| Step_0-AverageReturn    | 405            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 724            |
| Step_0-MinReturn        | 36             |
| Step_0-NumTrajs         | 977            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 136            |
| Step_1-AverageDiscou... | 162            |
| Step_1-AveragePolicyStd | 0.39074096     |
| Step_1-AverageReturn    | 387            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 792            |
| Step_1-MinReturn        | 29.5           |
| Step_1-NumTrajs         | 1065           |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 156            |
| Time                    | 4.05e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.874          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0070026475   |
| n_timesteps             | 187840000      |
--------------------------------------------

 ---------------- Iteration 587 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 587           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007205324  |
| LossBefore              | 7.766497e-09  |
| MeanKL                  | 0.008656089   |
| MeanKLBefore            | -2.980169e-09 |
| Step_0-AverageDiscou... | 162           |
| Step_0-AveragePolicyStd | 0.3907601     |
| Step_0-AverageReturn    | 390           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 733           |
| Step_0-MinReturn        | 27.2          |
| Step_0-NumTrajs         | 1013          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 141           |
| Step_1-AverageDiscou... | 162           |
| Step_1-AveragePolicyStd | 0.39068103    |
| Step_1-AverageReturn    | 377           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 736           |
| Step_1-MinReturn        | 44.6          |
| Step_1-NumTrajs         | 1100          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 143           |
| Time                    | 4.05e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.868         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0072053317  |
| n_timesteps             | 188160000     |
-------------------------------------------

 ---------------- Iteration 588 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 588            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.00684292    |
| LossBefore              | -1.0102785e-08 |
| MeanKL                  | 0.0072479397   |
| MeanKLBefore            | 1.4881595e-09  |
| Step_0-AverageDiscou... | 153            |
| Step_0-AveragePolicyStd | 0.39081234     |
| Step_0-AverageReturn    | 369            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 688            |
| Step_0-MinReturn        | 13.8           |
| Step_0-NumTrajs         | 1005           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 156            |
| Step_1-AveragePolicyStd | 0.39073792     |
| Step_1-AverageReturn    | 376            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 758            |
| Step_1-MinReturn        | 11.7           |
| Step_1-NumTrajs         | 1048           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 166            |
| Time                    | 4.06e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.817          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.1           |
| dLoss                   | 0.00684291     |
| n_timesteps             | 188480000      |
--------------------------------------------

 ---------------- Iteration 589 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 589           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.007834207  |
| LossBefore              | -4.412632e-09 |
| MeanKL                  | 0.009701054   |
| MeanKLBefore            | 5.9585368e-09 |
| Step_0-AverageDiscou... | 161           |
| Step_0-AveragePolicyStd | 0.390618      |
| Step_0-AverageReturn    | 385           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 722           |
| Step_0-MinReturn        | 18            |
| Step_0-NumTrajs         | 1032          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 145           |
| Step_1-AverageDiscou... | 160           |
| Step_1-AveragePolicyStd | 0.39063808    |
| Step_1-AverageReturn    | 378           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 766           |
| Step_1-MinReturn        | 15.7          |
| Step_1-NumTrajs         | 1088          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 162           |
| Time                    | 4.07e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.837         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007834203   |
| n_timesteps             | 188800000     |
-------------------------------------------

 ---------------- Iteration 590 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 590           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.007001534  |
| LossBefore              | 1.0073311e-09 |
| MeanKL                  | 0.007615489   |
| MeanKLBefore            | 1.6390512e-08 |
| Step_0-AverageDiscou... | 162           |
| Step_0-AveragePolicyStd | 0.39065072    |
| Step_0-AverageReturn    | 399           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 703           |
| Step_0-MinReturn        | 12.9          |
| Step_0-NumTrajs         | 966           |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 145           |
| Step_1-AverageDiscou... | 159           |
| Step_1-AveragePolicyStd | 0.39059317    |
| Step_1-AverageReturn    | 378           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 771           |
| Step_1-MinReturn        | 10.3          |
| Step_1-NumTrajs         | 1058          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 158           |
| Time                    | 4.07e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.822         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007001535   |
| n_timesteps             | 189120000     |
-------------------------------------------

 ---------------- Iteration 591 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 591          |
| ItrTime                 | 71.6         |
| LossAfter               | -0.007361926 |
| LossBefore              | 9.537417e-09 |
| MeanKL                  | 0.008933629  |
| MeanKLBefore            | 1.638979e-08 |
| Step_0-AverageDiscou... | 167          |
| Step_0-AveragePolicyStd | 0.39019984   |
| Step_0-AverageReturn    | 410          |
| Step_0-EnvExecTime      | 24           |
| Step_0-MaxReturn        | 701          |
| Step_0-MinReturn        | 25.7         |
| Step_0-NumTrajs         | 963          |
| Step_0-PolicyExecTime   | 1.34         |
| Step_0-StdReturn        | 143          |
| Step_1-AverageDiscou... | 170          |
| Step_1-AveragePolicyStd | 0.39009866   |
| Step_1-AverageReturn    | 411          |
| Step_1-EnvExecTime      | 24.1         |
| Step_1-MaxReturn        | 806          |
| Step_1-MinReturn        | 31.4         |
| Step_1-NumTrajs         | 1030         |
| Step_1-PolicyExecTime   | 4.22         |
| Step_1-StdReturn        | 155          |
| Time                    | 4.08e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.84         |
| Time-Sampling           | 57           |
| Time-TotalInner         | 58           |
| dLoss                   | 0.0073619355 |
| n_timesteps             | 189440000    |
------------------------------------------

 ---------------- Iteration 592 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 592            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.007362101   |
| LossBefore              | 6.21271e-09    |
| MeanKL                  | 0.00890385     |
| MeanKLBefore            | -1.4904333e-09 |
| Step_0-AverageDiscou... | 169            |
| Step_0-AveragePolicyStd | 0.38983142     |
| Step_0-AverageReturn    | 425            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 712            |
| Step_0-MinReturn        | 6.22           |
| Step_0-NumTrajs         | 950            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 154            |
| Step_1-AverageDiscou... | 164            |
| Step_1-AveragePolicyStd | 0.38978893     |
| Step_1-AverageReturn    | 395            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 768            |
| Step_1-MinReturn        | 29             |
| Step_1-NumTrajs         | 1046           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 155            |
| Time                    | 4.09e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.814          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58             |
| dLoss                   | 0.0073621073   |
| n_timesteps             | 189760000      |
--------------------------------------------

 ---------------- Iteration 593 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 593           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.007090588  |
| LossBefore              | -4.722332e-09 |
| MeanKL                  | 0.0081805     |
| MeanKLBefore            | 4.470688e-09  |
| Step_0-AverageDiscou... | 162           |
| Step_0-AveragePolicyStd | 0.38960242    |
| Step_0-AverageReturn    | 412           |
| Step_0-EnvExecTime      | 24.4          |
| Step_0-MaxReturn        | 722           |
| Step_0-MinReturn        | -12.9         |
| Step_0-NumTrajs         | 928           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 138           |
| Step_1-AverageDiscou... | 162           |
| Step_1-AveragePolicyStd | 0.38953325    |
| Step_1-AverageReturn    | 395           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 786           |
| Step_1-MinReturn        | 3.87          |
| Step_1-NumTrajs         | 1029          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 156           |
| Time                    | 4.1e+04       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.798         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.0070905834  |
| n_timesteps             | 190080000     |
-------------------------------------------

 ---------------- Iteration 594 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 594            |
| ItrTime                 | 72.2           |
| LossAfter               | -0.007411454   |
| LossBefore              | -5.0774746e-10 |
| MeanKL                  | 0.007830252    |
| MeanKLBefore            | -4.4681188e-09 |
| Step_0-AverageDiscou... | 163            |
| Step_0-AveragePolicyStd | 0.38990766     |
| Step_0-AverageReturn    | 405            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 723            |
| Step_0-MinReturn        | 24.1           |
| Step_0-NumTrajs         | 958            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 166            |
| Step_1-AveragePolicyStd | 0.38989064     |
| Step_1-AverageReturn    | 408            |
| Step_1-EnvExecTime      | 24.3           |
| Step_1-MaxReturn        | 785            |
| Step_1-MinReturn        | 19.4           |
| Step_1-NumTrajs         | 1018           |
| Step_1-PolicyExecTime   | 4.37           |
| Step_1-StdReturn        | 160            |
| Time                    | 4.1e+04        |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.853          |
| Time-Sampling           | 57.5           |
| Time-TotalInner         | 58.5           |
| dLoss                   | 0.0074114534   |
| n_timesteps             | 190400000      |
--------------------------------------------

 ---------------- Iteration 595 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 595            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007199804   |
| LossBefore              | -5.0527413e-09 |
| MeanKL                  | 0.008912568    |
| MeanKLBefore            | 1.191924e-08   |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3906535      |
| Step_0-AverageReturn    | 385            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 720            |
| Step_0-MinReturn        | 11.8           |
| Step_0-NumTrajs         | 1006           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 153            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.39057636     |
| Step_1-AverageReturn    | 369            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 814            |
| Step_1-MinReturn        | 13.2           |
| Step_1-NumTrajs         | 1107           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 168            |
| Time                    | 4.11e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.843          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0071997987   |
| n_timesteps             | 190720000      |
--------------------------------------------

 ---------------- Iteration 596 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 596            |
| ItrTime                 | 72.1           |
| LossAfter               | -0.0073337145  |
| LossBefore              | -4.8316235e-09 |
| MeanKL                  | 0.008092846    |
| MeanKLBefore            | 8.939627e-09   |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.39007562     |
| Step_0-AverageReturn    | 398            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 762            |
| Step_0-MinReturn        | 18.6           |
| Step_0-NumTrajs         | 951            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 160            |
| Step_1-AverageDiscou... | 160            |
| Step_1-AveragePolicyStd | 0.39002946     |
| Step_1-AverageReturn    | 392            |
| Step_1-EnvExecTime      | 24.2           |
| Step_1-MaxReturn        | 820            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1029           |
| Step_1-PolicyExecTime   | 4.28           |
| Step_1-StdReturn        | 174            |
| Time                    | 4.12e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.83           |
| Time-Sampling           | 57.4           |
| Time-TotalInner         | 58.4           |
| dLoss                   | 0.00733371     |
| n_timesteps             | 191040000      |
--------------------------------------------

 ---------------- Iteration 597 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 597            |
| ItrTime                 | 71             |
| LossAfter               | -0.007403343   |
| LossBefore              | -1.8244002e-09 |
| MeanKL                  | 0.008194007    |
| MeanKLBefore            | 7.4501485e-09  |
| Step_0-AverageDiscou... | 170            |
| Step_0-AveragePolicyStd | 0.39033395     |
| Step_0-AverageReturn    | 421            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 679            |
| Step_0-MinReturn        | 27             |
| Step_0-NumTrajs         | 961            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 138            |
| Step_1-AverageDiscou... | 163            |
| Step_1-AveragePolicyStd | 0.39027447     |
| Step_1-AverageReturn    | 390            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 779            |
| Step_1-MinReturn        | 21.7           |
| Step_1-NumTrajs         | 1043           |
| Step_1-PolicyExecTime   | 4.32           |
| Step_1-StdReturn        | 164            |
| Time                    | 4.12e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.854          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.007403341    |
| n_timesteps             | 191360000      |
--------------------------------------------

 ---------------- Iteration 598 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 598            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0068000383  |
| LossBefore              | -5.8272778e-09 |
| MeanKL                  | 0.0071370327   |
| MeanKLBefore            | 2.0857936e-08  |
| Step_0-AverageDiscou... | 167            |
| Step_0-AveragePolicyStd | 0.39043632     |
| Step_0-AverageReturn    | 420            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 719            |
| Step_0-MinReturn        | 10.1           |
| Step_0-NumTrajs         | 951            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 159            |
| Step_1-AveragePolicyStd | 0.39030096     |
| Step_1-AverageReturn    | 380            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 795            |
| Step_1-MinReturn        | 25.3           |
| Step_1-NumTrajs         | 1063           |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 156            |
| Time                    | 4.13e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.819          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.006800032    |
| n_timesteps             | 191680000      |
--------------------------------------------

 ---------------- Iteration 599 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 599           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007189484  |
| LossBefore              | 3.4627774e-09 |
| MeanKL                  | 0.008002302   |
| MeanKLBefore            | 1.4900562e-08 |
| Step_0-AverageDiscou... | 154           |
| Step_0-AveragePolicyStd | 0.39204147    |
| Step_0-AverageReturn    | 376           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 685           |
| Step_0-MinReturn        | 19.2          |
| Step_0-NumTrajs         | 980           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 154           |
| Step_1-AverageDiscou... | 152           |
| Step_1-AveragePolicyStd | 0.39199394    |
| Step_1-AverageReturn    | 358           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 713           |
| Step_1-MinReturn        | 11.7          |
| Step_1-NumTrajs         | 1066          |
| Step_1-PolicyExecTime   | 4.02          |
| Step_1-StdReturn        | 150           |
| Time                    | 4.14e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.819         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.007189487   |
| n_timesteps             | 192000000     |
-------------------------------------------

 ---------------- Iteration 600 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 600            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.007443358   |
| LossBefore              | -6.0783605e-09 |
| MeanKL                  | 0.0081074      |
| MeanKLBefore            | -1.0686562e-12 |
| Step_0-AverageDiscou... | 156            |
| Step_0-AveragePolicyStd | 0.39134988     |
| Step_0-AverageReturn    | 376            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 763            |
| Step_0-MinReturn        | 13.1           |
| Step_0-NumTrajs         | 1028           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 168            |
| Step_1-AverageDiscou... | 161            |
| Step_1-AveragePolicyStd | 0.39132696     |
| Step_1-AverageReturn    | 384            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 791            |
| Step_1-MinReturn        | 20.2           |
| Step_1-NumTrajs         | 1064           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 170            |
| Time                    | 4.14e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.837          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.007443352    |
| n_timesteps             | 192320000      |
--------------------------------------------

 ---------------- Iteration 601 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 601           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.00729665   |
| LossBefore              | -9.184452e-09 |
| MeanKL                  | 0.008479153   |
| MeanKLBefore            | 4.469945e-09  |
| Step_0-AverageDiscou... | 161           |
| Step_0-AveragePolicyStd | 0.39095777    |
| Step_0-AverageReturn    | 390           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 750           |
| Step_0-MinReturn        | 16.6          |
| Step_0-NumTrajs         | 1006          |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 160           |
| Step_1-AveragePolicyStd | 0.3908295     |
| Step_1-AverageReturn    | 383           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 769           |
| Step_1-MinReturn        | 8.89          |
| Step_1-NumTrajs         | 1053          |
| Step_1-PolicyExecTime   | 4.26          |
| Step_1-StdReturn        | 165           |
| Time                    | 4.15e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.918         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.007296641   |
| n_timesteps             | 192640000     |
-------------------------------------------

 ---------------- Iteration 602 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 602           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.007367036  |
| LossBefore              | 7.1702013e-09 |
| MeanKL                  | 0.008423366   |
| MeanKLBefore            | -8.939461e-09 |
| Step_0-AverageDiscou... | 159           |
| Step_0-AveragePolicyStd | 0.390717      |
| Step_0-AverageReturn    | 390           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 738           |
| Step_0-MinReturn        | 14.7          |
| Step_0-NumTrajs         | 990           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 164           |
| Step_1-AveragePolicyStd | 0.39071962    |
| Step_1-AverageReturn    | 402           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 787           |
| Step_1-MinReturn        | 18.9          |
| Step_1-NumTrajs         | 1023          |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 162           |
| Time                    | 4.16e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.007367043   |
| n_timesteps             | 192960000     |
-------------------------------------------

 ---------------- Iteration 603 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 603            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0069177463  |
| LossBefore              | 2.45261e-09    |
| MeanKL                  | 0.007867739    |
| MeanKLBefore            | -1.4891753e-09 |
| Step_0-AverageDiscou... | 163            |
| Step_0-AveragePolicyStd | 0.39073482     |
| Step_0-AverageReturn    | 402            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 713            |
| Step_0-MinReturn        | 8.28           |
| Step_0-NumTrajs         | 992            |
| Step_0-PolicyExecTime   | 1.43           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 164            |
| Step_1-AveragePolicyStd | 0.3906274      |
| Step_1-AverageReturn    | 398            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 725            |
| Step_1-MinReturn        | 11.8           |
| Step_1-NumTrajs         | 1038           |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 158            |
| Time                    | 4.17e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.897          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.0069177486   |
| n_timesteps             | 193280000      |
--------------------------------------------

 ---------------- Iteration 604 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 604           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0076092556 |
| LossBefore              | 5.0055013e-09 |
| MeanKL                  | 0.008490889   |
| MeanKLBefore            | 1.7877586e-08 |
| Step_0-AverageDiscou... | 165           |
| Step_0-AveragePolicyStd | 0.3911471     |
| Step_0-AverageReturn    | 403           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 736           |
| Step_0-MinReturn        | 26.6          |
| Step_0-NumTrajs         | 981           |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 140           |
| Step_1-AverageDiscou... | 165           |
| Step_1-AveragePolicyStd | 0.3910633     |
| Step_1-AverageReturn    | 392           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 757           |
| Step_1-MinReturn        | 31.5          |
| Step_1-NumTrajs         | 1042          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 153           |
| Time                    | 4.17e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.864         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.0076092607  |
| n_timesteps             | 193600000     |
-------------------------------------------

 ---------------- Iteration 605 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 605           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.007660657  |
| LossBefore              | 7.708361e-09  |
| MeanKL                  | 0.008564939   |
| MeanKLBefore            | 1.7879563e-08 |
| Step_0-AverageDiscou... | 164           |
| Step_0-AveragePolicyStd | 0.39087257    |
| Step_0-AverageReturn    | 408           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 683           |
| Step_0-MinReturn        | 4.58          |
| Step_0-NumTrajs         | 938           |
| Step_0-PolicyExecTime   | 1.41          |
| Step_0-StdReturn        | 137           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.39071727    |
| Step_1-AverageReturn    | 401           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 763           |
| Step_1-MinReturn        | 39.1          |
| Step_1-NumTrajs         | 1038          |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 159           |
| Time                    | 4.18e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.007660665   |
| n_timesteps             | 193920000     |
-------------------------------------------

 ---------------- Iteration 606 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 606            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0072249533  |
| LossBefore              | -1.0944767e-08 |
| MeanKL                  | 0.008823766    |
| MeanKLBefore            | -1.4904031e-09 |
| Step_0-AverageDiscou... | 162            |
| Step_0-AveragePolicyStd | 0.39134926     |
| Step_0-AverageReturn    | 413            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 770            |
| Step_0-MinReturn        | 16             |
| Step_0-NumTrajs         | 934            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 151            |
| Step_1-AverageDiscou... | 162            |
| Step_1-AveragePolicyStd | 0.39142182     |
| Step_1-AverageReturn    | 399            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 737            |
| Step_1-MinReturn        | 18.3           |
| Step_1-NumTrajs         | 1022           |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 163            |
| Time                    | 4.19e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.825          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.007224942    |
| n_timesteps             | 194240000      |
--------------------------------------------

 ---------------- Iteration 607 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 607            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0075263865  |
| LossBefore              | -1.2552718e-08 |
| MeanKL                  | 0.0082299765   |
| MeanKLBefore            | -4.4714987e-09 |
| Step_0-AverageDiscou... | 164            |
| Step_0-AveragePolicyStd | 0.39121592     |
| Step_0-AverageReturn    | 402            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 835            |
| Step_0-MinReturn        | 15.3           |
| Step_0-NumTrajs         | 992            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 148            |
| Step_1-AverageDiscou... | 162            |
| Step_1-AveragePolicyStd | 0.39118955     |
| Step_1-AverageReturn    | 381            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 751            |
| Step_1-MinReturn        | 13.5           |
| Step_1-NumTrajs         | 1096           |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 164            |
| Time                    | 4.19e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.862          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.007526374    |
| n_timesteps             | 194560000      |
--------------------------------------------

 ---------------- Iteration 608 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 608           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.007976744  |
| LossBefore              | 4.3430295e-09 |
| MeanKL                  | 0.008882539   |
| MeanKLBefore            | 2.9813734e-09 |
| Step_0-AverageDiscou... | 169           |
| Step_0-AveragePolicyStd | 0.3913702     |
| Step_0-AverageReturn    | 427           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 794           |
| Step_0-MinReturn        | 9.66          |
| Step_0-NumTrajs         | 920           |
| Step_0-PolicyExecTime   | 1.4           |
| Step_0-StdReturn        | 131           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.39130715    |
| Step_1-AverageReturn    | 417           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 754           |
| Step_1-MinReturn        | 19.3          |
| Step_1-NumTrajs         | 1007          |
| Step_1-PolicyExecTime   | 4.62          |
| Step_1-StdReturn        | 154           |
| Time                    | 4.2e+04       |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.876         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.007976749   |
| n_timesteps             | 194880000     |
-------------------------------------------

 ---------------- Iteration 609 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 609            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0072585978  |
| LossBefore              | -8.9515e-09    |
| MeanKL                  | 0.008140824    |
| MeanKLBefore            | 1.04299955e-08 |
| Step_0-AverageDiscou... | 164            |
| Step_0-AveragePolicyStd | 0.3907166      |
| Step_0-AverageReturn    | 397            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 801            |
| Step_0-MinReturn        | 9.28           |
| Step_0-NumTrajs         | 1002           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 156            |
| Step_1-AverageDiscou... | 161            |
| Step_1-AveragePolicyStd | 0.39060968     |
| Step_1-AverageReturn    | 380            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 771            |
| Step_1-MinReturn        | 4.8            |
| Step_1-NumTrajs         | 1078           |
| Step_1-PolicyExecTime   | 4.26           |
| Step_1-StdReturn        | 164            |
| Time                    | 4.21e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007258589    |
| n_timesteps             | 195200000      |
--------------------------------------------

 ---------------- Iteration 610 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 610            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0073225396  |
| LossBefore              | -4.2485704e-09 |
| MeanKL                  | 0.0077641467   |
| MeanKLBefore            | 4.4693076e-09  |
| Step_0-AverageDiscou... | 160            |
| Step_0-AveragePolicyStd | 0.3905368      |
| Step_0-AverageReturn    | 400            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 707            |
| Step_0-MinReturn        | 16.3           |
| Step_0-NumTrajs         | 961            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 144            |
| Step_1-AverageDiscou... | 159            |
| Step_1-AveragePolicyStd | 0.39047816     |
| Step_1-AverageReturn    | 381            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 751            |
| Step_1-MinReturn        | 13.9           |
| Step_1-NumTrajs         | 1073           |
| Step_1-PolicyExecTime   | 3.98           |
| Step_1-StdReturn        | 156            |
| Time                    | 4.22e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 14.2           |
| Time-OuterStep          | 14.2           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0073225354   |
| n_timesteps             | 195520000      |
--------------------------------------------

 ---------------- Iteration 611 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 611           |
| ItrTime                 | 74.6          |
| LossAfter               | -0.006970407  |
| LossBefore              | 1.3028716e-08 |
| MeanKL                  | 0.008249393   |
| MeanKLBefore            | 7.752021e-13  |
| Step_0-AverageDiscou... | 163           |
| Step_0-AveragePolicyStd | 0.3905901     |
| Step_0-AverageReturn    | 399           |
| Step_0-EnvExecTime      | 27.6          |
| Step_0-MaxReturn        | 710           |
| Step_0-MinReturn        | 11.9          |
| Step_0-NumTrajs         | 986           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 160           |
| Step_1-AveragePolicyStd | 0.39057553    |
| Step_1-AverageReturn    | 377           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 796           |
| Step_1-MinReturn        | 7.77          |
| Step_1-NumTrajs         | 1098          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 170           |
| Time                    | 4.22e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.826         |
| Time-Sampling           | 60            |
| Time-TotalInner         | 61.1          |
| dLoss                   | 0.00697042    |
| n_timesteps             | 195840000     |
-------------------------------------------

 ---------------- Iteration 612 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 612            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007217101   |
| LossBefore              | 2.090137e-09   |
| MeanKL                  | 0.008376701    |
| MeanKLBefore            | -1.4886368e-09 |
| Step_0-AverageDiscou... | 164            |
| Step_0-AveragePolicyStd | 0.39055336     |
| Step_0-AverageReturn    | 406            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 777            |
| Step_0-MinReturn        | 22.3           |
| Step_0-NumTrajs         | 946            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 130            |
| Step_1-AverageDiscou... | 163            |
| Step_1-AveragePolicyStd | 0.39049098     |
| Step_1-AverageReturn    | 394            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 729            |
| Step_1-MinReturn        | 21.5           |
| Step_1-NumTrajs         | 1022           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 144            |
| Time                    | 4.23e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.81           |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0072171027   |
| n_timesteps             | 196160000      |
--------------------------------------------

 ---------------- Iteration 613 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 613            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.0066420017  |
| LossBefore              | -1.0042348e-09 |
| MeanKL                  | 0.008053682    |
| MeanKLBefore            | 8.940735e-09   |
| Step_0-AverageDiscou... | 165            |
| Step_0-AveragePolicyStd | 0.39127392     |
| Step_0-AverageReturn    | 422            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 726            |
| Step_0-MinReturn        | 7.38           |
| Step_0-NumTrajs         | 908            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39118016     |
| Step_1-AverageReturn    | 407            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 746            |
| Step_1-MinReturn        | 22.7           |
| Step_1-NumTrajs         | 1014           |
| Step_1-PolicyExecTime   | 4.37           |
| Step_1-StdReturn        | 156            |
| Time                    | 4.24e+04       |
| Time-InnerStep          | 0.191          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.825          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58.1           |
| dLoss                   | 0.0066420007   |
| n_timesteps             | 196480000      |
--------------------------------------------

 ---------------- Iteration 614 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 614           |
| ItrTime                 | 70            |
| LossAfter               | -0.0071353465 |
| LossBefore              | 4.621921e-09  |
| MeanKL                  | 0.007832034   |
| MeanKLBefore            | 1.4923988e-09 |
| Step_0-AverageDiscou... | 163           |
| Step_0-AveragePolicyStd | 0.3915554     |
| Step_0-AverageReturn    | 393           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 771           |
| Step_0-MinReturn        | 28.9          |
| Step_0-NumTrajs         | 1017          |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 152           |
| Step_1-AverageDiscou... | 161           |
| Step_1-AveragePolicyStd | 0.39152107    |
| Step_1-AverageReturn    | 376           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 850           |
| Step_1-MinReturn        | 15.2          |
| Step_1-NumTrajs         | 1111          |
| Step_1-PolicyExecTime   | 4.24          |
| Step_1-StdReturn        | 163           |
| Time                    | 4.24e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.921         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007135351   |
| n_timesteps             | 196800000     |
-------------------------------------------

 ---------------- Iteration 615 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 615           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0072606443 |
| LossBefore              | 4.446167e-09  |
| MeanKL                  | 0.007504221   |
| MeanKLBefore            | 1.638939e-08  |
| Step_0-AverageDiscou... | 160           |
| Step_0-AveragePolicyStd | 0.3912201     |
| Step_0-AverageReturn    | 391           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 760           |
| Step_0-MinReturn        | 22.5          |
| Step_0-NumTrajs         | 992           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 145           |
| Step_1-AverageDiscou... | 162           |
| Step_1-AveragePolicyStd | 0.39105448    |
| Step_1-AverageReturn    | 390           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 786           |
| Step_1-MinReturn        | 31.9          |
| Step_1-NumTrajs         | 1038          |
| Step_1-PolicyExecTime   | 4.33          |
| Step_1-StdReturn        | 158           |
| Time                    | 4.25e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.847         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007260649   |
| n_timesteps             | 197120000     |
-------------------------------------------

 ---------------- Iteration 616 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 616           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0074364617 |
| LossBefore              | 6.977335e-09  |
| MeanKL                  | 0.008493398   |
| MeanKLBefore            | 2.979958e-09  |
| Step_0-AverageDiscou... | 170           |
| Step_0-AveragePolicyStd | 0.39142558    |
| Step_0-AverageReturn    | 415           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 823           |
| Step_0-MinReturn        | 24.5          |
| Step_0-NumTrajs         | 994           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 145           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.39136073    |
| Step_1-AverageReturn    | 407           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 788           |
| Step_1-MinReturn        | 29.1          |
| Step_1-NumTrajs         | 1048          |
| Step_1-PolicyExecTime   | 4.31          |
| Step_1-StdReturn        | 157           |
| Time                    | 4.26e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.864         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0074364687  |
| n_timesteps             | 197440000     |
-------------------------------------------

 ---------------- Iteration 617 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 617            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007025873   |
| LossBefore              | -4.7648854e-09 |
| MeanKL                  | 0.0075745387   |
| MeanKLBefore            | 2.981362e-09   |
| Step_0-AverageDiscou... | 160            |
| Step_0-AveragePolicyStd | 0.39209107     |
| Step_0-AverageReturn    | 390            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 749            |
| Step_0-MinReturn        | 18.4           |
| Step_0-NumTrajs         | 991            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 152            |
| Step_1-AverageDiscou... | 164            |
| Step_1-AveragePolicyStd | 0.39209184     |
| Step_1-AverageReturn    | 398            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 797            |
| Step_1-MinReturn        | 18.9           |
| Step_1-NumTrajs         | 1025           |
| Step_1-PolicyExecTime   | 4.38           |
| Step_1-StdReturn        | 164            |
| Time                    | 4.27e+04       |
| Time-InnerStep          | 0.169          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.876          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007025868    |
| n_timesteps             | 197760000      |
--------------------------------------------

 ---------------- Iteration 618 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 618           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0074906796 |
| LossBefore              | 2.427898e-09  |
| MeanKL                  | 0.00808345    |
| MeanKLBefore            | 7.45134e-09   |
| Step_0-AverageDiscou... | 166           |
| Step_0-AveragePolicyStd | 0.39206702    |
| Step_0-AverageReturn    | 398           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 766           |
| Step_0-MinReturn        | 10.8          |
| Step_0-NumTrajs         | 1022          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 152           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.392001      |
| Step_1-AverageReturn    | 402           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 761           |
| Step_1-MinReturn        | 9.66          |
| Step_1-NumTrajs         | 1042          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 164           |
| Time                    | 4.27e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.816         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.007490682   |
| n_timesteps             | 198080000     |
-------------------------------------------

 ---------------- Iteration 619 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 619            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.007125648   |
| LossBefore              | -1.1764576e-08 |
| MeanKL                  | 0.0077352626   |
| MeanKLBefore            | 4.47083e-09    |
| Step_0-AverageDiscou... | 162            |
| Step_0-AveragePolicyStd | 0.39139178     |
| Step_0-AverageReturn    | 411            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 757            |
| Step_0-MinReturn        | 23             |
| Step_0-NumTrajs         | 939            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 163            |
| Step_1-AveragePolicyStd | 0.3914417      |
| Step_1-AverageReturn    | 393            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 785            |
| Step_1-MinReturn        | 20.7           |
| Step_1-NumTrajs         | 1064           |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 167            |
| Time                    | 4.28e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.855          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.0071256366   |
| n_timesteps             | 198400000      |
--------------------------------------------

 ---------------- Iteration 620 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 620            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.006858735   |
| LossBefore              | -2.7029166e-09 |
| MeanKL                  | 0.007626109    |
| MeanKLBefore            | -1.4889878e-09 |
| Step_0-AverageDiscou... | 168            |
| Step_0-AveragePolicyStd | 0.39177877     |
| Step_0-AverageReturn    | 419            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 826            |
| Step_0-MinReturn        | 20             |
| Step_0-NumTrajs         | 973            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 142            |
| Step_1-AverageDiscou... | 162            |
| Step_1-AveragePolicyStd | 0.3917216      |
| Step_1-AverageReturn    | 386            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 791            |
| Step_1-MinReturn        | 27.9           |
| Step_1-NumTrajs         | 1081           |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 161            |
| Time                    | 4.29e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.874          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.006858732    |
| n_timesteps             | 198720000      |
--------------------------------------------

 ---------------- Iteration 621 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 621            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0068448693  |
| LossBefore              | -9.224868e-10  |
| MeanKL                  | 0.007922408    |
| MeanKLBefore            | -2.9802643e-09 |
| Step_0-AverageDiscou... | 159            |
| Step_0-AveragePolicyStd | 0.3922603      |
| Step_0-AverageReturn    | 383            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 836            |
| Step_0-MinReturn        | 5.41           |
| Step_0-NumTrajs         | 1019           |
| Step_0-PolicyExecTime   | 1.45           |
| Step_0-StdReturn        | 156            |
| Step_1-AverageDiscou... | 157            |
| Step_1-AveragePolicyStd | 0.3922808      |
| Step_1-AverageReturn    | 363            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 734            |
| Step_1-MinReturn        | 23.9           |
| Step_1-NumTrajs         | 1117           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 158            |
| Time                    | 4.29e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.907          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0068448684   |
| n_timesteps             | 199040000      |
--------------------------------------------

 ---------------- Iteration 622 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 622           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0066966712 |
| LossBefore              | 3.1989167e-09 |
| MeanKL                  | 0.007459766   |
| MeanKLBefore            | 1.4899231e-08 |
| Step_0-AverageDiscou... | 154           |
| Step_0-AveragePolicyStd | 0.39289403    |
| Step_0-AverageReturn    | 377           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 752           |
| Step_0-MinReturn        | 10.8          |
| Step_0-NumTrajs         | 1008          |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.39292684    |
| Step_1-AverageReturn    | 371           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 818           |
| Step_1-MinReturn        | 10.7          |
| Step_1-NumTrajs         | 1075          |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 156           |
| Time                    | 4.3e+04       |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.889         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0066966745  |
| n_timesteps             | 199360000     |
-------------------------------------------

 ---------------- Iteration 623 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 623           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.007111366  |
| LossBefore              | 2.5715046e-09 |
| MeanKL                  | 0.008078459   |
| MeanKLBefore            | 5.9600134e-09 |
| Step_0-AverageDiscou... | 152           |
| Step_0-AveragePolicyStd | 0.39340654    |
| Step_0-AverageReturn    | 366           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 704           |
| Step_0-MinReturn        | 13            |
| Step_0-NumTrajs         | 1036          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 157           |
| Step_1-AveragePolicyStd | 0.39334995    |
| Step_1-AverageReturn    | 374           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 753           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1071          |
| Step_1-PolicyExecTime   | 4.25          |
| Step_1-StdReturn        | 174           |
| Time                    | 4.31e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.861         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0071113687  |
| n_timesteps             | 199680000     |
-------------------------------------------

 ---------------- Iteration 624 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 624            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.006959811   |
| LossBefore              | -3.0761136e-09 |
| MeanKL                  | 0.008525966    |
| MeanKLBefore            | -4.469633e-09  |
| Step_0-AverageDiscou... | 168            |
| Step_0-AveragePolicyStd | 0.39344177     |
| Step_0-AverageReturn    | 413            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 760            |
| Step_0-MinReturn        | 25             |
| Step_0-NumTrajs         | 981            |
| Step_0-PolicyExecTime   | 1.41           |
| Step_0-StdReturn        | 145            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39337155     |
| Step_1-AverageReturn    | 396            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 754            |
| Step_1-MinReturn        | 14.9           |
| Step_1-NumTrajs         | 1041           |
| Step_1-PolicyExecTime   | 4.3            |
| Step_1-StdReturn        | 157            |
| Time                    | 4.32e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.863          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0069598076   |
| n_timesteps             | 200000000      |
--------------------------------------------

 ---------------- Iteration 625 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 625            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0071298974  |
| LossBefore              | -5.7501524e-09 |
| MeanKL                  | 0.008054511    |
| MeanKLBefore            | 1.638889e-08   |
| Step_0-AverageDiscou... | 165            |
| Step_0-AveragePolicyStd | 0.39351612     |
| Step_0-AverageReturn    | 403            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 773            |
| Step_0-MinReturn        | 8.72           |
| Step_0-NumTrajs         | 986            |
| Step_0-PolicyExecTime   | 1.41           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 166            |
| Step_1-AveragePolicyStd | 0.39345187     |
| Step_1-AverageReturn    | 396            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 857            |
| Step_1-MinReturn        | 12.7           |
| Step_1-NumTrajs         | 1062           |
| Step_1-PolicyExecTime   | 4.44           |
| Step_1-StdReturn        | 159            |
| Time                    | 4.32e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.94           |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.007129892    |
| n_timesteps             | 200320000      |
--------------------------------------------

 ---------------- Iteration 626 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 626            |
| ItrTime                 | 72             |
| LossAfter               | -0.0072010057  |
| LossBefore              | 8.651993e-09   |
| MeanKL                  | 0.008252157    |
| MeanKLBefore            | -1.4893512e-09 |
| Step_0-AverageDiscou... | 163            |
| Step_0-AveragePolicyStd | 0.393789       |
| Step_0-AverageReturn    | 404            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 757            |
| Step_0-MinReturn        | 21.9           |
| Step_0-NumTrajs         | 978            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 149            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39373034     |
| Step_1-AverageReturn    | 409            |
| Step_1-EnvExecTime      | 24.5           |
| Step_1-MaxReturn        | 784            |
| Step_1-MinReturn        | 21.2           |
| Step_1-NumTrajs         | 1003           |
| Step_1-PolicyExecTime   | 4.28           |
| Step_1-StdReturn        | 160            |
| Time                    | 4.33e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.829          |
| Time-Sampling           | 57.3           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.0072010146   |
| n_timesteps             | 200640000      |
--------------------------------------------

 ---------------- Iteration 627 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 627            |
| ItrTime                 | 71             |
| LossAfter               | -0.007385265   |
| LossBefore              | -4.5145123e-09 |
| MeanKL                  | 0.007836619    |
| MeanKLBefore            | 7.197798e-13   |
| Step_0-AverageDiscou... | 161            |
| Step_0-AveragePolicyStd | 0.39452022     |
| Step_0-AverageReturn    | 399            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 731            |
| Step_0-MinReturn        | 15.3           |
| Step_0-NumTrajs         | 990            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 161            |
| Step_1-AverageDiscou... | 159            |
| Step_1-AveragePolicyStd | 0.39449665     |
| Step_1-AverageReturn    | 379            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 803            |
| Step_1-MinReturn        | 20.8           |
| Step_1-NumTrajs         | 1060           |
| Step_1-PolicyExecTime   | 4.29           |
| Step_1-StdReturn        | 158            |
| Time                    | 4.34e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.859          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0073852604   |
| n_timesteps             | 200960000      |
--------------------------------------------

 ---------------- Iteration 628 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 628           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0077571003 |
| LossBefore              | 3.3141816e-09 |
| MeanKL                  | 0.00854957    |
| MeanKLBefore            | 1.488668e-09  |
| Step_0-AverageDiscou... | 169           |
| Step_0-AveragePolicyStd | 0.39480177    |
| Step_0-AverageReturn    | 422           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 812           |
| Step_0-MinReturn        | 20.5          |
| Step_0-NumTrajs         | 959           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 142           |
| Step_1-AverageDiscou... | 168           |
| Step_1-AveragePolicyStd | 0.39465958    |
| Step_1-AverageReturn    | 404           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 800           |
| Step_1-MinReturn        | 25.5          |
| Step_1-NumTrajs         | 1055          |
| Step_1-PolicyExecTime   | 4.27          |
| Step_1-StdReturn        | 160           |
| Time                    | 4.34e+04      |
| Time-InnerStep          | 0.168         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.846         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0077571035  |
| n_timesteps             | 201280000     |
-------------------------------------------

 ---------------- Iteration 629 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 629            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.0072266147  |
| LossBefore              | -7.3836057e-09 |
| MeanKL                  | 0.008134919    |
| MeanKLBefore            | 8.941576e-09   |
| Step_0-AverageDiscou... | 167            |
| Step_0-AveragePolicyStd | 0.39502597     |
| Step_0-AverageReturn    | 414            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 729            |
| Step_0-MinReturn        | 7.75           |
| Step_0-NumTrajs         | 976            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 147            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39487565     |
| Step_1-AverageReturn    | 409            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 761            |
| Step_1-MinReturn        | 6.54           |
| Step_1-NumTrajs         | 1008           |
| Step_1-PolicyExecTime   | 4.43           |
| Step_1-StdReturn        | 163            |
| Time                    | 4.35e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.888          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0072266073   |
| n_timesteps             | 201600000      |
--------------------------------------------

 ---------------- Iteration 630 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 630           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.006840244  |
| LossBefore              | 5.2649276e-09 |
| MeanKL                  | 0.0070599467  |
| MeanKLBefore            | 4.468135e-09  |
| Step_0-AverageDiscou... | 168           |
| Step_0-AveragePolicyStd | 0.39556256    |
| Step_0-AverageReturn    | 410           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 770           |
| Step_0-MinReturn        | 11.8          |
| Step_0-NumTrajs         | 1022          |
| Step_0-PolicyExecTime   | 1.4           |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 159           |
| Step_1-AveragePolicyStd | 0.395562      |
| Step_1-AverageReturn    | 375           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 730           |
| Step_1-MinReturn        | 4.87          |
| Step_1-NumTrajs         | 1116          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 175           |
| Time                    | 4.36e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.845         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.006840249   |
| n_timesteps             | 201920000     |
-------------------------------------------

 ---------------- Iteration 631 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 631           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.007528886  |
| LossBefore              | 2.728917e-09  |
| MeanKL                  | 0.0076868287  |
| MeanKLBefore            | 1.6389818e-08 |
| Step_0-AverageDiscou... | 174           |
| Step_0-AveragePolicyStd | 0.3958059     |
| Step_0-AverageReturn    | 435           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 807           |
| Step_0-MinReturn        | 20            |
| Step_0-NumTrajs         | 983           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.39565486    |
| Step_1-AverageReturn    | 398           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 825           |
| Step_1-MinReturn        | 7.05          |
| Step_1-NumTrajs         | 1071          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 163           |
| Time                    | 4.36e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.877         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007528889   |
| n_timesteps             | 202240000     |
-------------------------------------------

 ---------------- Iteration 632 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 632           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0077851713 |
| LossBefore              | 8.0364515e-09 |
| MeanKL                  | 0.008024123   |
| MeanKLBefore            | 5.9595866e-09 |
| Step_0-AverageDiscou... | 165           |
| Step_0-AveragePolicyStd | 0.39602697    |
| Step_0-AverageReturn    | 398           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 744           |
| Step_0-MinReturn        | 21.7          |
| Step_0-NumTrajs         | 1025          |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 169           |
| Step_1-AveragePolicyStd | 0.39597538    |
| Step_1-AverageReturn    | 408           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 762           |
| Step_1-MinReturn        | -2.05         |
| Step_1-NumTrajs         | 1042          |
| Step_1-PolicyExecTime   | 4.31          |
| Step_1-StdReturn        | 151           |
| Time                    | 4.37e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.857         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007785179   |
| n_timesteps             | 202560000     |
-------------------------------------------

 ---------------- Iteration 633 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 633           |
| ItrTime                 | 69            |
| LossAfter               | -0.0073070317 |
| LossBefore              | 4.724453e-09  |
| MeanKL                  | 0.008304365   |
| MeanKLBefore            | 5.9595076e-09 |
| Step_0-AverageDiscou... | 165           |
| Step_0-AveragePolicyStd | 0.3957868     |
| Step_0-AverageReturn    | 400           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 759           |
| Step_0-MinReturn        | 24.8          |
| Step_0-NumTrajs         | 1014          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 155           |
| Step_1-AverageDiscou... | 165           |
| Step_1-AveragePolicyStd | 0.3956887     |
| Step_1-AverageReturn    | 392           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 760           |
| Step_1-MinReturn        | 18.1          |
| Step_1-NumTrajs         | 1077          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 162           |
| Time                    | 4.38e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.3          |
| dLoss                   | 0.0073070363  |
| n_timesteps             | 202880000     |
-------------------------------------------

 ---------------- Iteration 634 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 634            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.0074068173  |
| LossBefore              | -4.1981663e-09 |
| MeanKL                  | 0.007491122    |
| MeanKLBefore            | 1.1918225e-08  |
| Step_0-AverageDiscou... | 168            |
| Step_0-AveragePolicyStd | 0.39657626     |
| Step_0-AverageReturn    | 424            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 692            |
| Step_0-MinReturn        | 7.72           |
| Step_0-NumTrajs         | 949            |
| Step_0-PolicyExecTime   | 1.4            |
| Step_0-StdReturn        | 149            |
| Step_1-AverageDiscou... | 166            |
| Step_1-AveragePolicyStd | 0.39647228     |
| Step_1-AverageReturn    | 407            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 771            |
| Step_1-MinReturn        | 2.46           |
| Step_1-NumTrajs         | 1036           |
| Step_1-PolicyExecTime   | 4.34           |
| Step_1-StdReturn        | 176            |
| Time                    | 4.39e+04       |
| Time-InnerStep          | 0.169          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.915          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007406813    |
| n_timesteps             | 203200000      |
--------------------------------------------

 ---------------- Iteration 635 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 635            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.0070707924  |
| LossBefore              | -2.1108497e-09 |
| MeanKL                  | 0.007791622    |
| MeanKLBefore            | 7.449261e-09   |
| Step_0-AverageDiscou... | 163            |
| Step_0-AveragePolicyStd | 0.39643902     |
| Step_0-AverageReturn    | 393            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 701            |
| Step_0-MinReturn        | 21.8           |
| Step_0-NumTrajs         | 1023           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 157            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39623484     |
| Step_1-AverageReturn    | 395            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 866            |
| Step_1-MinReturn        | 17.3           |
| Step_1-NumTrajs         | 1070           |
| Step_1-PolicyExecTime   | 4.3            |
| Step_1-StdReturn        | 174            |
| Time                    | 4.39e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.852          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.00707079     |
| n_timesteps             | 203520000      |
--------------------------------------------

 ---------------- Iteration 636 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 636            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.00730933    |
| LossBefore              | 2.4777498e-09  |
| MeanKL                  | 0.0076961583   |
| MeanKLBefore            | -4.3982596e-13 |
| Step_0-AverageDiscou... | 168            |
| Step_0-AveragePolicyStd | 0.39625034     |
| Step_0-AverageReturn    | 420            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 801            |
| Step_0-MinReturn        | 19             |
| Step_0-NumTrajs         | 976            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 152            |
| Step_1-AverageDiscou... | 168            |
| Step_1-AveragePolicyStd | 0.39611772     |
| Step_1-AverageReturn    | 409            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 783            |
| Step_1-MinReturn        | 12.1           |
| Step_1-NumTrajs         | 1049           |
| Step_1-PolicyExecTime   | 4.67           |
| Step_1-StdReturn        | 173            |
| Time                    | 4.4e+04        |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.866          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.0073093325   |
| n_timesteps             | 203840000      |
--------------------------------------------

 ---------------- Iteration 637 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 637            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.007035484   |
| LossBefore              | -1.0239285e-08 |
| MeanKL                  | 0.008739912    |
| MeanKLBefore            | -4.4706585e-09 |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.39615685     |
| Step_0-AverageReturn    | 430            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 760            |
| Step_0-MinReturn        | 13.1           |
| Step_0-NumTrajs         | 960            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 144            |
| Step_1-AverageDiscou... | 167            |
| Step_1-AveragePolicyStd | 0.39617538     |
| Step_1-AverageReturn    | 407            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 831            |
| Step_1-MinReturn        | 28             |
| Step_1-NumTrajs         | 1042           |
| Step_1-PolicyExecTime   | 4.3            |
| Step_1-StdReturn        | 168            |
| Time                    | 4.41e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.86           |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007035474    |
| n_timesteps             | 204160000      |
--------------------------------------------

 ---------------- Iteration 638 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 638           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0072516175 |
| LossBefore              | -3.523099e-09 |
| MeanKL                  | 0.008204473   |
| MeanKLBefore            | -5.96192e-09  |
| Step_0-AverageDiscou... | 164           |
| Step_0-AveragePolicyStd | 0.39671886    |
| Step_0-AverageReturn    | 403           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 772           |
| Step_0-MinReturn        | 12.4          |
| Step_0-NumTrajs         | 963           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 143           |
| Step_1-AverageDiscou... | 164           |
| Step_1-AveragePolicyStd | 0.3966756     |
| Step_1-AverageReturn    | 390           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 751           |
| Step_1-MinReturn        | 19.1          |
| Step_1-NumTrajs         | 1075          |
| Step_1-PolicyExecTime   | 4.37          |
| Step_1-StdReturn        | 161           |
| Time                    | 4.41e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.9          |
| Time-OuterStep          | 13.9          |
| Time-SampleProc         | 0.957         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007251614   |
| n_timesteps             | 204480000     |
-------------------------------------------

 ---------------- Iteration 639 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 639           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.0076475265 |
| LossBefore              | -7.912449e-09 |
| MeanKL                  | 0.009373046   |
| MeanKLBefore            | 2.9790486e-09 |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.39654317    |
| Step_0-AverageReturn    | 431           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 731           |
| Step_0-MinReturn        | 22            |
| Step_0-NumTrajs         | 945           |
| Step_0-PolicyExecTime   | 1.49          |
| Step_0-StdReturn        | 141           |
| Step_1-AverageDiscou... | 176           |
| Step_1-AveragePolicyStd | 0.39645615    |
| Step_1-AverageReturn    | 440           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 835           |
| Step_1-MinReturn        | 34.2          |
| Step_1-NumTrajs         | 984           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 144           |
| Time                    | 4.42e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.846         |
| Time-Sampling           | 57            |
| Time-TotalInner         | 58            |
| dLoss                   | 0.0076475185  |
| n_timesteps             | 204800000     |
-------------------------------------------

 ---------------- Iteration 640 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 640           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007497343  |
| LossBefore              | 4.1456873e-09 |
| MeanKL                  | 0.008219963   |
| MeanKLBefore            | 7.450411e-09  |
| Step_0-AverageDiscou... | 163           |
| Step_0-AveragePolicyStd | 0.3967977     |
| Step_0-AverageReturn    | 404           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 742           |
| Step_0-MinReturn        | -0.648        |
| Step_0-NumTrajs         | 996           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 162           |
| Step_1-AveragePolicyStd | 0.39675835    |
| Step_1-AverageReturn    | 395           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 824           |
| Step_1-MinReturn        | 18.3          |
| Step_1-NumTrajs         | 1043          |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 173           |
| Time                    | 4.43e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007497347   |
| n_timesteps             | 205120000     |
-------------------------------------------

 ---------------- Iteration 641 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 641           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.007457158  |
| LossBefore              | 1.1017631e-08 |
| MeanKL                  | 0.007727987   |
| MeanKLBefore            | 1.1920164e-08 |
| Step_0-AverageDiscou... | 164           |
| Step_0-AveragePolicyStd | 0.39704564    |
| Step_0-AverageReturn    | 412           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 723           |
| Step_0-MinReturn        | 11.1          |
| Step_0-NumTrajs         | 930           |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 149           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.39702976    |
| Step_1-AverageReturn    | 408           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 789           |
| Step_1-MinReturn        | 17.6          |
| Step_1-NumTrajs         | 1015          |
| Step_1-PolicyExecTime   | 4.35          |
| Step_1-StdReturn        | 165           |
| Time                    | 4.44e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.839         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0074571692  |
| n_timesteps             | 205440000     |
-------------------------------------------

 ---------------- Iteration 642 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 642           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007847555  |
| LossBefore              | 2.5453845e-10 |
| MeanKL                  | 0.008760865   |
| MeanKLBefore            | 4.468819e-09  |
| Step_0-AverageDiscou... | 166           |
| Step_0-AveragePolicyStd | 0.39700872    |
| Step_0-AverageReturn    | 416           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 770           |
| Step_0-MinReturn        | 12            |
| Step_0-NumTrajs         | 962           |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 148           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.3969486     |
| Step_1-AverageReturn    | 404           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 778           |
| Step_1-MinReturn        | 17.4          |
| Step_1-NumTrajs         | 1054          |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 164           |
| Time                    | 4.44e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.885         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007847555   |
| n_timesteps             | 205760000     |
-------------------------------------------

 ---------------- Iteration 643 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 643            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007025306   |
| LossBefore              | -1.1634727e-08 |
| MeanKL                  | 0.008115971    |
| MeanKLBefore            | 7.449972e-09   |
| Step_0-AverageDiscou... | 161            |
| Step_0-AveragePolicyStd | 0.39712065     |
| Step_0-AverageReturn    | 393            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 767            |
| Step_0-MinReturn        | 7.12           |
| Step_0-NumTrajs         | 1028           |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 163            |
| Step_1-AveragePolicyStd | 0.3970772      |
| Step_1-AverageReturn    | 386            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 777            |
| Step_1-MinReturn        | 7.02           |
| Step_1-NumTrajs         | 1090           |
| Step_1-PolicyExecTime   | 4.41           |
| Step_1-StdReturn        | 173            |
| Time                    | 4.45e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.918          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0070252945   |
| n_timesteps             | 206080000      |
--------------------------------------------

 ---------------- Iteration 644 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 644            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0070845573  |
| LossBefore              | 1.1104404e-09  |
| MeanKL                  | 0.007888478    |
| MeanKLBefore            | -7.4481674e-09 |
| Step_0-AverageDiscou... | 168            |
| Step_0-AveragePolicyStd | 0.39629656     |
| Step_0-AverageReturn    | 426            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 756            |
| Step_0-MinReturn        | 15.2           |
| Step_0-NumTrajs         | 956            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 157            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39629006     |
| Step_1-AverageReturn    | 405            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 785            |
| Step_1-MinReturn        | 14.4           |
| Step_1-NumTrajs         | 1029           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 166            |
| Time                    | 4.46e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0070845583   |
| n_timesteps             | 206400000      |
--------------------------------------------

 ---------------- Iteration 645 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 645            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.007049448   |
| LossBefore              | -2.7091187e-09 |
| MeanKL                  | 0.0076028584   |
| MeanKLBefore            | -1.4920779e-09 |
| Step_0-AverageDiscou... | 162            |
| Step_0-AveragePolicyStd | 0.3962704      |
| Step_0-AverageReturn    | 396            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 744            |
| Step_0-MinReturn        | 7.84           |
| Step_0-NumTrajs         | 1026           |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 165            |
| Step_1-AveragePolicyStd | 0.39624837     |
| Step_1-AverageReturn    | 399            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 739            |
| Step_1-MinReturn        | 22.2           |
| Step_1-NumTrajs         | 1064           |
| Step_1-PolicyExecTime   | 4.29           |
| Step_1-StdReturn        | 167            |
| Time                    | 4.46e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.869          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007049445    |
| n_timesteps             | 206720000      |
--------------------------------------------

 ---------------- Iteration 646 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 646           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0074892277 |
| LossBefore              | 3.10702e-09   |
| MeanKL                  | 0.008923279   |
| MeanKLBefore            | 7.7591265e-13 |
| Step_0-AverageDiscou... | 164           |
| Step_0-AveragePolicyStd | 0.39618838    |
| Step_0-AverageReturn    | 402           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 767           |
| Step_0-MinReturn        | 11.2          |
| Step_0-NumTrajs         | 1035          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 165           |
| Step_1-AveragePolicyStd | 0.39614287    |
| Step_1-AverageReturn    | 393           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 810           |
| Step_1-MinReturn        | 10.4          |
| Step_1-NumTrajs         | 1098          |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 175           |
| Time                    | 4.47e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.887         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007489231   |
| n_timesteps             | 207040000     |
-------------------------------------------

 ---------------- Iteration 647 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 647           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0076101897 |
| LossBefore              | 2.9468055e-09 |
| MeanKL                  | 0.008621557   |
| MeanKLBefore            | 8.939015e-09  |
| Step_0-AverageDiscou... | 158           |
| Step_0-AveragePolicyStd | 0.3961083     |
| Step_0-AverageReturn    | 378           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 742           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1055          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 166           |
| Step_1-AverageDiscou... | 159           |
| Step_1-AveragePolicyStd | 0.39595613    |
| Step_1-AverageReturn    | 375           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 801           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1100          |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 170           |
| Time                    | 4.48e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.876         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0076101925  |
| n_timesteps             | 207360000     |
-------------------------------------------

 ---------------- Iteration 648 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 648           |
| ItrTime                 | 72.9          |
| LossAfter               | -0.0074628135 |
| LossBefore              | 5.105089e-09  |
| MeanKL                  | 0.008283718   |
| MeanKLBefore            | 8.482104e-14  |
| Step_0-AverageDiscou... | 170           |
| Step_0-AveragePolicyStd | 0.3955658     |
| Step_0-AverageReturn    | 439           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 752           |
| Step_0-MinReturn        | 18.3          |
| Step_0-NumTrajs         | 900           |
| Step_0-PolicyExecTime   | 1.49          |
| Step_0-StdReturn        | 139           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.3954823     |
| Step_1-AverageReturn    | 438           |
| Step_1-EnvExecTime      | 24.3          |
| Step_1-MaxReturn        | 811           |
| Step_1-MinReturn        | 21.5          |
| Step_1-NumTrajs         | 964           |
| Step_1-PolicyExecTime   | 4.61          |
| Step_1-StdReturn        | 160           |
| Time                    | 4.49e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.896         |
| Time-Sampling           | 58            |
| Time-TotalInner         | 59.1          |
| dLoss                   | 0.0074628186  |
| n_timesteps             | 207680000     |
-------------------------------------------

 ---------------- Iteration 649 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 649           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0074499785 |
| LossBefore              | 2.307214e-09  |
| MeanKL                  | 0.0077471994  |
| MeanKLBefore            | -2.979195e-09 |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.3959313     |
| Step_0-AverageReturn    | 453           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 754           |
| Step_0-MinReturn        | 31.1          |
| Step_0-NumTrajs         | 903           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 130           |
| Step_1-AverageDiscou... | 167           |
| Step_1-AveragePolicyStd | 0.39586982    |
| Step_1-AverageReturn    | 410           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 813           |
| Step_1-MinReturn        | 31            |
| Step_1-NumTrajs         | 1043          |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 173           |
| Time                    | 4.49e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.823         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.007449981   |
| n_timesteps             | 208000000     |
-------------------------------------------

 ---------------- Iteration 650 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 650            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.0073904633  |
| LossBefore              | -1.0197965e-08 |
| MeanKL                  | 0.008063242    |
| MeanKLBefore            | -1.6389802e-08 |
| Step_0-AverageDiscou... | 178            |
| Step_0-AveragePolicyStd | 0.3961731      |
| Step_0-AverageReturn    | 449            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 765            |
| Step_0-MinReturn        | 11.9           |
| Step_0-NumTrajs         | 932            |
| Step_0-PolicyExecTime   | 1.8            |
| Step_0-StdReturn        | 140            |
| Step_1-AverageDiscou... | 171            |
| Step_1-AveragePolicyStd | 0.3960668      |
| Step_1-AverageReturn    | 421            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 806            |
| Step_1-MinReturn        | 18.5           |
| Step_1-NumTrajs         | 1003           |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 161            |
| Time                    | 4.5e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.839          |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.007390453    |
| n_timesteps             | 208320000      |
--------------------------------------------

 ---------------- Iteration 651 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 651            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.0074195443  |
| LossBefore              | -1.6790306e-09 |
| MeanKL                  | 0.008793814    |
| MeanKLBefore            | -1.4908998e-09 |
| Step_0-AverageDiscou... | 169            |
| Step_0-AveragePolicyStd | 0.39650604     |
| Step_0-AverageReturn    | 425            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 773            |
| Step_0-MinReturn        | 15.1           |
| Step_0-NumTrajs         | 974            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 170            |
| Step_1-AveragePolicyStd | 0.39637014     |
| Step_1-AverageReturn    | 419            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 785            |
| Step_1-MinReturn        | 20.4           |
| Step_1-NumTrajs         | 1038           |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 177            |
| Time                    | 4.51e+04       |
| Time-InnerStep          | 0.168          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.862          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0074195424   |
| n_timesteps             | 208640000      |
--------------------------------------------

 ---------------- Iteration 652 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 652            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0068598567  |
| LossBefore              | 3.5358325e-09  |
| MeanKL                  | 0.007777384    |
| MeanKLBefore            | -2.9799223e-09 |
| Step_0-AverageDiscou... | 163            |
| Step_0-AveragePolicyStd | 0.39648268     |
| Step_0-AverageReturn    | 396            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 744            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 1020           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 158            |
| Step_1-AverageDiscou... | 171            |
| Step_1-AveragePolicyStd | 0.3965608      |
| Step_1-AverageReturn    | 419            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 794            |
| Step_1-MinReturn        | 15             |
| Step_1-NumTrajs         | 1025           |
| Step_1-PolicyExecTime   | 4.42           |
| Step_1-StdReturn        | 165            |
| Time                    | 4.51e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.908          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0068598604   |
| n_timesteps             | 208960000      |
--------------------------------------------

 ---------------- Iteration 653 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 653            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0076286183  |
| LossBefore              | 3.1795733e-09  |
| MeanKL                  | 0.007984705    |
| MeanKLBefore            | -2.9778633e-09 |
| Step_0-AverageDiscou... | 167            |
| Step_0-AveragePolicyStd | 0.39608043     |
| Step_0-AverageReturn    | 414            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 781            |
| Step_0-MinReturn        | 15.4           |
| Step_0-NumTrajs         | 992            |
| Step_0-PolicyExecTime   | 1.41           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 166            |
| Step_1-AveragePolicyStd | 0.39608327     |
| Step_1-AverageReturn    | 398            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 788            |
| Step_1-MinReturn        | 20.6           |
| Step_1-NumTrajs         | 1080           |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 170            |
| Time                    | 4.52e+04       |
| Time-InnerStep          | 0.171          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.884          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0076286215   |
| n_timesteps             | 209280000      |
--------------------------------------------

 ---------------- Iteration 654 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 654            |
| ItrTime                 | 70             |
| LossAfter               | -0.0072250813  |
| LossBefore              | -3.4880496e-09 |
| MeanKL                  | 0.008074418    |
| MeanKLBefore            | -4.4685686e-09 |
| Step_0-AverageDiscou... | 165            |
| Step_0-AveragePolicyStd | 0.39626274     |
| Step_0-AverageReturn    | 397            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 773            |
| Step_0-MinReturn        | 18.4           |
| Step_0-NumTrajs         | 1024           |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 157            |
| Step_1-AverageDiscou... | 170            |
| Step_1-AveragePolicyStd | 0.39616057     |
| Step_1-AverageReturn    | 405            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 792            |
| Step_1-MinReturn        | 17.1           |
| Step_1-NumTrajs         | 1054           |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 152            |
| Time                    | 4.53e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.888          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007225078    |
| n_timesteps             | 209600000      |
--------------------------------------------

 ---------------- Iteration 655 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 655            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007381321   |
| LossBefore              | 8.858581e-09   |
| MeanKL                  | 0.008573306    |
| MeanKLBefore            | -1.4897369e-09 |
| Step_0-AverageDiscou... | 169            |
| Step_0-AveragePolicyStd | 0.39650685     |
| Step_0-AverageReturn    | 419            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 768            |
| Step_0-MinReturn        | 25.4           |
| Step_0-NumTrajs         | 1003           |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 162            |
| Step_1-AverageDiscou... | 169            |
| Step_1-AveragePolicyStd | 0.39639413     |
| Step_1-AverageReturn    | 417            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 793            |
| Step_1-MinReturn        | 26.4           |
| Step_1-NumTrajs         | 1029           |
| Step_1-PolicyExecTime   | 4.29           |
| Step_1-StdReturn        | 165            |
| Time                    | 4.53e+04       |
| Time-InnerStep          | 0.168          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.872          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.00738133     |
| n_timesteps             | 209920000      |
--------------------------------------------

 ---------------- Iteration 656 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 656           |
| ItrTime                 | 71            |
| LossAfter               | -0.0069831833 |
| LossBefore              | 3.1574572e-09 |
| MeanKL                  | 0.008415004   |
| MeanKLBefore            | 1.937065e-08  |
| Step_0-AverageDiscou... | 169           |
| Step_0-AveragePolicyStd | 0.3968149     |
| Step_0-AverageReturn    | 420           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 763           |
| Step_0-MinReturn        | 8.97          |
| Step_0-NumTrajs         | 977           |
| Step_0-PolicyExecTime   | 1.4           |
| Step_0-StdReturn        | 163           |
| Step_1-AverageDiscou... | 174           |
| Step_1-AveragePolicyStd | 0.39675364    |
| Step_1-AverageReturn    | 433           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 812           |
| Step_1-MinReturn        | 30            |
| Step_1-NumTrajs         | 995           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 168           |
| Time                    | 4.54e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.878         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0069831866  |
| n_timesteps             | 210240000     |
-------------------------------------------

 ---------------- Iteration 657 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 657           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.0070160194 |
| LossBefore              | 4.2006363e-09 |
| MeanKL                  | 0.008372337   |
| MeanKLBefore            | 1.4904348e-09 |
| Step_0-AverageDiscou... | 162           |
| Step_0-AveragePolicyStd | 0.3967501     |
| Step_0-AverageReturn    | 386           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 850           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 1056          |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 162           |
| Step_1-AveragePolicyStd | 0.39664316    |
| Step_1-AverageReturn    | 386           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 803           |
| Step_1-MinReturn        | 10.3          |
| Step_1-NumTrajs         | 1078          |
| Step_1-PolicyExecTime   | 4.36          |
| Step_1-StdReturn        | 181           |
| Time                    | 4.55e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.909         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0070160236  |
| n_timesteps             | 210560000     |
-------------------------------------------

 ---------------- Iteration 658 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 658            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0076956316  |
| LossBefore              | -5.4346005e-10 |
| MeanKL                  | 0.00836979     |
| MeanKLBefore            | 1.6389269e-08  |
| Step_0-AverageDiscou... | 176            |
| Step_0-AveragePolicyStd | 0.39672863     |
| Step_0-AverageReturn    | 434            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 783            |
| Step_0-MinReturn        | 11.5           |
| Step_0-NumTrajs         | 985            |
| Step_0-PolicyExecTime   | 1.47           |
| Step_0-StdReturn        | 162            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.39665166     |
| Step_1-AverageReturn    | 433            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 913            |
| Step_1-MinReturn        | 27.2           |
| Step_1-NumTrajs         | 1011           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 167            |
| Time                    | 4.56e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.832          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007695631    |
| n_timesteps             | 210880000      |
--------------------------------------------

 ---------------- Iteration 659 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 659           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0072509586 |
| LossBefore              | 1.531921e-08  |
| MeanKL                  | 0.008513832   |
| MeanKLBefore            | 1.0428977e-08 |
| Step_0-AverageDiscou... | 169           |
| Step_0-AveragePolicyStd | 0.39696512    |
| Step_0-AverageReturn    | 416           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 778           |
| Step_0-MinReturn        | 13.3          |
| Step_0-NumTrajs         | 1012          |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 171           |
| Step_1-AveragePolicyStd | 0.39688352    |
| Step_1-AverageReturn    | 420           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 819           |
| Step_1-MinReturn        | 8.62          |
| Step_1-NumTrajs         | 1030          |
| Step_1-PolicyExecTime   | 4.26          |
| Step_1-StdReturn        | 176           |
| Time                    | 4.56e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.007250974   |
| n_timesteps             | 211200000     |
-------------------------------------------

 ---------------- Iteration 660 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 660           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0074520847 |
| LossBefore              | 1.1924902e-08 |
| MeanKL                  | 0.00844199    |
| MeanKLBefore            | -1.489906e-09 |
| Step_0-AverageDiscou... | 172           |
| Step_0-AveragePolicyStd | 0.39692098    |
| Step_0-AverageReturn    | 424           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 720           |
| Step_0-MinReturn        | 16.9          |
| Step_0-NumTrajs         | 996           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 151           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.3969412     |
| Step_1-AverageReturn    | 418           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 845           |
| Step_1-MinReturn        | 14.3          |
| Step_1-NumTrajs         | 1003          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 167           |
| Time                    | 4.57e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.856         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007452097   |
| n_timesteps             | 211520000     |
-------------------------------------------

 ---------------- Iteration 661 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 661           |
| ItrTime                 | 72.7          |
| LossAfter               | -0.0074759843 |
| LossBefore              | 9.6792965e-09 |
| MeanKL                  | 0.008554617   |
| MeanKLBefore            | 8.938727e-09  |
| Step_0-AverageDiscou... | 178           |
| Step_0-AveragePolicyStd | 0.3966186     |
| Step_0-AverageReturn    | 453           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 827           |
| Step_0-MinReturn        | 16.8          |
| Step_0-NumTrajs         | 954           |
| Step_0-PolicyExecTime   | 1.48          |
| Step_0-StdReturn        | 155           |
| Step_1-AverageDiscou... | 169           |
| Step_1-AveragePolicyStd | 0.39642623    |
| Step_1-AverageReturn    | 410           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 812           |
| Step_1-MinReturn        | 15.2          |
| Step_1-NumTrajs         | 1056          |
| Step_1-PolicyExecTime   | 4.52          |
| Step_1-StdReturn        | 171           |
| Time                    | 4.58e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.9          |
| Time-OuterStep          | 13.9          |
| Time-SampleProc         | 0.913         |
| Time-Sampling           | 57.7          |
| Time-TotalInner         | 58.8          |
| dLoss                   | 0.007475994   |
| n_timesteps             | 211840000     |
-------------------------------------------

 ---------------- Iteration 662 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 662           |
| ItrTime                 | 71.4          |
| LossAfter               | -0.007978648  |
| LossBefore              | 8.931027e-09  |
| MeanKL                  | 0.008408137   |
| MeanKLBefore            | 1.4906888e-09 |
| Step_0-AverageDiscou... | 165           |
| Step_0-AveragePolicyStd | 0.39751709    |
| Step_0-AverageReturn    | 415           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 761           |
| Step_0-MinReturn        | 17.7          |
| Step_0-NumTrajs         | 984           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 171           |
| Step_1-AverageDiscou... | 167           |
| Step_1-AveragePolicyStd | 0.3974839     |
| Step_1-AverageReturn    | 407           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 800           |
| Step_1-MinReturn        | 21.3          |
| Step_1-NumTrajs         | 1067          |
| Step_1-PolicyExecTime   | 4.27          |
| Step_1-StdReturn        | 175           |
| Time                    | 4.58e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.841         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.007978657   |
| n_timesteps             | 212160000     |
-------------------------------------------

 ---------------- Iteration 663 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 663            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.007296052   |
| LossBefore              | -9.658658e-09  |
| MeanKL                  | 0.00774761     |
| MeanKLBefore            | -5.9599827e-09 |
| Step_0-AverageDiscou... | 169            |
| Step_0-AveragePolicyStd | 0.39819416     |
| Step_0-AverageReturn    | 430            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 793            |
| Step_0-MinReturn        | 7.38           |
| Step_0-NumTrajs         | 967            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 173            |
| Step_1-AveragePolicyStd | 0.39811876     |
| Step_1-AverageReturn    | 435            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 805            |
| Step_1-MinReturn        | 8.33           |
| Step_1-NumTrajs         | 999            |
| Step_1-PolicyExecTime   | 4.33           |
| Step_1-StdReturn        | 177            |
| Time                    | 4.59e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.842          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.007296042    |
| n_timesteps             | 212480000      |
--------------------------------------------

 ---------------- Iteration 664 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 664            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0070661632  |
| LossBefore              | -5.714284e-09  |
| MeanKL                  | 0.008188559    |
| MeanKLBefore            | -1.0430009e-08 |
| Step_0-AverageDiscou... | 182            |
| Step_0-AveragePolicyStd | 0.39827377     |
| Step_0-AverageReturn    | 465            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 779            |
| Step_0-MinReturn        | 12.5           |
| Step_0-NumTrajs         | 931            |
| Step_0-PolicyExecTime   | 1.78           |
| Step_0-StdReturn        | 144            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.39820448     |
| Step_1-AverageReturn    | 427            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 815            |
| Step_1-MinReturn        | 19.7           |
| Step_1-NumTrajs         | 1039           |
| Step_1-PolicyExecTime   | 4.29           |
| Step_1-StdReturn        | 172            |
| Time                    | 4.6e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.856          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0070661576   |
| n_timesteps             | 212800000      |
--------------------------------------------

 ---------------- Iteration 665 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 665           |
| ItrTime                 | 71            |
| LossAfter               | -0.007154031  |
| LossBefore              | 3.625006e-10  |
| MeanKL                  | 0.007922053   |
| MeanKLBefore            | 4.4691846e-09 |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.39732233    |
| Step_0-AverageReturn    | 452           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 738           |
| Step_0-MinReturn        | 15.5          |
| Step_0-NumTrajs         | 931           |
| Step_0-PolicyExecTime   | 1.41          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.39727658    |
| Step_1-AverageReturn    | 426           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 797           |
| Step_1-MinReturn        | 8.44          |
| Step_1-NumTrajs         | 1010          |
| Step_1-PolicyExecTime   | 4.36          |
| Step_1-StdReturn        | 190           |
| Time                    | 4.61e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.896         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0071540317  |
| n_timesteps             | 213120000     |
-------------------------------------------

 ---------------- Iteration 666 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 666           |
| ItrTime                 | 71            |
| LossAfter               | -0.006996681  |
| LossBefore              | 3.8329167e-09 |
| MeanKL                  | 0.007617435   |
| MeanKLBefore            | 1.489342e-09  |
| Step_0-AverageDiscou... | 165           |
| Step_0-AveragePolicyStd | 0.397046      |
| Step_0-AverageReturn    | 426           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 778           |
| Step_0-MinReturn        | 8.71          |
| Step_0-NumTrajs         | 936           |
| Step_0-PolicyExecTime   | 1.43          |
| Step_0-StdReturn        | 167           |
| Step_1-AverageDiscou... | 167           |
| Step_1-AveragePolicyStd | 0.3969669     |
| Step_1-AverageReturn    | 414           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 774           |
| Step_1-MinReturn        | 6.81          |
| Step_1-NumTrajs         | 1018          |
| Step_1-PolicyExecTime   | 4.4           |
| Step_1-StdReturn        | 173           |
| Time                    | 4.61e+04      |
| Time-InnerStep          | 0.168         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.838         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0069966847  |
| n_timesteps             | 213440000     |
-------------------------------------------

 ---------------- Iteration 667 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 667           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0072665615 |
| LossBefore              | 4.7388773e-09 |
| MeanKL                  | 0.007988883   |
| MeanKLBefore            | -2.104983e-14 |
| Step_0-AverageDiscou... | 175           |
| Step_0-AveragePolicyStd | 0.39659312    |
| Step_0-AverageReturn    | 450           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 792           |
| Step_0-MinReturn        | 10            |
| Step_0-NumTrajs         | 924           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 150           |
| Step_1-AverageDiscou... | 169           |
| Step_1-AveragePolicyStd | 0.39643982    |
| Step_1-AverageReturn    | 421           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 793           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1022          |
| Step_1-PolicyExecTime   | 4.3           |
| Step_1-StdReturn        | 170           |
| Time                    | 4.62e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.831         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 57.9          |
| dLoss                   | 0.007266566   |
| n_timesteps             | 213760000     |
-------------------------------------------

 ---------------- Iteration 668 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 668           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0077158352 |
| LossBefore              | 1.1678127e-08 |
| MeanKL                  | 0.008545926   |
| MeanKLBefore            | -2.979337e-09 |
| Step_0-AverageDiscou... | 176           |
| Step_0-AveragePolicyStd | 0.3963718     |
| Step_0-AverageReturn    | 428           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 774           |
| Step_0-MinReturn        | 27            |
| Step_0-NumTrajs         | 1003          |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 142           |
| Step_1-AverageDiscou... | 181           |
| Step_1-AveragePolicyStd | 0.39634004    |
| Step_1-AverageReturn    | 455           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 813           |
| Step_1-MinReturn        | 22.3          |
| Step_1-NumTrajs         | 974           |
| Step_1-PolicyExecTime   | 4.31          |
| Step_1-StdReturn        | 152           |
| Time                    | 4.63e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.835         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007715847   |
| n_timesteps             | 214080000     |
-------------------------------------------

 ---------------- Iteration 669 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 669           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0074645104 |
| LossBefore              | 4.36627e-09   |
| MeanKL                  | 0.008258233   |
| MeanKLBefore            | 7.450214e-09  |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.3960833     |
| Step_0-AverageReturn    | 413           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 782           |
| Step_0-MinReturn        | 11.3          |
| Step_0-NumTrajs         | 1034          |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39612284    |
| Step_1-AverageReturn    | 423           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 787           |
| Step_1-MinReturn        | 13.5          |
| Step_1-NumTrajs         | 1014          |
| Step_1-PolicyExecTime   | 4.31          |
| Step_1-StdReturn        | 161           |
| Time                    | 4.63e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.895         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 57            |
| dLoss                   | 0.0074645146  |
| n_timesteps             | 214400000     |
-------------------------------------------

 ---------------- Iteration 670 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 670           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.0073090456 |
| LossBefore              | 4.520987e-09  |
| MeanKL                  | 0.008138357   |
| MeanKLBefore            | 8.940271e-09  |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.39565036    |
| Step_0-AverageReturn    | 456           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 794           |
| Step_0-MinReturn        | 10.5          |
| Step_0-NumTrajs         | 952           |
| Step_0-PolicyExecTime   | 1.42          |
| Step_0-StdReturn        | 151           |
| Step_1-AverageDiscou... | 177           |
| Step_1-AveragePolicyStd | 0.39561832    |
| Step_1-AverageReturn    | 437           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 807           |
| Step_1-MinReturn        | 24.8          |
| Step_1-NumTrajs         | 1042          |
| Step_1-PolicyExecTime   | 4.54          |
| Step_1-StdReturn        | 180           |
| Time                    | 4.64e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.896         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0073090503  |
| n_timesteps             | 214720000     |
-------------------------------------------

 ---------------- Iteration 671 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 671            |
| ItrTime                 | 71             |
| LossAfter               | -0.0073364875  |
| LossBefore              | 6.0136456e-09  |
| MeanKL                  | 0.007847271    |
| MeanKLBefore            | -1.1919137e-08 |
| Step_0-AverageDiscou... | 180            |
| Step_0-AveragePolicyStd | 0.39582202     |
| Step_0-AverageReturn    | 449            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 798            |
| Step_0-MinReturn        | 17.6           |
| Step_0-NumTrajs         | 974            |
| Step_0-PolicyExecTime   | 1.44           |
| Step_0-StdReturn        | 148            |
| Step_1-AverageDiscou... | 176            |
| Step_1-AveragePolicyStd | 0.3957404      |
| Step_1-AverageReturn    | 432            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 780            |
| Step_1-MinReturn        | 20.8           |
| Step_1-NumTrajs         | 1026           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 163            |
| Time                    | 4.65e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.797          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.0073364936   |
| n_timesteps             | 215040000      |
--------------------------------------------

 ---------------- Iteration 672 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 672           |
| ItrTime                 | 72.3          |
| LossAfter               | -0.0077531054 |
| LossBefore              | 3.1826647e-10 |
| MeanKL                  | 0.00883302    |
| MeanKLBefore            | 5.960141e-09  |
| Step_0-AverageDiscou... | 174           |
| Step_0-AveragePolicyStd | 0.3961088     |
| Step_0-AverageReturn    | 449           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 757           |
| Step_0-MinReturn        | 11.2          |
| Step_0-NumTrajs         | 933           |
| Step_0-PolicyExecTime   | 1.42          |
| Step_0-StdReturn        | 147           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39606008    |
| Step_1-AverageReturn    | 438           |
| Step_1-EnvExecTime      | 24.3          |
| Step_1-MaxReturn        | 809           |
| Step_1-MinReturn        | 1.63          |
| Step_1-NumTrajs         | 974           |
| Step_1-PolicyExecTime   | 4.27          |
| Step_1-StdReturn        | 171           |
| Time                    | 4.66e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.838         |
| Time-Sampling           | 57.6          |
| Time-TotalInner         | 58.7          |
| dLoss                   | 0.007753106   |
| n_timesteps             | 215360000     |
-------------------------------------------

 ---------------- Iteration 673 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 673            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007739595   |
| LossBefore              | -8.8065875e-09 |
| MeanKL                  | 0.008371245    |
| MeanKLBefore            | -1.4909766e-09 |
| Step_0-AverageDiscou... | 174            |
| Step_0-AveragePolicyStd | 0.39639485     |
| Step_0-AverageReturn    | 432            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 759            |
| Step_0-MinReturn        | 21.5           |
| Step_0-NumTrajs         | 990            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 173            |
| Step_1-AveragePolicyStd | 0.39643994     |
| Step_1-AverageReturn    | 424            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 737            |
| Step_1-MinReturn        | 24.3           |
| Step_1-NumTrajs         | 1019           |
| Step_1-PolicyExecTime   | 4.33           |
| Step_1-StdReturn        | 169            |
| Time                    | 4.66e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.844          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0077395863   |
| n_timesteps             | 215680000      |
--------------------------------------------

 ---------------- Iteration 674 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 674            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.0075858906  |
| LossBefore              | -3.747874e-09  |
| MeanKL                  | 0.008652595    |
| MeanKLBefore            | -2.9797558e-09 |
| Step_0-AverageDiscou... | 183            |
| Step_0-AveragePolicyStd | 0.39620283     |
| Step_0-AverageReturn    | 467            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 790            |
| Step_0-MinReturn        | 6.64           |
| Step_0-NumTrajs         | 950            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 152            |
| Step_1-AverageDiscou... | 178            |
| Step_1-AveragePolicyStd | 0.39611453     |
| Step_1-AverageReturn    | 453            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 823            |
| Step_1-MinReturn        | 29.2           |
| Step_1-NumTrajs         | 984            |
| Step_1-PolicyExecTime   | 4.35           |
| Step_1-StdReturn        | 173            |
| Time                    | 4.67e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.007585887    |
| n_timesteps             | 216000000      |
--------------------------------------------

 ---------------- Iteration 675 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 675            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.0073649893  |
| LossBefore              | -2.0092251e-08 |
| MeanKL                  | 0.0076884404   |
| MeanKLBefore            | 1.4894577e-09  |
| Step_0-AverageDiscou... | 165            |
| Step_0-AveragePolicyStd | 0.39603806     |
| Step_0-AverageReturn    | 409            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 800            |
| Step_0-MinReturn        | 20.8           |
| Step_0-NumTrajs         | 1012           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 173            |
| Step_1-AverageDiscou... | 166            |
| Step_1-AveragePolicyStd | 0.39593247     |
| Step_1-AverageReturn    | 411            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 802            |
| Step_1-MinReturn        | 20.4           |
| Step_1-NumTrajs         | 1038           |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 179            |
| Time                    | 4.68e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.835          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.0073649692   |
| n_timesteps             | 216320000      |
--------------------------------------------

 ---------------- Iteration 676 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 676           |
| ItrTime                 | 70            |
| LossAfter               | -0.0074950727 |
| LossBefore              | 1.2631581e-08 |
| MeanKL                  | 0.007907522   |
| MeanKLBefore            | 8.940716e-09  |
| Step_0-AverageDiscou... | 169           |
| Step_0-AveragePolicyStd | 0.39624205    |
| Step_0-AverageReturn    | 421           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 775           |
| Step_0-MinReturn        | 15            |
| Step_0-NumTrajs         | 993           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 165           |
| Step_1-AveragePolicyStd | 0.3960235     |
| Step_1-AverageReturn    | 403           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 812           |
| Step_1-MinReturn        | 12            |
| Step_1-NumTrajs         | 1038          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 168           |
| Time                    | 4.68e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.816         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.0074950852  |
| n_timesteps             | 216640000     |
-------------------------------------------

 ---------------- Iteration 677 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 677           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007085073  |
| LossBefore              | 5.4817657e-09 |
| MeanKL                  | 0.008105429   |
| MeanKLBefore            | 1.3409806e-08 |
| Step_0-AverageDiscou... | 170           |
| Step_0-AveragePolicyStd | 0.39607325    |
| Step_0-AverageReturn    | 418           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 838           |
| Step_0-MinReturn        | 15.6          |
| Step_0-NumTrajs         | 1027          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 164           |
| Step_1-AveragePolicyStd | 0.39595205    |
| Step_1-AverageReturn    | 386           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 757           |
| Step_1-MinReturn        | 17.8          |
| Step_1-NumTrajs         | 1103          |
| Step_1-PolicyExecTime   | 4.44          |
| Step_1-StdReturn        | 173           |
| Time                    | 4.69e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.867         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0070850784  |
| n_timesteps             | 216960000     |
-------------------------------------------

 ---------------- Iteration 678 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 678           |
| ItrTime                 | 72            |
| LossAfter               | -0.006920277  |
| LossBefore              | 7.273191e-09  |
| MeanKL                  | 0.007845683   |
| MeanKLBefore            | 1.0428205e-08 |
| Step_0-AverageDiscou... | 178           |
| Step_0-AveragePolicyStd | 0.39614365    |
| Step_0-AverageReturn    | 458           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 789           |
| Step_0-MinReturn        | 18.6          |
| Step_0-NumTrajs         | 943           |
| Step_0-PolicyExecTime   | 1.44          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 178           |
| Step_1-AveragePolicyStd | 0.39603096    |
| Step_1-AverageReturn    | 448           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 814           |
| Step_1-MinReturn        | 15.4          |
| Step_1-NumTrajs         | 1001          |
| Step_1-PolicyExecTime   | 4.54          |
| Step_1-StdReturn        | 167           |
| Time                    | 4.7e+04       |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.901         |
| Time-Sampling           | 57            |
| Time-TotalInner         | 58.2          |
| dLoss                   | 0.0069202846  |
| n_timesteps             | 217280000     |
-------------------------------------------

 ---------------- Iteration 679 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 679            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0068020644  |
| LossBefore              | -2.6853832e-09 |
| MeanKL                  | 0.007860437    |
| MeanKLBefore            | -1.4905268e-09 |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.39658868     |
| Step_0-AverageReturn    | 436            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 816            |
| Step_0-MinReturn        | 8.89           |
| Step_0-NumTrajs         | 966            |
| Step_0-PolicyExecTime   | 1.46           |
| Step_0-StdReturn        | 165            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.39650688     |
| Step_1-AverageReturn    | 437            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 774            |
| Step_1-MinReturn        | 23.2           |
| Step_1-NumTrajs         | 1001           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 164            |
| Time                    | 4.7e+04        |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0068020616   |
| n_timesteps             | 217600000      |
--------------------------------------------

 ---------------- Iteration 680 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 680            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.00707589    |
| LossBefore              | -3.5662887e-10 |
| MeanKL                  | 0.008618394    |
| MeanKLBefore            | 2.9806508e-09  |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.39655167     |
| Step_0-AverageReturn    | 472            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 792            |
| Step_0-MinReturn        | 21.2           |
| Step_0-NumTrajs         | 946            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 177            |
| Step_1-AveragePolicyStd | 0.39649472     |
| Step_1-AverageReturn    | 429            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 811            |
| Step_1-MinReturn        | 29.5           |
| Step_1-NumTrajs         | 1049           |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 168            |
| Time                    | 4.71e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.855          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0070758895   |
| n_timesteps             | 217920000      |
--------------------------------------------

 ---------------- Iteration 681 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 681            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.007458719   |
| LossBefore              | -1.1123798e-08 |
| MeanKL                  | 0.008507406    |
| MeanKLBefore            | 7.452013e-09   |
| Step_0-AverageDiscou... | 174            |
| Step_0-AveragePolicyStd | 0.39711776     |
| Step_0-AverageReturn    | 432            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 753            |
| Step_0-MinReturn        | 15             |
| Step_0-NumTrajs         | 993            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 182            |
| Step_1-AverageDiscou... | 169            |
| Step_1-AveragePolicyStd | 0.39708808     |
| Step_1-AverageReturn    | 414            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 794            |
| Step_1-MinReturn        | -3.35          |
| Step_1-NumTrajs         | 1048           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 188            |
| Time                    | 4.72e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.85           |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.007458708    |
| n_timesteps             | 218240000      |
--------------------------------------------

 ---------------- Iteration 682 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 682           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.0074752765 |
| LossBefore              | 5.7620366e-09 |
| MeanKL                  | 0.008085873   |
| MeanKLBefore            | 5.9604828e-09 |
| Step_0-AverageDiscou... | 182           |
| Step_0-AveragePolicyStd | 0.39712194    |
| Step_0-AverageReturn    | 463           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 772           |
| Step_0-MinReturn        | 12.1          |
| Step_0-NumTrajs         | 943           |
| Step_0-PolicyExecTime   | 1.46          |
| Step_0-StdReturn        | 154           |
| Step_1-AverageDiscou... | 179           |
| Step_1-AveragePolicyStd | 0.39692214    |
| Step_1-AverageReturn    | 447           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 785           |
| Step_1-MinReturn        | 9.45          |
| Step_1-NumTrajs         | 1002          |
| Step_1-PolicyExecTime   | 4.54          |
| Step_1-StdReturn        | 176           |
| Time                    | 4.73e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.901         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58            |
| dLoss                   | 0.007475282   |
| n_timesteps             | 218560000     |
-------------------------------------------

 ---------------- Iteration 683 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 683           |
| ItrTime                 | 71            |
| LossAfter               | -0.0072744302 |
| LossBefore              | -9.92752e-09  |
| MeanKL                  | 0.008216617   |
| MeanKLBefore            | -8.937714e-09 |
| Step_0-AverageDiscou... | 181           |
| Step_0-AveragePolicyStd | 0.3973021     |
| Step_0-AverageReturn    | 459           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 772           |
| Step_0-MinReturn        | 24.5          |
| Step_0-NumTrajs         | 954           |
| Step_0-PolicyExecTime   | 1.47          |
| Step_0-StdReturn        | 147           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.3972634     |
| Step_1-AverageReturn    | 451           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 809           |
| Step_1-MinReturn        | 21.7          |
| Step_1-NumTrajs         | 1010          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 162           |
| Time                    | 4.73e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.0072744205  |
| n_timesteps             | 218880000     |
-------------------------------------------

 ---------------- Iteration 684 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 684           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0068096416 |
| LossBefore              | 1.233749e-08  |
| MeanKL                  | 0.008312638   |
| MeanKLBefore            | 2.9798262e-09 |
| Step_0-AverageDiscou... | 178           |
| Step_0-AveragePolicyStd | 0.3971283     |
| Step_0-AverageReturn    | 445           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 776           |
| Step_0-MinReturn        | 29.2          |
| Step_0-NumTrajs         | 972           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 177           |
| Step_1-AveragePolicyStd | 0.39711824    |
| Step_1-AverageReturn    | 447           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 761           |
| Step_1-MinReturn        | 5.82          |
| Step_1-NumTrajs         | 966           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 163           |
| Time                    | 4.74e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.787         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0068096537  |
| n_timesteps             | 219200000     |
-------------------------------------------

 ---------------- Iteration 685 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 685           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007446885  |
| LossBefore              | -2.956248e-09 |
| MeanKL                  | 0.008510774   |
| MeanKLBefore            | 5.959067e-09  |
| Step_0-AverageDiscou... | 175           |
| Step_0-AveragePolicyStd | 0.3975949     |
| Step_0-AverageReturn    | 457           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 737           |
| Step_0-MinReturn        | 25.1          |
| Step_0-NumTrajs         | 907           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 147           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.3974548     |
| Step_1-AverageReturn    | 440           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 791           |
| Step_1-MinReturn        | 14.9          |
| Step_1-NumTrajs         | 955           |
| Step_1-PolicyExecTime   | 4.29          |
| Step_1-StdReturn        | 161           |
| Time                    | 4.75e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.791         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0074468823  |
| n_timesteps             | 219520000     |
-------------------------------------------

 ---------------- Iteration 686 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 686           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.0074293674 |
| LossBefore              | -8.53723e-10  |
| MeanKL                  | 0.008456121   |
| MeanKLBefore            | 5.9593637e-09 |
| Step_0-AverageDiscou... | 184           |
| Step_0-AveragePolicyStd | 0.39750388    |
| Step_0-AverageReturn    | 470           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 766           |
| Step_0-MinReturn        | 0.335         |
| Step_0-NumTrajs         | 953           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 155           |
| Step_1-AverageDiscou... | 183           |
| Step_1-AveragePolicyStd | 0.39745012    |
| Step_1-AverageReturn    | 456           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 847           |
| Step_1-MinReturn        | 22.4          |
| Step_1-NumTrajs         | 1004          |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 168           |
| Time                    | 4.75e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.818         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.0074293665  |
| n_timesteps             | 219840000     |
-------------------------------------------

 ---------------- Iteration 687 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 687           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.0076076984 |
| LossBefore              | 8.1136715e-09 |
| MeanKL                  | 0.008446289   |
| MeanKLBefore            | 4.470893e-09  |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.39741015    |
| Step_0-AverageReturn    | 411           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 781           |
| Step_0-MinReturn        | 7.73          |
| Step_0-NumTrajs         | 1078          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 179           |
| Step_1-AverageDiscou... | 173           |
| Step_1-AveragePolicyStd | 0.39724028    |
| Step_1-AverageReturn    | 413           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 795           |
| Step_1-MinReturn        | 19.4          |
| Step_1-NumTrajs         | 1096          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 177           |
| Time                    | 4.76e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.88          |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0076077064  |
| n_timesteps             | 220160000     |
-------------------------------------------

 ---------------- Iteration 688 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 688           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.006782112  |
| LossBefore              | -6.337321e-09 |
| MeanKL                  | 0.0077463714  |
| MeanKLBefore            | 5.959236e-09  |
| Step_0-AverageDiscou... | 166           |
| Step_0-AveragePolicyStd | 0.39722204    |
| Step_0-AverageReturn    | 399           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 750           |
| Step_0-MinReturn        | 18.6          |
| Step_0-NumTrajs         | 1056          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 168           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39719027    |
| Step_1-AverageReturn    | 420           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 878           |
| Step_1-MinReturn        | 15.7          |
| Step_1-NumTrajs         | 1047          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 176           |
| Time                    | 4.77e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.874         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0067821057  |
| n_timesteps             | 220480000     |
-------------------------------------------

 ---------------- Iteration 689 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 689           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.0072566657 |
| LossBefore              | 3.365836e-09  |
| MeanKL                  | 0.0076146186  |
| MeanKLBefore            | 7.4499127e-09 |
| Step_0-AverageDiscou... | 179           |
| Step_0-AveragePolicyStd | 0.39684805    |
| Step_0-AverageReturn    | 444           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 845           |
| Step_0-MinReturn        | 17            |
| Step_0-NumTrajs         | 989           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 175           |
| Step_1-AveragePolicyStd | 0.3967387     |
| Step_1-AverageReturn    | 420           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 819           |
| Step_1-MinReturn        | 23.6          |
| Step_1-NumTrajs         | 1056          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 169           |
| Time                    | 4.78e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.834         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.007256669   |
| n_timesteps             | 220800000     |
-------------------------------------------

 ---------------- Iteration 690 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 690           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0076301754 |
| LossBefore              | -7.165954e-10 |
| MeanKL                  | 0.00797505    |
| MeanKLBefore            | 5.959722e-09  |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.3965783     |
| Step_0-AverageReturn    | 437           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 843           |
| Step_0-MinReturn        | 17.3          |
| Step_0-NumTrajs         | 972           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 176           |
| Step_1-AveragePolicyStd | 0.39656645    |
| Step_1-AverageReturn    | 445           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 857           |
| Step_1-MinReturn        | 12.8          |
| Step_1-NumTrajs         | 1002          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 195           |
| Time                    | 4.78e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.805         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0076301745  |
| n_timesteps             | 221120000     |
-------------------------------------------

 ---------------- Iteration 691 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 691            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007613808   |
| LossBefore              | -2.8700893e-09 |
| MeanKL                  | 0.007973688    |
| MeanKLBefore            | 8.940744e-09   |
| Step_0-AverageDiscou... | 180            |
| Step_0-AveragePolicyStd | 0.39606833     |
| Step_0-AverageReturn    | 454            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 825            |
| Step_0-MinReturn        | 23.1           |
| Step_0-NumTrajs         | 972            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 153            |
| Step_1-AverageDiscou... | 176            |
| Step_1-AveragePolicyStd | 0.39597875     |
| Step_1-AverageReturn    | 439            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 830            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1000           |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 172            |
| Time                    | 4.79e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.796          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007613805    |
| n_timesteps             | 221440000      |
--------------------------------------------

 ---------------- Iteration 692 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 692           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.007256643  |
| LossBefore              | 7.338426e-09  |
| MeanKL                  | 0.007867122   |
| MeanKLBefore            | 2.9809928e-09 |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.3963321     |
| Step_0-AverageReturn    | 448           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 820           |
| Step_0-MinReturn        | 17.1          |
| Step_0-NumTrajs         | 975           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 169           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.39636734    |
| Step_1-AverageReturn    | 421           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 807           |
| Step_1-MinReturn        | 12.8          |
| Step_1-NumTrajs         | 1024          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 180           |
| Time                    | 4.8e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.0072566504  |
| n_timesteps             | 221760000     |
-------------------------------------------

 ---------------- Iteration 693 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 693           |
| ItrTime                 | 71            |
| LossAfter               | -0.007500857  |
| LossBefore              | -1.322892e-08 |
| MeanKL                  | 0.00851534    |
| MeanKLBefore            | -3.375078e-14 |
| Step_0-AverageDiscou... | 178           |
| Step_0-AveragePolicyStd | 0.3968518     |
| Step_0-AverageReturn    | 457           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 840           |
| Step_0-MinReturn        | 19.9          |
| Step_0-NumTrajs         | 944           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 164           |
| Step_1-AverageDiscou... | 177           |
| Step_1-AveragePolicyStd | 0.39680693    |
| Step_1-AverageReturn    | 450           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 846           |
| Step_1-MinReturn        | 13.2          |
| Step_1-NumTrajs         | 988           |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 174           |
| Time                    | 4.8e+04       |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.815         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.007500844   |
| n_timesteps             | 222080000     |
-------------------------------------------

 ---------------- Iteration 694 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 694           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0076335995 |
| LossBefore              | 1.1809927e-09 |
| MeanKL                  | 0.009129221   |
| MeanKLBefore            | 4.47033e-09   |
| Step_0-AverageDiscou... | 175           |
| Step_0-AveragePolicyStd | 0.39665988    |
| Step_0-AverageReturn    | 439           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 792           |
| Step_0-MinReturn        | 6.18          |
| Step_0-NumTrajs         | 996           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 168           |
| Step_1-AverageDiscou... | 171           |
| Step_1-AveragePolicyStd | 0.39667508    |
| Step_1-AverageReturn    | 423           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 851           |
| Step_1-MinReturn        | 6.3           |
| Step_1-NumTrajs         | 1040          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 178           |
| Time                    | 4.81e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.819         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007633601   |
| n_timesteps             | 222400000     |
-------------------------------------------

 ---------------- Iteration 695 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 695            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.006975852   |
| LossBefore              | 7.3344815e-09  |
| MeanKL                  | 0.007977913    |
| MeanKLBefore            | -8.9399546e-09 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.39643902     |
| Step_0-AverageReturn    | 492            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 827            |
| Step_0-MinReturn        | 18.8           |
| Step_0-NumTrajs         | 905            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.39630654     |
| Step_1-AverageReturn    | 456            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 805            |
| Step_1-MinReturn        | 20.8           |
| Step_1-NumTrajs         | 983            |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 175            |
| Time                    | 4.82e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.779          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.7           |
| dLoss                   | 0.0069758594   |
| n_timesteps             | 222720000      |
--------------------------------------------

 ---------------- Iteration 696 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 696           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.007445343  |
| LossBefore              | -8.206447e-09 |
| MeanKL                  | 0.008419616   |
| MeanKLBefore            | 7.449792e-09  |
| Step_0-AverageDiscou... | 182           |
| Step_0-AveragePolicyStd | 0.39647955    |
| Step_0-AverageReturn    | 459           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 789           |
| Step_0-MinReturn        | 22.4          |
| Step_0-NumTrajs         | 974           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 163           |
| Step_1-AverageDiscou... | 176           |
| Step_1-AveragePolicyStd | 0.39645645    |
| Step_1-AverageReturn    | 433           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 841           |
| Step_1-MinReturn        | 21.8          |
| Step_1-NumTrajs         | 1031          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 178           |
| Time                    | 4.82e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.802         |
| Time-Sampling           | 54.5          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.0074453345  |
| n_timesteps             | 223040000     |
-------------------------------------------

 ---------------- Iteration 697 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 697            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0073174178  |
| LossBefore              | 4.0771995e-09  |
| MeanKL                  | 0.007843912    |
| MeanKLBefore            | 1.19187025e-08 |
| Step_0-AverageDiscou... | 170            |
| Step_0-AveragePolicyStd | 0.39680067     |
| Step_0-AverageReturn    | 432            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 785            |
| Step_0-MinReturn        | 26.8           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 170            |
| Step_1-AveragePolicyStd | 0.39675716     |
| Step_1-AverageReturn    | 421            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 840            |
| Step_1-MinReturn        | 25             |
| Step_1-NumTrajs         | 1054           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 185            |
| Time                    | 4.83e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.814          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007317422    |
| n_timesteps             | 223360000      |
--------------------------------------------

 ---------------- Iteration 698 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 698            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.0072366446  |
| LossBefore              | 4.3081996e-09  |
| MeanKL                  | 0.0076727113   |
| MeanKLBefore            | -1.4915231e-09 |
| Step_0-AverageDiscou... | 175            |
| Step_0-AveragePolicyStd | 0.3970197      |
| Step_0-AverageReturn    | 450            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 791            |
| Step_0-MinReturn        | 13.3           |
| Step_0-NumTrajs         | 958            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 171            |
| Step_1-AverageDiscou... | 177            |
| Step_1-AveragePolicyStd | 0.39690343     |
| Step_1-AverageReturn    | 455            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 825            |
| Step_1-MinReturn        | 18.8           |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.34           |
| Step_1-StdReturn        | 171            |
| Time                    | 4.84e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.835          |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.007236649    |
| n_timesteps             | 223680000      |
--------------------------------------------

 ---------------- Iteration 699 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 699           |
| ItrTime                 | 72.2          |
| LossAfter               | -0.0072782887 |
| LossBefore              | 6.173501e-09  |
| MeanKL                  | 0.0076905517  |
| MeanKLBefore            | 1.489897e-08  |
| Step_0-AverageDiscou... | 187           |
| Step_0-AveragePolicyStd | 0.39733082    |
| Step_0-AverageReturn    | 491           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 785           |
| Step_0-MinReturn        | 26.6          |
| Step_0-NumTrajs         | 916           |
| Step_0-PolicyExecTime   | 1.5           |
| Step_0-StdReturn        | 157           |
| Step_1-AverageDiscou... | 181           |
| Step_1-AveragePolicyStd | 0.39726493    |
| Step_1-AverageReturn    | 452           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 814           |
| Step_1-MinReturn        | 17.4          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.42          |
| Step_1-StdReturn        | 173           |
| Time                    | 4.85e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.8          |
| Time-OuterStep          | 13.8          |
| Time-SampleProc         | 0.922         |
| Time-Sampling           | 57.2          |
| Time-TotalInner         | 58.3          |
| dLoss                   | 0.007278295   |
| n_timesteps             | 224000000     |
-------------------------------------------

 ---------------- Iteration 700 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 700            |
| ItrTime                 | 69.3           |
| LossAfter               | -0.0069845305  |
| LossBefore              | -3.3020036e-09 |
| MeanKL                  | 0.0077580675   |
| MeanKLBefore            | -4.470067e-09  |
| Step_0-AverageDiscou... | 186            |
| Step_0-AveragePolicyStd | 0.39715186     |
| Step_0-AverageReturn    | 462            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 789            |
| Step_0-MinReturn        | 27.4           |
| Step_0-NumTrajs         | 989            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 159            |
| Step_1-AverageDiscou... | 185            |
| Step_1-AveragePolicyStd | 0.39707783     |
| Step_1-AverageReturn    | 456            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 838            |
| Step_1-MinReturn        | 32.6           |
| Step_1-NumTrajs         | 1017           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 173            |
| Time                    | 4.85e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.809          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.006984527    |
| n_timesteps             | 224320000      |
--------------------------------------------

 ---------------- Iteration 701 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 701            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0071080895  |
| LossBefore              | 5.1682187e-09  |
| MeanKL                  | 0.007558857    |
| MeanKLBefore            | -1.4893871e-09 |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.39800736     |
| Step_0-AverageReturn    | 419            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 804            |
| Step_0-MinReturn        | 19.8           |
| Step_0-NumTrajs         | 1041           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 179            |
| Step_1-AverageDiscou... | 168            |
| Step_1-AveragePolicyStd | 0.3979787      |
| Step_1-AverageReturn    | 405            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 790            |
| Step_1-MinReturn        | 13.7           |
| Step_1-NumTrajs         | 1078           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 186            |
| Time                    | 4.86e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.87           |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0071080946   |
| n_timesteps             | 224640000      |
--------------------------------------------

 ---------------- Iteration 702 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 702            |
| ItrTime                 | 71             |
| LossAfter               | -0.0073272116  |
| LossBefore              | -4.452956e-09  |
| MeanKL                  | 0.00787315     |
| MeanKLBefore            | -1.4908471e-09 |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.3980768      |
| Step_0-AverageReturn    | 442            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 794            |
| Step_0-MinReturn        | 14.1           |
| Step_0-NumTrajs         | 945            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 181            |
| Step_1-AverageDiscou... | 169            |
| Step_1-AveragePolicyStd | 0.39801848     |
| Step_1-AverageReturn    | 415            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 836            |
| Step_1-MinReturn        | 16.7           |
| Step_1-NumTrajs         | 1045           |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 193            |
| Time                    | 4.87e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.845          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.007327207    |
| n_timesteps             | 224960000      |
--------------------------------------------

 ---------------- Iteration 703 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 703           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.0071127387 |
| LossBefore              | 1.1085655e-08 |
| MeanKL                  | 0.008361448   |
| MeanKLBefore            | -8.940053e-09 |
| Step_0-AverageDiscou... | 173           |
| Step_0-AveragePolicyStd | 0.398624      |
| Step_0-AverageReturn    | 424           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 819           |
| Step_0-MinReturn        | 22            |
| Step_0-NumTrajs         | 1039          |
| Step_0-PolicyExecTime   | 1.28          |
| Step_0-StdReturn        | 179           |
| Step_1-AverageDiscou... | 175           |
| Step_1-AveragePolicyStd | 0.3985006     |
| Step_1-AverageReturn    | 425           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 816           |
| Step_1-MinReturn        | 12.9          |
| Step_1-NumTrajs         | 1068          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 183           |
| Time                    | 4.87e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.821         |
| Time-Sampling           | 53.8          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.00711275    |
| n_timesteps             | 225280000     |
-------------------------------------------

 ---------------- Iteration 704 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 704            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.00702645    |
| LossBefore              | -1.3706733e-10 |
| MeanKL                  | 0.007868464    |
| MeanKLBefore            | 7.418066e-13   |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.398479       |
| Step_0-AverageReturn    | 480            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 782            |
| Step_0-MinReturn        | 9.33           |
| Step_0-NumTrajs         | 931            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 152            |
| Step_1-AverageDiscou... | 179            |
| Step_1-AveragePolicyStd | 0.3983516      |
| Step_1-AverageReturn    | 453            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 884            |
| Step_1-MinReturn        | 21.9           |
| Step_1-NumTrajs         | 994            |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 175            |
| Time                    | 4.88e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.799          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.00702645     |
| n_timesteps             | 225600000      |
--------------------------------------------

 ---------------- Iteration 705 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 705            |
| ItrTime                 | 71.9           |
| LossAfter               | -0.0076060733  |
| LossBefore              | -5.96458e-11   |
| MeanKL                  | 0.008847432    |
| MeanKLBefore            | -3.8058445e-13 |
| Step_0-AverageDiscou... | 176            |
| Step_0-AveragePolicyStd | 0.3989464      |
| Step_0-AverageReturn    | 465            |
| Step_0-EnvExecTime      | 24.5           |
| Step_0-MaxReturn        | 794            |
| Step_0-MinReturn        | 14.9           |
| Step_0-NumTrajs         | 920            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 177            |
| Step_1-AveragePolicyStd | 0.39889205     |
| Step_1-AverageReturn    | 458            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 818            |
| Step_1-MinReturn        | 5.7            |
| Step_1-NumTrajs         | 972            |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 184            |
| Time                    | 4.89e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.828          |
| Time-Sampling           | 57.4           |
| Time-TotalInner         | 58.4           |
| dLoss                   | 0.0076060733   |
| n_timesteps             | 225920000      |
--------------------------------------------

 ---------------- Iteration 706 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 706           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.00657067   |
| LossBefore              | 6.9060677e-09 |
| MeanKL                  | 0.007865127   |
| MeanKLBefore            | 5.9589196e-09 |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.3990745     |
| Step_0-AverageReturn    | 422           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 848           |
| Step_0-MinReturn        | 23.7          |
| Step_0-NumTrajs         | 1029          |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 181           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39900097    |
| Step_1-AverageReturn    | 422           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 911           |
| Step_1-MinReturn        | 21.4          |
| Step_1-NumTrajs         | 1049          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 183           |
| Time                    | 4.9e+04       |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.825         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.006570677   |
| n_timesteps             | 226240000     |
-------------------------------------------

 ---------------- Iteration 707 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 707           |
| ItrTime                 | 68.8          |
| LossAfter               | -0.0073322216 |
| LossBefore              | -6.290766e-09 |
| MeanKL                  | 0.008111331   |
| MeanKLBefore            | 1.4893569e-09 |
| Step_0-AverageDiscou... | 170           |
| Step_0-AveragePolicyStd | 0.39870986    |
| Step_0-AverageReturn    | 406           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 771           |
| Step_0-MinReturn        | 20.6          |
| Step_0-NumTrajs         | 1084          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 182           |
| Step_1-AverageDiscou... | 160           |
| Step_1-AveragePolicyStd | 0.39855045    |
| Step_1-AverageReturn    | 372           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 792           |
| Step_1-MinReturn        | 18.4          |
| Step_1-NumTrajs         | 1159          |
| Step_1-PolicyExecTime   | 4.01          |
| Step_1-StdReturn        | 194           |
| Time                    | 4.9e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.864         |
| Time-Sampling           | 54.1          |
| Time-TotalInner         | 55.2          |
| dLoss                   | 0.007332215   |
| n_timesteps             | 226560000     |
-------------------------------------------

 ---------------- Iteration 708 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 708           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.00740472   |
| LossBefore              | 1.7961206e-09 |
| MeanKL                  | 0.007868255   |
| MeanKLBefore            | -1.638936e-08 |
| Step_0-AverageDiscou... | 187           |
| Step_0-AveragePolicyStd | 0.39878407    |
| Step_0-AverageReturn    | 486           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 771           |
| Step_0-MinReturn        | 15.1          |
| Step_0-NumTrajs         | 918           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 147           |
| Step_1-AverageDiscou... | 185           |
| Step_1-AveragePolicyStd | 0.39882824    |
| Step_1-AverageReturn    | 470           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 795           |
| Step_1-MinReturn        | 3.86          |
| Step_1-NumTrajs         | 981           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 172           |
| Time                    | 4.91e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.785         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007404722   |
| n_timesteps             | 226880000     |
-------------------------------------------

 ---------------- Iteration 709 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 709           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0071907057 |
| LossBefore              | 7.511986e-09  |
| MeanKL                  | 0.007571532   |
| MeanKLBefore            | -7.643663e-13 |
| Step_0-AverageDiscou... | 173           |
| Step_0-AveragePolicyStd | 0.39857092    |
| Step_0-AverageReturn    | 427           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 808           |
| Step_0-MinReturn        | 6.67          |
| Step_0-NumTrajs         | 1048          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 189           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39849743    |
| Step_1-AverageReturn    | 421           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 902           |
| Step_1-MinReturn        | 6.67          |
| Step_1-NumTrajs         | 1057          |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 185           |
| Time                    | 4.92e+04      |
| Time-InnerStep          | 0.17          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.824         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007190713   |
| n_timesteps             | 227200000     |
-------------------------------------------

 ---------------- Iteration 710 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 710            |
| ItrTime                 | 72             |
| LossAfter               | -0.007336291   |
| LossBefore              | -1.2239667e-08 |
| MeanKL                  | 0.008028373    |
| MeanKLBefore            | 8.939919e-09   |
| Step_0-AverageDiscou... | 177            |
| Step_0-AveragePolicyStd | 0.39886072     |
| Step_0-AverageReturn    | 446            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 841            |
| Step_0-MinReturn        | 27.9           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 179            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.39880657     |
| Step_1-AverageReturn    | 441            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 842            |
| Step_1-MinReturn        | 15.6           |
| Step_1-NumTrajs         | 1005           |
| Step_1-PolicyExecTime   | 4.54           |
| Step_1-StdReturn        | 181            |
| Time                    | 4.92e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.908          |
| Time-Sampling           | 57.1           |
| Time-TotalInner         | 58.2           |
| dLoss                   | 0.007336279    |
| n_timesteps             | 227520000      |
--------------------------------------------

 ---------------- Iteration 711 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 711           |
| ItrTime                 | 71.6          |
| LossAfter               | -0.006842463  |
| LossBefore              | 2.480919e-09  |
| MeanKL                  | 0.008064892   |
| MeanKLBefore            | -5.959678e-09 |
| Step_0-AverageDiscou... | 187           |
| Step_0-AveragePolicyStd | 0.39847454    |
| Step_0-AverageReturn    | 492           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 791           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 901           |
| Step_0-PolicyExecTime   | 1.5           |
| Step_0-StdReturn        | 154           |
| Step_1-AverageDiscou... | 186           |
| Step_1-AveragePolicyStd | 0.39850146    |
| Step_1-AverageReturn    | 483           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 805           |
| Step_1-MinReturn        | 5.87          |
| Step_1-NumTrajs         | 963           |
| Step_1-PolicyExecTime   | 4.39          |
| Step_1-StdReturn        | 180           |
| Time                    | 4.93e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.846         |
| Time-Sampling           | 56.9          |
| Time-TotalInner         | 58            |
| dLoss                   | 0.006842465   |
| n_timesteps             | 227840000     |
-------------------------------------------

 ---------------- Iteration 712 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 712            |
| ItrTime                 | 71             |
| LossAfter               | -0.0074431747  |
| LossBefore              | -1.2992649e-08 |
| MeanKL                  | 0.008282664    |
| MeanKLBefore            | -5.958925e-09  |
| Step_0-AverageDiscou... | 179            |
| Step_0-AveragePolicyStd | 0.39870277     |
| Step_0-AverageReturn    | 464            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 789            |
| Step_0-MinReturn        | 15.6           |
| Step_0-NumTrajs         | 955            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 170            |
| Step_1-AverageDiscou... | 177            |
| Step_1-AveragePolicyStd | 0.3986863      |
| Step_1-AverageReturn    | 443            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 827            |
| Step_1-MinReturn        | -2.35          |
| Step_1-NumTrajs         | 1022           |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 179            |
| Time                    | 4.94e+04       |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.815          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0074431617   |
| n_timesteps             | 228160000      |
--------------------------------------------

 ---------------- Iteration 713 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 713           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007417825  |
| LossBefore              | 1.5363009e-08 |
| MeanKL                  | 0.008430805   |
| MeanKLBefore            | -5.958932e-09 |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.3984193     |
| Step_0-AverageReturn    | 466           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 796           |
| Step_0-MinReturn        | 7.48          |
| Step_0-NumTrajs         | 952           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 181           |
| Step_1-AverageDiscou... | 183           |
| Step_1-AveragePolicyStd | 0.398422      |
| Step_1-AverageReturn    | 457           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 869           |
| Step_1-MinReturn        | 11.8          |
| Step_1-NumTrajs         | 1022          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 183           |
| Time                    | 4.94e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0074178404  |
| n_timesteps             | 228480000     |
-------------------------------------------

 ---------------- Iteration 714 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 714           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0068696076 |
| LossBefore              | 5.08896e-09   |
| MeanKL                  | 0.007850888   |
| MeanKLBefore            | 4.469931e-09  |
| Step_0-AverageDiscou... | 172           |
| Step_0-AveragePolicyStd | 0.39856124    |
| Step_0-AverageReturn    | 416           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 792           |
| Step_0-MinReturn        | 19.5          |
| Step_0-NumTrajs         | 1065          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 179           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.39852995    |
| Step_1-AverageReturn    | 388           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 822           |
| Step_1-MinReturn        | 13.3          |
| Step_1-NumTrajs         | 1159          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 186           |
| Time                    | 4.95e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.851         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0068696127  |
| n_timesteps             | 228800000     |
-------------------------------------------

 ---------------- Iteration 715 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 715           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.007516384  |
| LossBefore              | -7.081528e-10 |
| MeanKL                  | 0.008055239   |
| MeanKLBefore            | 5.9601426e-09 |
| Step_0-AverageDiscou... | 181           |
| Step_0-AveragePolicyStd | 0.39858118    |
| Step_0-AverageReturn    | 469           |
| Step_0-EnvExecTime      | 24.1          |
| Step_0-MaxReturn        | 804           |
| Step_0-MinReturn        | 14.7          |
| Step_0-NumTrajs         | 941           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 174           |
| Step_1-AverageDiscou... | 173           |
| Step_1-AveragePolicyStd | 0.3985292     |
| Step_1-AverageReturn    | 430           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 852           |
| Step_1-MinReturn        | 16.5          |
| Step_1-NumTrajs         | 1025          |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 183           |
| Time                    | 4.96e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.786         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.007516383   |
| n_timesteps             | 229120000     |
-------------------------------------------

 ---------------- Iteration 716 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 716           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.00717475   |
| LossBefore              | -2.5443e-09   |
| MeanKL                  | 0.0077767996  |
| MeanKLBefore            | 4.4704516e-09 |
| Step_0-AverageDiscou... | 170           |
| Step_0-AveragePolicyStd | 0.39907274    |
| Step_0-AverageReturn    | 435           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 750           |
| Step_0-MinReturn        | 6.39          |
| Step_0-NumTrajs         | 971           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 172           |
| Step_1-AveragePolicyStd | 0.39903936    |
| Step_1-AverageReturn    | 437           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 813           |
| Step_1-MinReturn        | 6.89          |
| Step_1-NumTrajs         | 996           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 196           |
| Time                    | 4.97e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.814         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.0071747475  |
| n_timesteps             | 229440000     |
-------------------------------------------

 ---------------- Iteration 717 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 717           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0077096364 |
| LossBefore              | 6.527337e-09  |
| MeanKL                  | 0.00924809    |
| MeanKLBefore            | -4.468572e-09 |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.3987452     |
| Step_0-AverageReturn    | 463           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 866           |
| Step_0-MinReturn        | 18.3          |
| Step_0-NumTrajs         | 971           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 186           |
| Step_1-AverageDiscou... | 175           |
| Step_1-AveragePolicyStd | 0.39875782    |
| Step_1-AverageReturn    | 432           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 842           |
| Step_1-MinReturn        | 23.1          |
| Step_1-NumTrajs         | 1041          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 182           |
| Time                    | 4.97e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007709643   |
| n_timesteps             | 229760000     |
-------------------------------------------

 ---------------- Iteration 718 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 718            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0071208524  |
| LossBefore              | 4.329361e-09   |
| MeanKL                  | 0.008030288    |
| MeanKLBefore            | -1.1918599e-08 |
| Step_0-AverageDiscou... | 182            |
| Step_0-AveragePolicyStd | 0.39973867     |
| Step_0-AverageReturn    | 470            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 841            |
| Step_0-MinReturn        | 16.5           |
| Step_0-NumTrajs         | 957            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.3996724      |
| Step_1-AverageReturn    | 440            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 830            |
| Step_1-MinReturn        | 17             |
| Step_1-NumTrajs         | 1023           |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 189            |
| Time                    | 4.98e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.835          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0071208565   |
| n_timesteps             | 230080000      |
--------------------------------------------

 ---------------- Iteration 719 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 719            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007302343   |
| LossBefore              | -8.2702325e-09 |
| MeanKL                  | 0.0074630394   |
| MeanKLBefore            | 7.4494446e-09  |
| Step_0-AverageDiscou... | 186            |
| Step_0-AveragePolicyStd | 0.40026787     |
| Step_0-AverageReturn    | 488            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 851            |
| Step_0-MinReturn        | 14.1           |
| Step_0-NumTrajs         | 918            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 153            |
| Step_1-AverageDiscou... | 175            |
| Step_1-AveragePolicyStd | 0.40026388     |
| Step_1-AverageReturn    | 432            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 828            |
| Step_1-MinReturn        | 8.46           |
| Step_1-NumTrajs         | 1041           |
| Step_1-PolicyExecTime   | 4.55           |
| Step_1-StdReturn        | 175            |
| Time                    | 4.99e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0073023345   |
| n_timesteps             | 230400000      |
--------------------------------------------

 ---------------- Iteration 720 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 720           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0069227926 |
| LossBefore              | 3.3335414e-09 |
| MeanKL                  | 0.007948561   |
| MeanKLBefore            | 1.4897552e-09 |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.40027037    |
| Step_0-AverageReturn    | 444           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 850           |
| Step_0-MinReturn        | 15.9          |
| Step_0-NumTrajs         | 1006          |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 184           |
| Step_1-AverageDiscou... | 176           |
| Step_1-AveragePolicyStd | 0.40020895    |
| Step_1-AverageReturn    | 435           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 859           |
| Step_1-MinReturn        | 7.85          |
| Step_1-NumTrajs         | 1053          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 189           |
| Time                    | 4.99e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.834         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.006922796   |
| n_timesteps             | 230720000     |
-------------------------------------------

 ---------------- Iteration 721 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 721           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.007165806  |
| LossBefore              | 6.2748966e-09 |
| MeanKL                  | 0.0078851255  |
| MeanKLBefore            | -8.939656e-09 |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.40005815    |
| Step_0-AverageReturn    | 464           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 802           |
| Step_0-MinReturn        | 7.46          |
| Step_0-NumTrajs         | 1000          |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.39998603    |
| Step_1-AverageReturn    | 449           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 788           |
| Step_1-MinReturn        | 26.7          |
| Step_1-NumTrajs         | 1014          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 175           |
| Time                    | 5e+04         |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.878         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.007165812   |
| n_timesteps             | 231040000     |
-------------------------------------------

 ---------------- Iteration 722 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 722           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.007591373  |
| LossBefore              | 1.9472608e-09 |
| MeanKL                  | 0.008004611   |
| MeanKLBefore            | 4.4692685e-09 |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.4001393     |
| Step_0-AverageReturn    | 471           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 814           |
| Step_0-MinReturn        | 6.73          |
| Step_0-NumTrajs         | 920           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 178           |
| Step_1-AveragePolicyStd | 0.40006116    |
| Step_1-AverageReturn    | 459           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 815           |
| Step_1-MinReturn        | 10            |
| Step_1-NumTrajs         | 978           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 192           |
| Time                    | 5.01e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.778         |
| Time-Sampling           | 57.3          |
| Time-TotalInner         | 58.2          |
| dLoss                   | 0.0075913747  |
| n_timesteps             | 231360000     |
-------------------------------------------

 ---------------- Iteration 723 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 723           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007205002  |
| LossBefore              | 1.161057e-08  |
| MeanKL                  | 0.008417761   |
| MeanKLBefore            | 2.9794431e-09 |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.40029562    |
| Step_0-AverageReturn    | 456           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 820           |
| Step_0-MinReturn        | 18.4          |
| Step_0-NumTrajs         | 982           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 176           |
| Step_1-AveragePolicyStd | 0.40029377    |
| Step_1-AverageReturn    | 435           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 873           |
| Step_1-MinReturn        | 21.8          |
| Step_1-NumTrajs         | 1050          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 182           |
| Time                    | 5.01e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.811         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0072050137  |
| n_timesteps             | 231680000     |
-------------------------------------------

 ---------------- Iteration 724 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 724            |
| ItrTime                 | 67.2           |
| LossAfter               | -0.0069084293  |
| LossBefore              | 5.320859e-09   |
| MeanKL                  | 0.007105687    |
| MeanKLBefore            | -2.4868995e-15 |
| Step_0-AverageDiscou... | 172            |
| Step_0-AveragePolicyStd | 0.39982992     |
| Step_0-AverageReturn    | 405            |
| Step_0-EnvExecTime      | 21.8           |
| Step_0-MaxReturn        | 819            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 1126           |
| Step_0-PolicyExecTime   | 1.27           |
| Step_0-StdReturn        | 188            |
| Step_1-AverageDiscou... | 169            |
| Step_1-AveragePolicyStd | 0.39981398     |
| Step_1-AverageReturn    | 396            |
| Step_1-EnvExecTime      | 22.2           |
| Step_1-MaxReturn        | 858            |
| Step_1-MinReturn        | 17.9           |
| Step_1-NumTrajs         | 1130           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 186            |
| Time                    | 5.02e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.891          |
| Time-Sampling           | 52.5           |
| Time-TotalInner         | 53.6           |
| dLoss                   | 0.0069084344   |
| n_timesteps             | 232000000      |
--------------------------------------------

 ---------------- Iteration 725 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 725           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.007229132  |
| LossBefore              | 3.2743828e-09 |
| MeanKL                  | 0.008794906   |
| MeanKLBefore            | 5.9597127e-09 |
| Step_0-AverageDiscou... | 164           |
| Step_0-AveragePolicyStd | 0.39989385    |
| Step_0-AverageReturn    | 404           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 802           |
| Step_0-MinReturn        | -3.42         |
| Step_0-NumTrajs         | 1049          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 192           |
| Step_1-AverageDiscou... | 167           |
| Step_1-AveragePolicyStd | 0.3999275     |
| Step_1-AverageReturn    | 401           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 863           |
| Step_1-MinReturn        | 15.5          |
| Step_1-NumTrajs         | 1087          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 194           |
| Time                    | 5.03e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.835         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0072291354  |
| n_timesteps             | 232320000     |
-------------------------------------------

 ---------------- Iteration 726 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 726            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.007257962   |
| LossBefore              | 2.6827223e-09  |
| MeanKL                  | 0.0080804825   |
| MeanKLBefore            | -4.4712722e-09 |
| Step_0-AverageDiscou... | 184            |
| Step_0-AveragePolicyStd | 0.39988506     |
| Step_0-AverageReturn    | 461            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 822            |
| Step_0-MinReturn        | 56             |
| Step_0-NumTrajs         | 1002           |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 178            |
| Step_1-AveragePolicyStd | 0.39978617     |
| Step_1-AverageReturn    | 432            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 895            |
| Step_1-MinReturn        | 56             |
| Step_1-NumTrajs         | 1084           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 190            |
| Time                    | 5.04e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.828          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.007257965    |
| n_timesteps             | 232640000      |
--------------------------------------------

 ---------------- Iteration 727 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 727           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0074586524 |
| LossBefore              | 2.3571407e-09 |
| MeanKL                  | 0.008101511   |
| MeanKLBefore            | -7.526424e-13 |
| Step_0-AverageDiscou... | 173           |
| Step_0-AveragePolicyStd | 0.4007417     |
| Step_0-AverageReturn    | 430           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 809           |
| Step_0-MinReturn        | 14.3          |
| Step_0-NumTrajs         | 1022          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 190           |
| Step_1-AverageDiscou... | 166           |
| Step_1-AveragePolicyStd | 0.40055808    |
| Step_1-AverageReturn    | 400           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 818           |
| Step_1-MinReturn        | 16.3          |
| Step_1-NumTrajs         | 1080          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 184           |
| Time                    | 5.04e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.825         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.0074586547  |
| n_timesteps             | 232960000     |
-------------------------------------------

 ---------------- Iteration 728 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 728            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0073919343  |
| LossBefore              | -7.1742966e-11 |
| MeanKL                  | 0.0080953045   |
| MeanKLBefore            | 2.9791825e-09  |
| Step_0-AverageDiscou... | 190            |
| Step_0-AveragePolicyStd | 0.4008322      |
| Step_0-AverageReturn    | 494            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 792            |
| Step_0-MinReturn        | 24.4           |
| Step_0-NumTrajs         | 923            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.4007846      |
| Step_1-AverageReturn    | 453            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 853            |
| Step_1-MinReturn        | 7.59           |
| Step_1-NumTrajs         | 1007           |
| Step_1-PolicyExecTime   | 4.46           |
| Step_1-StdReturn        | 173            |
| Time                    | 5.05e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.846          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0073919343   |
| n_timesteps             | 233280000      |
--------------------------------------------

 ---------------- Iteration 729 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 729          |
| ItrTime                 | 70.5         |
| LossAfter               | -0.007365824 |
| LossBefore              | 3.961129e-09 |
| MeanKL                  | 0.008546415  |
| MeanKLBefore            | 1.48995e-09  |
| Step_0-AverageDiscou... | 178          |
| Step_0-AveragePolicyStd | 0.40109622   |
| Step_0-AverageReturn    | 444          |
| Step_0-EnvExecTime      | 22.9         |
| Step_0-MaxReturn        | 812          |
| Step_0-MinReturn        | 14           |
| Step_0-NumTrajs         | 1008         |
| Step_0-PolicyExecTime   | 1.44         |
| Step_0-StdReturn        | 178          |
| Step_1-AverageDiscou... | 173          |
| Step_1-AveragePolicyStd | 0.40109286   |
| Step_1-AverageReturn    | 416          |
| Step_1-EnvExecTime      | 23.4         |
| Step_1-MaxReturn        | 863          |
| Step_1-MinReturn        | 16.7         |
| Step_1-NumTrajs         | 1085         |
| Step_1-PolicyExecTime   | 4.48         |
| Step_1-StdReturn        | 188          |
| Time                    | 5.06e+04     |
| Time-InnerStep          | 0.167        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.942        |
| Time-Sampling           | 55.7         |
| Time-TotalInner         | 56.8         |
| dLoss                   | 0.0073658284 |
| n_timesteps             | 233600000    |
------------------------------------------

 ---------------- Iteration 730 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 730            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0072705196  |
| LossBefore              | -4.802506e-09  |
| MeanKL                  | 0.008214453    |
| MeanKLBefore            | -4.4699924e-09 |
| Step_0-AverageDiscou... | 181            |
| Step_0-AveragePolicyStd | 0.40086251     |
| Step_0-AverageReturn    | 468            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 797            |
| Step_0-MinReturn        | 12.3           |
| Step_0-NumTrajs         | 956            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.4008587      |
| Step_1-AverageReturn    | 457            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 889            |
| Step_1-MinReturn        | -0.936         |
| Step_1-NumTrajs         | 1005           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 179            |
| Time                    | 5.06e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007270515    |
| n_timesteps             | 233920000      |
--------------------------------------------

 ---------------- Iteration 731 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 731            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0069664717  |
| LossBefore              | -1.085598e-08  |
| MeanKL                  | 0.007571318    |
| MeanKLBefore            | -5.9607976e-09 |
| Step_0-AverageDiscou... | 184            |
| Step_0-AveragePolicyStd | 0.4015075      |
| Step_0-AverageReturn    | 469            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 806            |
| Step_0-MinReturn        | 52             |
| Step_0-NumTrajs         | 967            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 170            |
| Step_1-AverageDiscou... | 185            |
| Step_1-AveragePolicyStd | 0.4014017      |
| Step_1-AverageReturn    | 463            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 861            |
| Step_1-MinReturn        | 21.5           |
| Step_1-NumTrajs         | 1020           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 187            |
| Time                    | 5.07e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.806          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.006966461    |
| n_timesteps             | 234240000      |
--------------------------------------------

 ---------------- Iteration 732 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 732           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.0074113375 |
| LossBefore              | -7.92903e-09  |
| MeanKL                  | 0.00851515    |
| MeanKLBefore            | -4.469655e-09 |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.4012134     |
| Step_0-AverageReturn    | 470           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 815           |
| Step_0-MinReturn        | 22.8          |
| Step_0-NumTrajs         | 981           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 175           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.40116578    |
| Step_1-AverageReturn    | 450           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 804           |
| Step_1-MinReturn        | 21.6          |
| Step_1-NumTrajs         | 1038          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 189           |
| Time                    | 5.08e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.822         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0074113295  |
| n_timesteps             | 234560000     |
-------------------------------------------

 ---------------- Iteration 733 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 733            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.007303805   |
| LossBefore              | -1.0126866e-09 |
| MeanKL                  | 0.007903563    |
| MeanKLBefore            | 1.34099265e-08 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.40087125     |
| Step_0-AverageReturn    | 487            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 802            |
| Step_0-MinReturn        | 25.3           |
| Step_0-NumTrajs         | 913            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 160            |
| Step_1-AverageDiscou... | 185            |
| Step_1-AveragePolicyStd | 0.4006885      |
| Step_1-AverageReturn    | 484            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 810            |
| Step_1-MinReturn        | 22.6           |
| Step_1-NumTrajs         | 942            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 180            |
| Time                    | 5.08e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.781          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007303804    |
| n_timesteps             | 234880000      |
--------------------------------------------

 ---------------- Iteration 734 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 734           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0075117843 |
| LossBefore              | 4.3150137e-09 |
| MeanKL                  | 0.008210031   |
| MeanKLBefore            | 8.938808e-09  |
| Step_0-AverageDiscou... | 177           |
| Step_0-AveragePolicyStd | 0.40086746    |
| Step_0-AverageReturn    | 438           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 872           |
| Step_0-MinReturn        | 15.3          |
| Step_0-NumTrajs         | 1048          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 194           |
| Step_1-AverageDiscou... | 170           |
| Step_1-AveragePolicyStd | 0.400696      |
| Step_1-AverageReturn    | 417           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 866           |
| Step_1-MinReturn        | 10.9          |
| Step_1-NumTrajs         | 1076          |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 200           |
| Time                    | 5.09e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.818         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0075117885  |
| n_timesteps             | 235200000     |
-------------------------------------------

 ---------------- Iteration 735 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 735           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.007396574  |
| LossBefore              | -4.519677e-10 |
| MeanKL                  | 0.007944314   |
| MeanKLBefore            | 1.4909837e-09 |
| Step_0-AverageDiscou... | 181           |
| Step_0-AveragePolicyStd | 0.40102467    |
| Step_0-AverageReturn    | 447           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 801           |
| Step_0-MinReturn        | 18.7          |
| Step_0-NumTrajs         | 1009          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 171           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.40097725    |
| Step_1-AverageReturn    | 451           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 832           |
| Step_1-MinReturn        | 8.89          |
| Step_1-NumTrajs         | 995           |
| Step_1-PolicyExecTime   | 4.25          |
| Step_1-StdReturn        | 170           |
| Time                    | 5.1e+04       |
| Time-InnerStep          | 0.16          |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.8           |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0073965737  |
| n_timesteps             | 235520000     |
-------------------------------------------

 ---------------- Iteration 736 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 736            |
| ItrTime                 | 69             |
| LossAfter               | -0.0069640167  |
| LossBefore              | 8.556972e-09   |
| MeanKL                  | 0.007663954    |
| MeanKLBefore            | -4.4630966e-13 |
| Step_0-AverageDiscou... | 165            |
| Step_0-AveragePolicyStd | 0.4006784      |
| Step_0-AverageReturn    | 389            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 799            |
| Step_0-MinReturn        | 3.96           |
| Step_0-NumTrajs         | 1134           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 191            |
| Step_1-AverageDiscou... | 171            |
| Step_1-AveragePolicyStd | 0.40052187     |
| Step_1-AverageReturn    | 408            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 869            |
| Step_1-MinReturn        | 10.7           |
| Step_1-NumTrajs         | 1101           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 190            |
| Time                    | 5.11e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.87           |
| Time-Sampling           | 54.3           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.006964025    |
| n_timesteps             | 235840000      |
--------------------------------------------

 ---------------- Iteration 737 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 737            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.0072836885  |
| LossBefore              | -3.5578845e-10 |
| MeanKL                  | 0.007482922    |
| MeanKLBefore            | -2.9806373e-09 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.40042964     |
| Step_0-AverageReturn    | 475            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 844            |
| Step_0-MinReturn        | 16.9           |
| Step_0-NumTrajs         | 974            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 179            |
| Step_1-AveragePolicyStd | 0.40032145     |
| Step_1-AverageReturn    | 448            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 831            |
| Step_1-MinReturn        | 19.5           |
| Step_1-NumTrajs         | 1024           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 176            |
| Time                    | 5.11e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.824          |
| Time-Sampling           | 55.9           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007283688    |
| n_timesteps             | 236160000      |
--------------------------------------------

 ---------------- Iteration 738 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 738           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0070183454 |
| LossBefore              | 1.096641e-08  |
| MeanKL                  | 0.008131013   |
| MeanKLBefore            | 1.0428131e-08 |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.40052733    |
| Step_0-AverageReturn    | 477           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 809           |
| Step_0-MinReturn        | 7.01          |
| Step_0-NumTrajs         | 952           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.4004907     |
| Step_1-AverageReturn    | 464           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 856           |
| Step_1-MinReturn        | 14.1          |
| Step_1-NumTrajs         | 982           |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 181           |
| Time                    | 5.12e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.792         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0070183566  |
| n_timesteps             | 236480000     |
-------------------------------------------

 ---------------- Iteration 739 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 739            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0077160546  |
| LossBefore              | 1.9992537e-09  |
| MeanKL                  | 0.0077879084   |
| MeanKLBefore            | -2.9816178e-09 |
| Step_0-AverageDiscou... | 181            |
| Step_0-AveragePolicyStd | 0.40014258     |
| Step_0-AverageReturn    | 464            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 793            |
| Step_0-MinReturn        | 14.6           |
| Step_0-NumTrajs         | 977            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.40017933     |
| Step_1-AverageReturn    | 451            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 924            |
| Step_1-MinReturn        | 17.9           |
| Step_1-NumTrajs         | 1030           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 182            |
| Time                    | 5.13e+04       |
| Time-InnerStep          | 0.183          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.81           |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.0077160564   |
| n_timesteps             | 236800000      |
--------------------------------------------

 ---------------- Iteration 740 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 740            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0073888316  |
| LossBefore              | 1.4259239e-09  |
| MeanKL                  | 0.00836193     |
| MeanKLBefore            | -5.9620673e-09 |
| Step_0-AverageDiscou... | 175            |
| Step_0-AveragePolicyStd | 0.40093908     |
| Step_0-AverageReturn    | 432            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 889            |
| Step_0-MinReturn        | 6.35           |
| Step_0-NumTrajs         | 1062           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 198            |
| Step_1-AverageDiscou... | 174            |
| Step_1-AveragePolicyStd | 0.4008403      |
| Step_1-AverageReturn    | 429            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 856            |
| Step_1-MinReturn        | 7.64           |
| Step_1-NumTrajs         | 1072           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 192            |
| Time                    | 5.13e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.861          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007388833    |
| n_timesteps             | 237120000      |
--------------------------------------------

 ---------------- Iteration 741 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 741            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0071436586  |
| LossBefore              | -2.9269068e-10 |
| MeanKL                  | 0.008554274    |
| MeanKLBefore            | 2.980197e-09   |
| Step_0-AverageDiscou... | 177            |
| Step_0-AveragePolicyStd | 0.4005961      |
| Step_0-AverageReturn    | 440            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 826            |
| Step_0-MinReturn        | 14.6           |
| Step_0-NumTrajs         | 1034           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 184            |
| Step_1-AverageDiscou... | 177            |
| Step_1-AveragePolicyStd | 0.400546       |
| Step_1-AverageReturn    | 438            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 845            |
| Step_1-MinReturn        | 20.3           |
| Step_1-NumTrajs         | 1045           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 184            |
| Time                    | 5.14e+04       |
| Time-InnerStep          | 0.159          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.841          |
| Time-Sampling           | 54.8           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.007143658    |
| n_timesteps             | 237440000      |
--------------------------------------------

 ---------------- Iteration 742 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 742            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007327837   |
| LossBefore              | -5.4795364e-09 |
| MeanKL                  | 0.007970216    |
| MeanKLBefore            | -7.1054274e-15 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.4003686      |
| Step_0-AverageReturn    | 480            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 793            |
| Step_0-MinReturn        | 17.4           |
| Step_0-NumTrajs         | 941            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 169            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.4002774      |
| Step_1-AverageReturn    | 485            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 877            |
| Step_1-MinReturn        | 19.3           |
| Step_1-NumTrajs         | 954            |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 171            |
| Time                    | 5.15e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.792          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0073278314   |
| n_timesteps             | 237760000      |
--------------------------------------------

 ---------------- Iteration 743 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 743           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.006950674  |
| LossBefore              | 7.1852453e-09 |
| MeanKL                  | 0.00758866    |
| MeanKLBefore            | 2.9817884e-09 |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.39987782    |
| Step_0-AverageReturn    | 470           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 763           |
| Step_0-MinReturn        | 5.34          |
| Step_0-NumTrajs         | 966           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 167           |
| Step_1-AverageDiscou... | 183           |
| Step_1-AveragePolicyStd | 0.39980975    |
| Step_1-AverageReturn    | 453           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 852           |
| Step_1-MinReturn        | 9.55          |
| Step_1-NumTrajs         | 1033          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 176           |
| Time                    | 5.16e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.816         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.006950681   |
| n_timesteps             | 238080000     |
-------------------------------------------

 ---------------- Iteration 744 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 744            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0070208786  |
| LossBefore              | -7.2005785e-10 |
| MeanKL                  | 0.0075094253   |
| MeanKLBefore            | -8.939556e-09  |
| Step_0-AverageDiscou... | 177            |
| Step_0-AveragePolicyStd | 0.3995606      |
| Step_0-AverageReturn    | 446            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 849            |
| Step_0-MinReturn        | 15             |
| Step_0-NumTrajs         | 1009           |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 187            |
| Step_1-AverageDiscou... | 179            |
| Step_1-AveragePolicyStd | 0.39963523     |
| Step_1-AverageReturn    | 450            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 876            |
| Step_1-MinReturn        | 25.2           |
| Step_1-NumTrajs         | 1002           |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 178            |
| Time                    | 5.16e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.811          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0070208777   |
| n_timesteps             | 238400000      |
--------------------------------------------

 ---------------- Iteration 745 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 745           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.00752192   |
| LossBefore              | 6.0675043e-09 |
| MeanKL                  | 0.0075145117  |
| MeanKLBefore            | 1.4905247e-09 |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.3995354     |
| Step_0-AverageReturn    | 465           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 888           |
| Step_0-MinReturn        | 13.5          |
| Step_0-NumTrajs         | 973           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 178           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.39942208    |
| Step_1-AverageReturn    | 450           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 829           |
| Step_1-MinReturn        | 10.9          |
| Step_1-NumTrajs         | 1023          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 190           |
| Time                    | 5.17e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.82          |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007521926   |
| n_timesteps             | 238720000     |
-------------------------------------------

 ---------------- Iteration 746 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 746           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.007125031  |
| LossBefore              | 1.0628766e-09 |
| MeanKL                  | 0.0073921606  |
| MeanKLBefore            | 7.4498088e-09 |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.39935347    |
| Step_0-AverageReturn    | 481           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 819           |
| Step_0-MinReturn        | 12.1          |
| Step_0-NumTrajs         | 940           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.399314      |
| Step_1-AverageReturn    | 489           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 926           |
| Step_1-MinReturn        | 22            |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 178           |
| Time                    | 5.18e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.788         |
| Time-Sampling           | 56.8          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.007125032   |
| n_timesteps             | 239040000     |
-------------------------------------------

 ---------------- Iteration 747 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 747            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.007242414   |
| LossBefore              | -9.148998e-09  |
| MeanKL                  | 0.007964604    |
| MeanKLBefore            | -5.9614536e-09 |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.39873937     |
| Step_0-AverageReturn    | 486            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 822            |
| Step_0-MinReturn        | 23.3           |
| Step_0-NumTrajs         | 939            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 158            |
| Step_1-AverageDiscou... | 190            |
| Step_1-AveragePolicyStd | 0.3985843      |
| Step_1-AverageReturn    | 483            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 867            |
| Step_1-MinReturn        | 33.4           |
| Step_1-NumTrajs         | 977            |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 172            |
| Time                    | 5.18e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.787          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007242405    |
| n_timesteps             | 239360000      |
--------------------------------------------

 ---------------- Iteration 748 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 748           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.006707573  |
| LossBefore              | 3.5548766e-09 |
| MeanKL                  | 0.007983809   |
| MeanKLBefore            | 5.9589746e-09 |
| Step_0-AverageDiscou... | 173           |
| Step_0-AveragePolicyStd | 0.39827254    |
| Step_0-AverageReturn    | 431           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 814           |
| Step_0-MinReturn        | 15.5          |
| Step_0-NumTrajs         | 1022          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 192           |
| Step_1-AverageDiscou... | 178           |
| Step_1-AveragePolicyStd | 0.3981939     |
| Step_1-AverageReturn    | 445           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 855           |
| Step_1-MinReturn        | 16.3          |
| Step_1-NumTrajs         | 1033          |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 186           |
| Time                    | 5.19e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.821         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0067075766  |
| n_timesteps             | 239680000     |
-------------------------------------------

 ---------------- Iteration 749 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 749           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0073919026 |
| LossBefore              | -2.04974e-09  |
| MeanKL                  | 0.008219505   |
| MeanKLBefore            | 8.943071e-09  |
| Step_0-AverageDiscou... | 171           |
| Step_0-AveragePolicyStd | 0.39881113    |
| Step_0-AverageReturn    | 411           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 870           |
| Step_0-MinReturn        | 14.4          |
| Step_0-NumTrajs         | 1078          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 186           |
| Step_1-AverageDiscou... | 175           |
| Step_1-AveragePolicyStd | 0.39865458    |
| Step_1-AverageReturn    | 433           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 875           |
| Step_1-MinReturn        | 16            |
| Step_1-NumTrajs         | 1042          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 187           |
| Time                    | 5.2e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.827         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0073919008  |
| n_timesteps             | 240000000     |
-------------------------------------------

 ---------------- Iteration 750 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 750           |
| ItrTime                 | 71            |
| LossAfter               | -0.007893041  |
| LossBefore              | -7.300495e-09 |
| MeanKL                  | 0.008485267   |
| MeanKLBefore            | 1.6389773e-08 |
| Step_0-AverageDiscou... | 188           |
| Step_0-AveragePolicyStd | 0.39859775    |
| Step_0-AverageReturn    | 485           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 883           |
| Step_0-MinReturn        | 33.4          |
| Step_0-NumTrajs         | 946           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 186           |
| Step_1-AveragePolicyStd | 0.3985973     |
| Step_1-AverageReturn    | 479           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 869           |
| Step_1-MinReturn        | 24.8          |
| Step_1-NumTrajs         | 955           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 165           |
| Time                    | 5.2e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.788         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.007893033   |
| n_timesteps             | 240320000     |
-------------------------------------------

 ---------------- Iteration 751 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 751            |
| ItrTime                 | 71.5           |
| LossAfter               | -0.006789907   |
| LossBefore              | -1.3686538e-08 |
| MeanKL                  | 0.007895672    |
| MeanKLBefore            | 8.938537e-09   |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.39879063     |
| Step_0-AverageReturn    | 498            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 818            |
| Step_0-MinReturn        | 27.8           |
| Step_0-NumTrajs         | 917            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 155            |
| Step_1-AverageDiscou... | 186            |
| Step_1-AveragePolicyStd | 0.39877903     |
| Step_1-AverageReturn    | 476            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 870            |
| Step_1-MinReturn        | 34.1           |
| Step_1-NumTrajs         | 994            |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 179            |
| Time                    | 5.21e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.799          |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.0067898934   |
| n_timesteps             | 240640000      |
--------------------------------------------

 ---------------- Iteration 752 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 752           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.007065095  |
| LossBefore              | -7.870142e-09 |
| MeanKL                  | 0.0070494837  |
| MeanKLBefore            | 1.0431508e-08 |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.39804927    |
| Step_0-AverageReturn    | 464           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 800           |
| Step_0-MinReturn        | 5.54          |
| Step_0-NumTrajs         | 982           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.39793223    |
| Step_1-AverageReturn    | 453           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 845           |
| Step_1-MinReturn        | 7.66          |
| Step_1-NumTrajs         | 1002          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 173           |
| Time                    | 5.22e+04      |
| Time-InnerStep          | 0.167         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.814         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007065087   |
| n_timesteps             | 240960000     |
-------------------------------------------

 ---------------- Iteration 753 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 753            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0071265823  |
| LossBefore              | -1.0152064e-08 |
| MeanKL                  | 0.007479829    |
| MeanKLBefore            | -8.939513e-09  |
| Step_0-AverageDiscou... | 186            |
| Step_0-AveragePolicyStd | 0.39764917     |
| Step_0-AverageReturn    | 475            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 805            |
| Step_0-MinReturn        | 9.55           |
| Step_0-NumTrajs         | 967            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 171            |
| Step_1-AverageDiscou... | 184            |
| Step_1-AveragePolicyStd | 0.39761055     |
| Step_1-AverageReturn    | 469            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 821            |
| Step_1-MinReturn        | 20.3           |
| Step_1-NumTrajs         | 981            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.23e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.007126572    |
| n_timesteps             | 241280000      |
--------------------------------------------

 ---------------- Iteration 754 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 754          |
| ItrTime                 | 71.4         |
| LossAfter               | -0.008002868 |
| LossBefore              | 5.970615e-09 |
| MeanKL                  | 0.00878372   |
| MeanKLBefore            | 7.452013e-09 |
| Step_0-AverageDiscou... | 183          |
| Step_0-AveragePolicyStd | 0.39751044   |
| Step_0-AverageReturn    | 484          |
| Step_0-EnvExecTime      | 23.8         |
| Step_0-MaxReturn        | 853          |
| Step_0-MinReturn        | 19           |
| Step_0-NumTrajs         | 939          |
| Step_0-PolicyExecTime   | 1.35         |
| Step_0-StdReturn        | 182          |
| Step_1-AverageDiscou... | 182          |
| Step_1-AveragePolicyStd | 0.39737982   |
| Step_1-AverageReturn    | 476          |
| Step_1-EnvExecTime      | 24.1         |
| Step_1-MaxReturn        | 847          |
| Step_1-MinReturn        | 19.8         |
| Step_1-NumTrajs         | 980          |
| Step_1-PolicyExecTime   | 4.22         |
| Step_1-StdReturn        | 190          |
| Time                    | 5.23e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.816        |
| Time-Sampling           | 56.8         |
| Time-TotalInner         | 57.8         |
| dLoss                   | 0.0080028735 |
| n_timesteps             | 241600000    |
------------------------------------------

 ---------------- Iteration 755 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 755            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007037247   |
| LossBefore              | -1.0118368e-08 |
| MeanKL                  | 0.007910764    |
| MeanKLBefore            | 1.04304885e-08 |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.39787784     |
| Step_0-AverageReturn    | 488            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 842            |
| Step_0-MinReturn        | 12             |
| Step_0-NumTrajs         | 938            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 160            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.39773265     |
| Step_1-AverageReturn    | 478            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 966            |
| Step_1-MinReturn        | 11.5           |
| Step_1-NumTrajs         | 981            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 171            |
| Time                    | 5.24e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.79           |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.007037237    |
| n_timesteps             | 241920000      |
--------------------------------------------

 ---------------- Iteration 756 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 756            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.0072444035  |
| LossBefore              | 3.9582373e-09  |
| MeanKL                  | 0.0077931187   |
| MeanKLBefore            | -1.4912651e-09 |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.3985606      |
| Step_0-AverageReturn    | 497            |
| Step_0-EnvExecTime      | 24.3           |
| Step_0-MaxReturn        | 803            |
| Step_0-MinReturn        | 41.2           |
| Step_0-NumTrajs         | 932            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 159            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.39851645     |
| Step_1-AverageReturn    | 479            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 829            |
| Step_1-MinReturn        | 24.4           |
| Step_1-NumTrajs         | 995            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 175            |
| Time                    | 5.25e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.816          |
| Time-Sampling           | 57.2           |
| Time-TotalInner         | 58.3           |
| dLoss                   | 0.0072444077   |
| n_timesteps             | 242240000      |
--------------------------------------------

 ---------------- Iteration 757 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 757            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0074586263  |
| LossBefore              | -2.5270814e-09 |
| MeanKL                  | 0.007965587    |
| MeanKLBefore            | 5.9601217e-09  |
| Step_0-AverageDiscou... | 183            |
| Step_0-AveragePolicyStd | 0.3989057      |
| Step_0-AverageReturn    | 474            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 803            |
| Step_0-MinReturn        | 21.6           |
| Step_0-NumTrajs         | 921            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 158            |
| Step_1-AverageDiscou... | 174            |
| Step_1-AveragePolicyStd | 0.3989675      |
| Step_1-AverageReturn    | 434            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 833            |
| Step_1-MinReturn        | 8.06           |
| Step_1-NumTrajs         | 1012           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.25e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.78           |
| Time-Sampling           | 56.8           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.007458624    |
| n_timesteps             | 242560000      |
--------------------------------------------

 ---------------- Iteration 758 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 758           |
| ItrTime                 | 68.4          |
| LossAfter               | -0.0077169677 |
| LossBefore              | 9.243496e-09  |
| MeanKL                  | 0.0077560646  |
| MeanKLBefore            | 4.469494e-09  |
| Step_0-AverageDiscou... | 172           |
| Step_0-AveragePolicyStd | 0.39932355    |
| Step_0-AverageReturn    | 410           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 790           |
| Step_0-MinReturn        | 29.2          |
| Step_0-NumTrajs         | 1116          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 191           |
| Step_1-AverageDiscou... | 169           |
| Step_1-AveragePolicyStd | 0.3992319     |
| Step_1-AverageReturn    | 393           |
| Step_1-EnvExecTime      | 22.6          |
| Step_1-MaxReturn        | 864           |
| Step_1-MinReturn        | 3.66          |
| Step_1-NumTrajs         | 1142          |
| Step_1-PolicyExecTime   | 4.03          |
| Step_1-StdReturn        | 183           |
| Time                    | 5.26e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.86          |
| Time-Sampling           | 53.7          |
| Time-TotalInner         | 54.7          |
| dLoss                   | 0.007716977   |
| n_timesteps             | 242880000     |
-------------------------------------------

 ---------------- Iteration 759 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 759           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0070378794 |
| LossBefore              | -9.52304e-10  |
| MeanKL                  | 0.007520575   |
| MeanKLBefore            | 7.037926e-13  |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.39977896    |
| Step_0-AverageReturn    | 451           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 848           |
| Step_0-MinReturn        | 14.7          |
| Step_0-NumTrajs         | 990           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 178           |
| Step_1-AverageDiscou... | 185           |
| Step_1-AveragePolicyStd | 0.39976478    |
| Step_1-AverageReturn    | 470           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 798           |
| Step_1-MinReturn        | 22.3          |
| Step_1-NumTrajs         | 984           |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 175           |
| Time                    | 5.27e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.0070378785  |
| n_timesteps             | 243200000     |
-------------------------------------------

 ---------------- Iteration 760 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 760           |
| ItrTime                 | 70.9          |
| LossAfter               | -0.0073795095 |
| LossBefore              | 6.9986705e-09 |
| MeanKL                  | 0.007990025   |
| MeanKLBefore            | -4.469638e-09 |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.3998621     |
| Step_0-AverageReturn    | 462           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 803           |
| Step_0-MinReturn        | 21            |
| Step_0-NumTrajs         | 993           |
| Step_0-PolicyExecTime   | 1.71          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 184           |
| Step_1-AveragePolicyStd | 0.3998485     |
| Step_1-AverageReturn    | 461           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 778           |
| Step_1-MinReturn        | 23.3          |
| Step_1-NumTrajs         | 998           |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 171           |
| Time                    | 5.27e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.805         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.0073795165  |
| n_timesteps             | 243520000     |
-------------------------------------------

 ---------------- Iteration 761 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 761           |
| ItrTime                 | 71            |
| LossAfter               | -0.0072383424 |
| LossBefore              | 3.3283858e-09 |
| MeanKL                  | 0.0072938963  |
| MeanKLBefore            | -8.940631e-09 |
| Step_0-AverageDiscou... | 182           |
| Step_0-AveragePolicyStd | 0.40016672    |
| Step_0-AverageReturn    | 476           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 809           |
| Step_0-MinReturn        | 7.67          |
| Step_0-NumTrajs         | 963           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 192           |
| Step_1-AverageDiscou... | 181           |
| Step_1-AveragePolicyStd | 0.4001236     |
| Step_1-AverageReturn    | 465           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 868           |
| Step_1-MinReturn        | 10.6          |
| Step_1-NumTrajs         | 1006          |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 202           |
| Time                    | 5.28e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.814         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.0072383457  |
| n_timesteps             | 243840000     |
-------------------------------------------

 ---------------- Iteration 762 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 762            |
| ItrTime                 | 71.8           |
| LossAfter               | -0.0072749928  |
| LossBefore              | -4.6774646e-09 |
| MeanKL                  | 0.0080273915   |
| MeanKLBefore            | 7.449023e-09   |
| Step_0-AverageDiscou... | 187            |
| Step_0-AveragePolicyStd | 0.39917737     |
| Step_0-AverageReturn    | 483            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 796            |
| Step_0-MinReturn        | 25.5           |
| Step_0-NumTrajs         | 954            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 168            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.39916003     |
| Step_1-AverageReturn    | 486            |
| Step_1-EnvExecTime      | 24.3           |
| Step_1-MaxReturn        | 830            |
| Step_1-MinReturn        | 4.35           |
| Step_1-NumTrajs         | 952            |
| Step_1-PolicyExecTime   | 4.27           |
| Step_1-StdReturn        | 165            |
| Time                    | 5.29e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.812          |
| Time-Sampling           | 57.2           |
| Time-TotalInner         | 58.2           |
| dLoss                   | 0.007274988    |
| n_timesteps             | 244160000      |
--------------------------------------------

 ---------------- Iteration 763 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 763            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.007304441   |
| LossBefore              | -1.4022442e-09 |
| MeanKL                  | 0.00751206     |
| MeanKLBefore            | -5.9600653e-09 |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.39928904     |
| Step_0-AverageReturn    | 491            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 793            |
| Step_0-MinReturn        | 13.3           |
| Step_0-NumTrajs         | 947            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 173            |
| Step_1-AverageDiscou... | 184            |
| Step_1-AveragePolicyStd | 0.39918587     |
| Step_1-AverageReturn    | 463            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 797            |
| Step_1-MinReturn        | 17.6           |
| Step_1-NumTrajs         | 1016           |
| Step_1-PolicyExecTime   | 4.26           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.3e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.8           |
| Time-OuterStep          | 13.8           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.00730444     |
| n_timesteps             | 244480000      |
--------------------------------------------

 ---------------- Iteration 764 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 764            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0076540173  |
| LossBefore              | 4.0535966e-10  |
| MeanKL                  | 0.009505406    |
| MeanKLBefore            | -1.0427911e-08 |
| Step_0-AverageDiscou... | 180            |
| Step_0-AveragePolicyStd | 0.39948496     |
| Step_0-AverageReturn    | 452            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 811            |
| Step_0-MinReturn        | 22             |
| Step_0-NumTrajs         | 1017           |
| Step_0-PolicyExecTime   | 1.29           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 178            |
| Step_1-AveragePolicyStd | 0.39928666     |
| Step_1-AverageReturn    | 436            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 879            |
| Step_1-MinReturn        | 9.04           |
| Step_1-NumTrajs         | 1054           |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 183            |
| Time                    | 5.3e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.834          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.007654018    |
| n_timesteps             | 244800000      |
--------------------------------------------

 ---------------- Iteration 765 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 765            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.0074566067  |
| LossBefore              | -1.26897e-09   |
| MeanKL                  | 0.007928867    |
| MeanKLBefore            | -1.6388418e-08 |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.39967328     |
| Step_0-AverageReturn    | 495            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 825            |
| Step_0-MinReturn        | 20.5           |
| Step_0-NumTrajs         | 954            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 165            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.39963716     |
| Step_1-AverageReturn    | 482            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 861            |
| Step_1-MinReturn        | 26.7           |
| Step_1-NumTrajs         | 1007           |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 176            |
| Time                    | 5.31e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.0074566053   |
| n_timesteps             | 245120000      |
--------------------------------------------

 ---------------- Iteration 766 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 766          |
| ItrTime                 | 70.7         |
| LossAfter               | -0.006951943 |
| LossBefore              | 5.12771e-09  |
| MeanKL                  | 0.007590884  |
| MeanKLBefore            | 1.489889e-08 |
| Step_0-AverageDiscou... | 190          |
| Step_0-AveragePolicyStd | 0.39963487   |
| Step_0-AverageReturn    | 496          |
| Step_0-EnvExecTime      | 23.9         |
| Step_0-MaxReturn        | 854          |
| Step_0-MinReturn        | 23           |
| Step_0-NumTrajs         | 942          |
| Step_0-PolicyExecTime   | 1.36         |
| Step_0-StdReturn        | 163          |
| Step_1-AverageDiscou... | 180          |
| Step_1-AveragePolicyStd | 0.39952102   |
| Step_1-AverageReturn    | 449          |
| Step_1-EnvExecTime      | 23.4         |
| Step_1-MaxReturn        | 834          |
| Step_1-MinReturn        | 15.7         |
| Step_1-NumTrajs         | 1030         |
| Step_1-PolicyExecTime   | 4.11         |
| Step_1-StdReturn        | 183          |
| Time                    | 5.32e+04     |
| Time-InnerStep          | 0.164        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.789        |
| Time-Sampling           | 56.1         |
| Time-TotalInner         | 57.1         |
| dLoss                   | 0.006951948  |
| n_timesteps             | 245440000    |
------------------------------------------

 ---------------- Iteration 767 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 767           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0070471875 |
| LossBefore              | -5.325003e-09 |
| MeanKL                  | 0.007848054   |
| MeanKLBefore            | 4.470084e-09  |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.3993487     |
| Step_0-AverageReturn    | 469           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 833           |
| Step_0-MinReturn        | 19.8          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 171           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.39920574    |
| Step_1-AverageReturn    | 463           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 844           |
| Step_1-MinReturn        | 25.6          |
| Step_1-NumTrajs         | 1021          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 187           |
| Time                    | 5.32e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.811         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0070471824  |
| n_timesteps             | 245760000     |
-------------------------------------------

 ---------------- Iteration 768 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 768            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0072943084  |
| LossBefore              | 2.7495624e-09  |
| MeanKL                  | 0.007899122    |
| MeanKLBefore            | -1.5411672e-12 |
| Step_0-AverageDiscou... | 190            |
| Step_0-AveragePolicyStd | 0.3985161      |
| Step_0-AverageReturn    | 479            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 822            |
| Step_0-MinReturn        | 32.5           |
| Step_0-NumTrajs         | 977            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 185            |
| Step_1-AveragePolicyStd | 0.3985149      |
| Step_1-AverageReturn    | 472            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 813            |
| Step_1-MinReturn        | 29.2           |
| Step_1-NumTrajs         | 990            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 180            |
| Time                    | 5.33e+04       |
| Time-InnerStep          | 0.179          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.825          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.007294311    |
| n_timesteps             | 246080000      |
--------------------------------------------

 ---------------- Iteration 769 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 769            |
| ItrTime                 | 68.6           |
| LossAfter               | -0.007233271   |
| LossBefore              | -7.1205717e-09 |
| MeanKL                  | 0.00839        |
| MeanKLBefore            | -5.959245e-09  |
| Step_0-AverageDiscou... | 188            |
| Step_0-AveragePolicyStd | 0.39904383     |
| Step_0-AverageReturn    | 473            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 827            |
| Step_0-MinReturn        | 26.3           |
| Step_0-NumTrajs         | 996            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.39896193     |
| Step_1-AverageReturn    | 435            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 844            |
| Step_1-MinReturn        | 2.42           |
| Step_1-NumTrajs         | 1089           |
| Step_1-PolicyExecTime   | 4.05           |
| Step_1-StdReturn        | 192            |
| Time                    | 5.34e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.82           |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55             |
| dLoss                   | 0.007233264    |
| n_timesteps             | 246400000      |
--------------------------------------------

 ---------------- Iteration 770 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 770           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0072338185 |
| LossBefore              | 5.8007195e-09 |
| MeanKL                  | 0.007849972   |
| MeanKLBefore            | 2.9783902e-09 |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.39937642    |
| Step_0-AverageReturn    | 479           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 881           |
| Step_0-MinReturn        | 15.9          |
| Step_0-NumTrajs         | 960           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 180           |
| Step_1-AverageDiscou... | 184           |
| Step_1-AveragePolicyStd | 0.3991631     |
| Step_1-AverageReturn    | 468           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 801           |
| Step_1-MinReturn        | 3.22          |
| Step_1-NumTrajs         | 993           |
| Step_1-PolicyExecTime   | 4.26          |
| Step_1-StdReturn        | 178           |
| Time                    | 5.35e+04      |
| Time-InnerStep          | 0.173         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.007233824   |
| n_timesteps             | 246720000     |
-------------------------------------------

 ---------------- Iteration 771 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 771           |
| ItrTime                 | 70            |
| LossAfter               | -0.007161861  |
| LossBefore              | -7.813362e-10 |
| MeanKL                  | 0.007458794   |
| MeanKLBefore            | 8.940008e-09  |
| Step_0-AverageDiscou... | 192           |
| Step_0-AveragePolicyStd | 0.3991552     |
| Step_0-AverageReturn    | 496           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 782           |
| Step_0-MinReturn        | 17.4          |
| Step_0-NumTrajs         | 949           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 154           |
| Step_1-AverageDiscou... | 183           |
| Step_1-AveragePolicyStd | 0.399155      |
| Step_1-AverageReturn    | 460           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 845           |
| Step_1-MinReturn        | 22.9          |
| Step_1-NumTrajs         | 1009          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 177           |
| Time                    | 5.35e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.795         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.00716186    |
| n_timesteps             | 247040000     |
-------------------------------------------

 ---------------- Iteration 772 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 772            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0071709664  |
| LossBefore              | -1.1418559e-08 |
| MeanKL                  | 0.008375734    |
| MeanKLBefore            | 1.4900609e-09  |
| Step_0-AverageDiscou... | 193            |
| Step_0-AveragePolicyStd | 0.3997269      |
| Step_0-AverageReturn    | 490            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 813            |
| Step_0-MinReturn        | 31.7           |
| Step_0-NumTrajs         | 975            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 166            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.3996507      |
| Step_1-AverageReturn    | 467            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 899            |
| Step_1-MinReturn        | 26.9           |
| Step_1-NumTrajs         | 1043           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 182            |
| Time                    | 5.36e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.823          |
| Time-Sampling           | 54.8           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.0071709547   |
| n_timesteps             | 247360000      |
--------------------------------------------

 ---------------- Iteration 773 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 773           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.0076175286 |
| LossBefore              | 5.738927e-09  |
| MeanKL                  | 0.007992512   |
| MeanKLBefore            | 2.9794751e-09 |
| Step_0-AverageDiscou... | 190           |
| Step_0-AveragePolicyStd | 0.399605      |
| Step_0-AverageReturn    | 488           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 826           |
| Step_0-MinReturn        | 16.2          |
| Step_0-NumTrajs         | 958           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 160           |
| Step_1-AverageDiscou... | 178           |
| Step_1-AveragePolicyStd | 0.39957058    |
| Step_1-AverageReturn    | 456           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 865           |
| Step_1-MinReturn        | -0.542        |
| Step_1-NumTrajs         | 977           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 183           |
| Time                    | 5.37e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.797         |
| Time-Sampling           | 56.3          |
| Time-TotalInner         | 57.3          |
| dLoss                   | 0.007617534   |
| n_timesteps             | 247680000     |
-------------------------------------------

 ---------------- Iteration 774 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 774           |
| ItrTime                 | 70            |
| LossAfter               | -0.007407309  |
| LossBefore              | 2.1944309e-09 |
| MeanKL                  | 0.007992687   |
| MeanKLBefore            | -7.449242e-09 |
| Step_0-AverageDiscou... | 180           |
| Step_0-AveragePolicyStd | 0.3999367     |
| Step_0-AverageReturn    | 441           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 882           |
| Step_0-MinReturn        | 22.5          |
| Step_0-NumTrajs         | 1068          |
| Step_0-PolicyExecTime   | 1.27          |
| Step_0-StdReturn        | 187           |
| Step_1-AverageDiscou... | 181           |
| Step_1-AveragePolicyStd | 0.39992288    |
| Step_1-AverageReturn    | 440           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 845           |
| Step_1-MinReturn        | 22.8          |
| Step_1-NumTrajs         | 1073          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 182           |
| Time                    | 5.37e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.9           |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.0074073114  |
| n_timesteps             | 248000000     |
-------------------------------------------

 ---------------- Iteration 775 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 775            |
| ItrTime                 | 71             |
| LossAfter               | -0.0073136836  |
| LossBefore              | 1.081862e-08   |
| MeanKL                  | 0.008083929    |
| MeanKLBefore            | -2.9776017e-09 |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40012994     |
| Step_0-AverageReturn    | 512            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 888            |
| Step_0-MinReturn        | 18.4           |
| Step_0-NumTrajs         | 951            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 194            |
| Step_1-AveragePolicyStd | 0.40011004     |
| Step_1-AverageReturn    | 501            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 855            |
| Step_1-MinReturn        | 21.1           |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 171            |
| Time                    | 5.38e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.796          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0073136943   |
| n_timesteps             | 248320000      |
--------------------------------------------

 ---------------- Iteration 776 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 776            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.006848348   |
| LossBefore              | 9.048674e-09   |
| MeanKL                  | 0.0074227555   |
| MeanKLBefore            | -2.9786116e-09 |
| Step_0-AverageDiscou... | 179            |
| Step_0-AveragePolicyStd | 0.4004397      |
| Step_0-AverageReturn    | 439            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 878            |
| Step_0-MinReturn        | 15.1           |
| Step_0-NumTrajs         | 1066           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 204            |
| Step_1-AverageDiscou... | 181            |
| Step_1-AveragePolicyStd | 0.4003747      |
| Step_1-AverageReturn    | 450            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 821            |
| Step_1-MinReturn        | 19.9           |
| Step_1-NumTrajs         | 1034           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 187            |
| Time                    | 5.39e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.826          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 56             |
| dLoss                   | 0.0068483567   |
| n_timesteps             | 248640000      |
--------------------------------------------

 ---------------- Iteration 777 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 777           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.0072587407 |
| LossBefore              | 6.6260126e-09 |
| MeanKL                  | 0.0075908927  |
| MeanKLBefore            | 1.1918223e-08 |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.40064704    |
| Step_0-AverageReturn    | 473           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 807           |
| Step_0-MinReturn        | 18.8          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 178           |
| Step_1-AverageDiscou... | 186           |
| Step_1-AveragePolicyStd | 0.4005985     |
| Step_1-AverageReturn    | 477           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 839           |
| Step_1-MinReturn        | 9.79          |
| Step_1-NumTrajs         | 984           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 180           |
| Time                    | 5.39e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.824         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0072587472  |
| n_timesteps             | 248960000     |
-------------------------------------------

 ---------------- Iteration 778 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 778           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007270597  |
| LossBefore              | 6.1790146e-09 |
| MeanKL                  | 0.008674001   |
| MeanKLBefore            | 1.787945e-08  |
| Step_0-AverageDiscou... | 182           |
| Step_0-AveragePolicyStd | 0.40148544    |
| Step_0-AverageReturn    | 466           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 786           |
| Step_0-MinReturn        | 15.6          |
| Step_0-NumTrajs         | 958           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 178           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.4015005     |
| Step_1-AverageReturn    | 455           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 815           |
| Step_1-MinReturn        | 16.7          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 180           |
| Time                    | 5.4e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.796         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.007270603   |
| n_timesteps             | 249280000     |
-------------------------------------------

 ---------------- Iteration 779 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 779            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0078079565  |
| LossBefore              | -1.2183992e-08 |
| MeanKL                  | 0.00824591     |
| MeanKLBefore            | 1.042622e-08   |
| Step_0-AverageDiscou... | 182            |
| Step_0-AveragePolicyStd | 0.40110144     |
| Step_0-AverageReturn    | 457            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 830            |
| Step_0-MinReturn        | 11.1           |
| Step_0-NumTrajs         | 1009           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 184            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.40105292     |
| Step_1-AverageReturn    | 445            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 926            |
| Step_1-MinReturn        | 38.6           |
| Step_1-NumTrajs         | 1052           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 199            |
| Time                    | 5.41e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.816          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0078079444   |
| n_timesteps             | 249600000      |
--------------------------------------------

 ---------------- Iteration 780 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 780            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0072244974  |
| LossBefore              | -9.520595e-09  |
| MeanKL                  | 0.007896192    |
| MeanKLBefore            | -4.4692947e-09 |
| Step_0-AverageDiscou... | 183            |
| Step_0-AveragePolicyStd | 0.40124628     |
| Step_0-AverageReturn    | 465            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 817            |
| Step_0-MinReturn        | 21.9           |
| Step_0-NumTrajs         | 984            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 183            |
| Step_1-AveragePolicyStd | 0.40116197     |
| Step_1-AverageReturn    | 467            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 895            |
| Step_1-MinReturn        | 22.6           |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 180            |
| Time                    | 5.42e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.825          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.007224488    |
| n_timesteps             | 249920000      |
--------------------------------------------

 ---------------- Iteration 781 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 781           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.007500738  |
| LossBefore              | 5.938806e-09  |
| MeanKL                  | 0.008694095   |
| MeanKLBefore            | 1.0429746e-08 |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.40141314    |
| Step_0-AverageReturn    | 475           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 803           |
| Step_0-MinReturn        | 33.5          |
| Step_0-NumTrajs         | 962           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 168           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.40133312    |
| Step_1-AverageReturn    | 457           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 860           |
| Step_1-MinReturn        | 38.8          |
| Step_1-NumTrajs         | 995           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 175           |
| Time                    | 5.42e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.817         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007500744   |
| n_timesteps             | 250240000     |
-------------------------------------------

 ---------------- Iteration 782 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 782            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0071340697  |
| LossBefore              | -2.5693463e-09 |
| MeanKL                  | 0.00842775     |
| MeanKLBefore            | 5.9607457e-09  |
| Step_0-AverageDiscou... | 187            |
| Step_0-AveragePolicyStd | 0.40165347     |
| Step_0-AverageReturn    | 482            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 827            |
| Step_0-MinReturn        | 16             |
| Step_0-NumTrajs         | 966            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 183            |
| Step_1-AverageDiscou... | 190            |
| Step_1-AveragePolicyStd | 0.40171218     |
| Step_1-AverageReturn    | 491            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 842            |
| Step_1-MinReturn        | 24.7           |
| Step_1-NumTrajs         | 957            |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 169            |
| Time                    | 5.43e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007134067    |
| n_timesteps             | 250560000      |
--------------------------------------------

 ---------------- Iteration 783 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 783           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.007219563  |
| LossBefore              | 5.114519e-10  |
| MeanKL                  | 0.007797352   |
| MeanKLBefore            | -2.980426e-09 |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.40168902    |
| Step_0-AverageReturn    | 474           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 894           |
| Step_0-MinReturn        | 30.9          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 190           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.40162316    |
| Step_1-AverageReturn    | 447           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 823           |
| Step_1-MinReturn        | 11.3          |
| Step_1-NumTrajs         | 1035          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 191           |
| Time                    | 5.44e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.874         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0072195632  |
| n_timesteps             | 250880000     |
-------------------------------------------

 ---------------- Iteration 784 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 784            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0070057     |
| LossBefore              | 1.3977682e-10  |
| MeanKL                  | 0.007681983    |
| MeanKLBefore            | -1.1920951e-08 |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.40212214     |
| Step_0-AverageReturn    | 497            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 858            |
| Step_0-MinReturn        | 19.8           |
| Step_0-NumTrajs         | 929            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 170            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.40210223     |
| Step_1-AverageReturn    | 481            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 868            |
| Step_1-MinReturn        | 19.7           |
| Step_1-NumTrajs         | 969            |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 178            |
| Time                    | 5.44e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0070057      |
| n_timesteps             | 251200000      |
--------------------------------------------

 ---------------- Iteration 785 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 785           |
| ItrTime                 | 71            |
| LossAfter               | -0.0072841197 |
| LossBefore              | 9.690152e-09  |
| MeanKL                  | 0.007557293   |
| MeanKLBefore            | 1.3410383e-08 |
| Step_0-AverageDiscou... | 191           |
| Step_0-AveragePolicyStd | 0.40170735    |
| Step_0-AverageReturn    | 502           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 872           |
| Step_0-MinReturn        | 36.4          |
| Step_0-NumTrajs         | 928           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 182           |
| Step_1-AveragePolicyStd | 0.40171015    |
| Step_1-AverageReturn    | 461           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 869           |
| Step_1-MinReturn        | 13.1          |
| Step_1-NumTrajs         | 1001          |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 188           |
| Time                    | 5.45e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.779         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.0072841295  |
| n_timesteps             | 251520000     |
-------------------------------------------

 ---------------- Iteration 786 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 786            |
| ItrTime                 | 69.3           |
| LossAfter               | -0.007509984   |
| LossBefore              | 1.20961685e-08 |
| MeanKL                  | 0.007869791    |
| MeanKLBefore            | 8.941144e-09   |
| Step_0-AverageDiscou... | 191            |
| Step_0-AveragePolicyStd | 0.40214235     |
| Step_0-AverageReturn    | 482            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 847            |
| Step_0-MinReturn        | 15.5           |
| Step_0-NumTrajs         | 976            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 193            |
| Step_1-AveragePolicyStd | 0.40211278     |
| Step_1-AverageReturn    | 494            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 851            |
| Step_1-MinReturn        | 20             |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 175            |
| Time                    | 5.46e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.007509996    |
| n_timesteps             | 251840000      |
--------------------------------------------

 ---------------- Iteration 787 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 787            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0073985457  |
| LossBefore              | -6.5052523e-09 |
| MeanKL                  | 0.008737859    |
| MeanKLBefore            | 8.939162e-09   |
| Step_0-AverageDiscou... | 191            |
| Step_0-AveragePolicyStd | 0.40258732     |
| Step_0-AverageReturn    | 487            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 825            |
| Step_0-MinReturn        | 20.4           |
| Step_0-NumTrajs         | 969            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 179            |
| Step_1-AverageDiscou... | 186            |
| Step_1-AveragePolicyStd | 0.40257502     |
| Step_1-AverageReturn    | 463            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 843            |
| Step_1-MinReturn        | 15.4           |
| Step_1-NumTrajs         | 1033           |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 193            |
| Time                    | 5.46e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.007398539    |
| n_timesteps             | 252160000      |
--------------------------------------------

 ---------------- Iteration 788 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 788            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.007479777   |
| LossBefore              | -3.4606304e-09 |
| MeanKL                  | 0.008769788    |
| MeanKLBefore            | 2.9799314e-09  |
| Step_0-AverageDiscou... | 186            |
| Step_0-AveragePolicyStd | 0.40298373     |
| Step_0-AverageReturn    | 480            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 797            |
| Step_0-MinReturn        | 8.22           |
| Step_0-NumTrajs         | 956            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 176            |
| Step_1-AverageDiscou... | 186            |
| Step_1-AveragePolicyStd | 0.40304017     |
| Step_1-AverageReturn    | 473            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 840            |
| Step_1-MinReturn        | 25.8           |
| Step_1-NumTrajs         | 985            |
| Step_1-PolicyExecTime   | 4.56           |
| Step_1-StdReturn        | 178            |
| Time                    | 5.47e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.828          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007479774    |
| n_timesteps             | 252480000      |
--------------------------------------------

 ---------------- Iteration 789 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 789           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.0070780804 |
| LossBefore              | 3.5693897e-09 |
| MeanKL                  | 0.0074313222  |
| MeanKLBefore            | -8.941474e-09 |
| Step_0-AverageDiscou... | 181           |
| Step_0-AveragePolicyStd | 0.40274802    |
| Step_0-AverageReturn    | 467           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 890           |
| Step_0-MinReturn        | 7.78          |
| Step_0-NumTrajs         | 979           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 190           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.40266964    |
| Step_1-AverageReturn    | 461           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 833           |
| Step_1-MinReturn        | 10.2          |
| Step_1-NumTrajs         | 998           |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 187           |
| Time                    | 5.48e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.831         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007078084   |
| n_timesteps             | 252800000     |
-------------------------------------------

 ---------------- Iteration 790 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 790           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0070981197 |
| LossBefore              | 2.1927087e-09 |
| MeanKL                  | 0.008353614   |
| MeanKLBefore            | 2.9773728e-09 |
| Step_0-AverageDiscou... | 188           |
| Step_0-AveragePolicyStd | 0.40274164    |
| Step_0-AverageReturn    | 482           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 795           |
| Step_0-MinReturn        | 15.4          |
| Step_0-NumTrajs         | 953           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 166           |
| Step_1-AverageDiscou... | 187           |
| Step_1-AveragePolicyStd | 0.40278873    |
| Step_1-AverageReturn    | 474           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 832           |
| Step_1-MinReturn        | 34            |
| Step_1-NumTrajs         | 986           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 173           |
| Time                    | 5.49e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.786         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007098122   |
| n_timesteps             | 253120000     |
-------------------------------------------

 ---------------- Iteration 791 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 791          |
| ItrTime                 | 70           |
| LossAfter               | -0.006984928 |
| LossBefore              | 8.525221e-09 |
| MeanKL                  | 0.007720302  |
| MeanKLBefore            | 7.448363e-09 |
| Step_0-AverageDiscou... | 178          |
| Step_0-AveragePolicyStd | 0.40222532   |
| Step_0-AverageReturn    | 443          |
| Step_0-EnvExecTime      | 23.1         |
| Step_0-MaxReturn        | 803          |
| Step_0-MinReturn        | 20.4         |
| Step_0-NumTrajs         | 1045         |
| Step_0-PolicyExecTime   | 1.33         |
| Step_0-StdReturn        | 194          |
| Step_1-AverageDiscou... | 179          |
| Step_1-AveragePolicyStd | 0.402183     |
| Step_1-AverageReturn    | 438          |
| Step_1-EnvExecTime      | 23.4         |
| Step_1-MaxReturn        | 901          |
| Step_1-MinReturn        | 20.4         |
| Step_1-NumTrajs         | 1069         |
| Step_1-PolicyExecTime   | 4.15         |
| Step_1-StdReturn        | 187          |
| Time                    | 5.49e+04     |
| Time-InnerStep          | 0.165        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.835        |
| Time-Sampling           | 55.3         |
| Time-TotalInner         | 56.4         |
| dLoss                   | 0.0069849365 |
| n_timesteps             | 253440000    |
------------------------------------------

 ---------------- Iteration 792 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 792           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0071202544 |
| LossBefore              | -9.85422e-09  |
| MeanKL                  | 0.007934454   |
| MeanKLBefore            | 4.4684336e-09 |
| Step_0-AverageDiscou... | 190           |
| Step_0-AveragePolicyStd | 0.40197003    |
| Step_0-AverageReturn    | 487           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 832           |
| Step_0-MinReturn        | 16.9          |
| Step_0-NumTrajs         | 962           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 189           |
| Step_1-AveragePolicyStd | 0.40189022    |
| Step_1-AverageReturn    | 479           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 922           |
| Step_1-MinReturn        | 17.6          |
| Step_1-NumTrajs         | 1000          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 180           |
| Time                    | 5.5e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.834         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0071202447  |
| n_timesteps             | 253760000     |
-------------------------------------------

 ---------------- Iteration 793 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 793           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.0070692645 |
| LossBefore              | 5.5688867e-09 |
| MeanKL                  | 0.007908611   |
| MeanKLBefore            | 4.4703117e-09 |
| Step_0-AverageDiscou... | 190           |
| Step_0-AveragePolicyStd | 0.40229255    |
| Step_0-AverageReturn    | 491           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 848           |
| Step_0-MinReturn        | 13.5          |
| Step_0-NumTrajs         | 940           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 186           |
| Step_1-AveragePolicyStd | 0.40214726    |
| Step_1-AverageReturn    | 467           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 878           |
| Step_1-MinReturn        | 26.2          |
| Step_1-NumTrajs         | 1017          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 195           |
| Time                    | 5.51e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.809         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.00706927    |
| n_timesteps             | 254080000     |
-------------------------------------------

 ---------------- Iteration 794 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 794            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.0069699674  |
| LossBefore              | 1.9152722e-09  |
| MeanKL                  | 0.008037578    |
| MeanKLBefore            | -7.4490787e-09 |
| Step_0-AverageDiscou... | 180            |
| Step_0-AveragePolicyStd | 0.4022838      |
| Step_0-AverageReturn    | 460            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 831            |
| Step_0-MinReturn        | 16.2           |
| Step_0-NumTrajs         | 978            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 181            |
| Step_1-AverageDiscou... | 183            |
| Step_1-AveragePolicyStd | 0.40217975     |
| Step_1-AverageReturn    | 458            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 886            |
| Step_1-MinReturn        | 17.7           |
| Step_1-NumTrajs         | 1011           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 186            |
| Time                    | 5.51e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.0069699693   |
| n_timesteps             | 254400000      |
--------------------------------------------

 ---------------- Iteration 795 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 795            |
| ItrTime                 | 68.5           |
| LossAfter               | -0.0071016685  |
| LossBefore              | 2.0398538e-09  |
| MeanKL                  | 0.007958256    |
| MeanKLBefore            | -1.4897445e-09 |
| Step_0-AverageDiscou... | 181            |
| Step_0-AveragePolicyStd | 0.40288857     |
| Step_0-AverageReturn    | 447            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 800            |
| Step_0-MinReturn        | 8.91           |
| Step_0-NumTrajs         | 1025           |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 192            |
| Step_1-AverageDiscou... | 179            |
| Step_1-AveragePolicyStd | 0.40277287     |
| Step_1-AverageReturn    | 437            |
| Step_1-EnvExecTime      | 22.4           |
| Step_1-MaxReturn        | 899            |
| Step_1-MinReturn        | 11.3           |
| Step_1-NumTrajs         | 1051           |
| Step_1-PolicyExecTime   | 3.99           |
| Step_1-StdReturn        | 197            |
| Time                    | 5.52e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.853          |
| Time-Sampling           | 53.8           |
| Time-TotalInner         | 54.9           |
| dLoss                   | 0.0071016704   |
| n_timesteps             | 254720000      |
--------------------------------------------

 ---------------- Iteration 796 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 796            |
| ItrTime                 | 70             |
| LossAfter               | -0.007199377   |
| LossBefore              | 2.6875973e-09  |
| MeanKL                  | 0.0074479906   |
| MeanKLBefore            | -1.4899271e-09 |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.40316415     |
| Step_0-AverageReturn    | 488            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 813            |
| Step_0-MinReturn        | 13.9           |
| Step_0-NumTrajs         | 969            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 193            |
| Step_1-AveragePolicyStd | 0.40309528     |
| Step_1-AverageReturn    | 492            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 913            |
| Step_1-MinReturn        | 10.9           |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.53e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.799          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0071993796   |
| n_timesteps             | 255040000      |
--------------------------------------------

 ---------------- Iteration 797 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 797            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.006745027   |
| LossBefore              | 2.7157046e-09  |
| MeanKL                  | 0.007540582    |
| MeanKLBefore            | -1.1918627e-08 |
| Step_0-AverageDiscou... | 193            |
| Step_0-AveragePolicyStd | 0.40332964     |
| Step_0-AverageReturn    | 501            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 875            |
| Step_0-MinReturn        | 14.1           |
| Step_0-NumTrajs         | 944            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 166            |
| Step_1-AverageDiscou... | 190            |
| Step_1-AveragePolicyStd | 0.4033141      |
| Step_1-AverageReturn    | 482            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 893            |
| Step_1-MinReturn        | 16.2           |
| Step_1-NumTrajs         | 998            |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.53e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.778          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.1           |
| dLoss                   | 0.0067450297   |
| n_timesteps             | 255360000      |
--------------------------------------------

 ---------------- Iteration 798 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 798            |
| ItrTime                 | 71             |
| LossAfter               | -0.007308848   |
| LossBefore              | -1.2766252e-08 |
| MeanKL                  | 0.008002624    |
| MeanKLBefore            | 8.941089e-09   |
| Step_0-AverageDiscou... | 201            |
| Step_0-AveragePolicyStd | 0.4036462      |
| Step_0-AverageReturn    | 528            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 878            |
| Step_0-MinReturn        | 23.7           |
| Step_0-NumTrajs         | 906            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 143            |
| Step_1-AverageDiscou... | 197            |
| Step_1-AveragePolicyStd | 0.4035966      |
| Step_1-AverageReturn    | 513            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 837            |
| Step_1-MinReturn        | 29.8           |
| Step_1-NumTrajs         | 949            |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 170            |
| Time                    | 5.54e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.787          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0073088356   |
| n_timesteps             | 255680000      |
--------------------------------------------

 ---------------- Iteration 799 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 799           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0076744943 |
| LossBefore              | 5.54274e-09   |
| MeanKL                  | 0.007836012   |
| MeanKLBefore            | 8.941501e-09  |
| Step_0-AverageDiscou... | 191           |
| Step_0-AveragePolicyStd | 0.40401492    |
| Step_0-AverageReturn    | 495           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 961           |
| Step_0-MinReturn        | 22.2          |
| Step_0-NumTrajs         | 938           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 175           |
| Step_1-AverageDiscou... | 187           |
| Step_1-AveragePolicyStd | 0.40395424    |
| Step_1-AverageReturn    | 474           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 850           |
| Step_1-MinReturn        | 22.9          |
| Step_1-NumTrajs         | 1003          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 181           |
| Time                    | 5.55e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.784         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0076745     |
| n_timesteps             | 256000000     |
-------------------------------------------

 ---------------- Iteration 800 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 800            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.007353753   |
| LossBefore              | -2.5986757e-09 |
| MeanKL                  | 0.0077237575   |
| MeanKLBefore            | -8.096634e-13  |
| Step_0-AverageDiscou... | 184            |
| Step_0-AveragePolicyStd | 0.40445948     |
| Step_0-AverageReturn    | 470            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 811            |
| Step_0-MinReturn        | 19.2           |
| Step_0-NumTrajs         | 971            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 190            |
| Step_1-AverageDiscou... | 180            |
| Step_1-AveragePolicyStd | 0.40430135     |
| Step_1-AverageReturn    | 450            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 863            |
| Step_1-MinReturn        | 10.2           |
| Step_1-NumTrajs         | 1030           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 203            |
| Time                    | 5.56e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.815          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.00735375     |
| n_timesteps             | 256320000      |
--------------------------------------------

 ---------------- Iteration 801 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 801           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.006852988  |
| LossBefore              | 1.0975898e-09 |
| MeanKL                  | 0.007733631   |
| MeanKLBefore            | 1.6390171e-08 |
| Step_0-AverageDiscou... | 191           |
| Step_0-AveragePolicyStd | 0.40441078    |
| Step_0-AverageReturn    | 489           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 849           |
| Step_0-MinReturn        | 26.2          |
| Step_0-NumTrajs         | 948           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 188           |
| Step_1-AveragePolicyStd | 0.40430805    |
| Step_1-AverageReturn    | 476           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 856           |
| Step_1-MinReturn        | 25.1          |
| Step_1-NumTrajs         | 978           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 170           |
| Time                    | 5.56e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.796         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.006852989   |
| n_timesteps             | 256640000     |
-------------------------------------------

 ---------------- Iteration 802 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 802           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007403807  |
| LossBefore              | 5.291048e-09  |
| MeanKL                  | 0.008585902   |
| MeanKLBefore            | -8.939367e-09 |
| Step_0-AverageDiscou... | 190           |
| Step_0-AveragePolicyStd | 0.40436026    |
| Step_0-AverageReturn    | 481           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 864           |
| Step_0-MinReturn        | 11.2          |
| Step_0-NumTrajs         | 986           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 180           |
| Step_1-AverageDiscou... | 185           |
| Step_1-AveragePolicyStd | 0.40432855    |
| Step_1-AverageReturn    | 460           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 837           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1025          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 180           |
| Time                    | 5.57e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.859         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.007403812   |
| n_timesteps             | 256960000     |
-------------------------------------------

 ---------------- Iteration 803 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 803           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0072725983 |
| LossBefore              | 9.829793e-10  |
| MeanKL                  | 0.008611233   |
| MeanKLBefore            | 1.1917988e-08 |
| Step_0-AverageDiscou... | 174           |
| Step_0-AveragePolicyStd | 0.4047682     |
| Step_0-AverageReturn    | 438           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 829           |
| Step_0-MinReturn        | 14            |
| Step_0-NumTrajs         | 1035          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 207           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.4046392     |
| Step_1-AverageReturn    | 451           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 839           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 1051          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 208           |
| Time                    | 5.58e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.817         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0072725993  |
| n_timesteps             | 257280000     |
-------------------------------------------

 ---------------- Iteration 804 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 804           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0072568143 |
| LossBefore              | -1.03537e-09  |
| MeanKL                  | 0.008218905   |
| MeanKLBefore            | 1.0431562e-08 |
| Step_0-AverageDiscou... | 179           |
| Step_0-AveragePolicyStd | 0.40486082    |
| Step_0-AverageReturn    | 457           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 777           |
| Step_0-MinReturn        | 23.2          |
| Step_0-NumTrajs         | 966           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 186           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.4048075     |
| Step_1-AverageReturn    | 458           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 875           |
| Step_1-MinReturn        | 8.31          |
| Step_1-NumTrajs         | 989           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 181           |
| Time                    | 5.58e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.824         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0072568133  |
| n_timesteps             | 257600000     |
-------------------------------------------

 ---------------- Iteration 805 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 805            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0071900534  |
| LossBefore              | -6.3978023e-09 |
| MeanKL                  | 0.008323819    |
| MeanKLBefore            | 1.4907429e-09  |
| Step_0-AverageDiscou... | 190            |
| Step_0-AveragePolicyStd | 0.405105       |
| Step_0-AverageReturn    | 496            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 867            |
| Step_0-MinReturn        | 15.7           |
| Step_0-NumTrajs         | 930            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 168            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.40498564     |
| Step_1-AverageReturn    | 473            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 824            |
| Step_1-MinReturn        | 18             |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 181            |
| Time                    | 5.59e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.007190047    |
| n_timesteps             | 257920000      |
--------------------------------------------

 ---------------- Iteration 806 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 806            |
| ItrTime                 | 70             |
| LossAfter               | -0.007733793   |
| LossBefore              | 2.8859262e-09  |
| MeanKL                  | 0.008447627    |
| MeanKLBefore            | -7.4505566e-09 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.4047305      |
| Step_0-AverageReturn    | 475            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 835            |
| Step_0-MinReturn        | 26.7           |
| Step_0-NumTrajs         | 957            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.40461192     |
| Step_1-AverageReturn    | 488            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 841            |
| Step_1-MinReturn        | 22.2           |
| Step_1-NumTrajs         | 957            |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 170            |
| Time                    | 5.6e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.775          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007733796    |
| n_timesteps             | 258240000      |
--------------------------------------------

 ---------------- Iteration 807 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 807            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.007118699   |
| LossBefore              | -4.6904675e-10 |
| MeanKL                  | 0.0072262683   |
| MeanKLBefore            | 1.19177175e-08 |
| Step_0-AverageDiscou... | 196            |
| Step_0-AveragePolicyStd | 0.40474015     |
| Step_0-AverageReturn    | 530            |
| Step_0-EnvExecTime      | 24.4           |
| Step_0-MaxReturn        | 886            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 880            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 154            |
| Step_1-AverageDiscou... | 192            |
| Step_1-AveragePolicyStd | 0.40468016     |
| Step_1-AverageReturn    | 493            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 882            |
| Step_1-MinReturn        | 16             |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 175            |
| Time                    | 5.6e+04        |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.821          |
| Time-Sampling           | 57.1           |
| Time-TotalInner         | 58.1           |
| dLoss                   | 0.0071186987   |
| n_timesteps             | 258560000      |
--------------------------------------------

 ---------------- Iteration 808 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 808           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0075605167 |
| LossBefore              | -9.99758e-09  |
| MeanKL                  | 0.008197835   |
| MeanKLBefore            | 4.4691855e-09 |
| Step_0-AverageDiscou... | 192           |
| Step_0-AveragePolicyStd | 0.40443334    |
| Step_0-AverageReturn    | 490           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 858           |
| Step_0-MinReturn        | 22.1          |
| Step_0-NumTrajs         | 984           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 184           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.40446436    |
| Step_1-AverageReturn    | 480           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 879           |
| Step_1-MinReturn        | 17.7          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 192           |
| Time                    | 5.61e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.816         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.007560507   |
| n_timesteps             | 258880000     |
-------------------------------------------

 ---------------- Iteration 809 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 809           |
| ItrTime                 | 71.2          |
| LossAfter               | -0.007766108  |
| LossBefore              | -4.377406e-09 |
| MeanKL                  | 0.0079796575  |
| MeanKLBefore            | 1.787835e-08  |
| Step_0-AverageDiscou... | 192           |
| Step_0-AveragePolicyStd | 0.4041104     |
| Step_0-AverageReturn    | 494           |
| Step_0-EnvExecTime      | 24            |
| Step_0-MaxReturn        | 865           |
| Step_0-MinReturn        | 21.4          |
| Step_0-NumTrajs         | 956           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 171           |
| Step_1-AverageDiscou... | 187           |
| Step_1-AveragePolicyStd | 0.40406784    |
| Step_1-AverageReturn    | 470           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 803           |
| Step_1-MinReturn        | 25.5          |
| Step_1-NumTrajs         | 1011          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 187           |
| Time                    | 5.62e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.783         |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.007766104   |
| n_timesteps             | 259200000     |
-------------------------------------------

 ---------------- Iteration 810 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 810           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.006898514  |
| LossBefore              | 7.0453736e-09 |
| MeanKL                  | 0.0071681663  |
| MeanKLBefore            | 6.686207e-13  |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.40456992    |
| Step_0-AverageReturn    | 524           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 927           |
| Step_0-MinReturn        | 23.8          |
| Step_0-NumTrajs         | 928           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 163           |
| Step_1-AverageDiscou... | 196           |
| Step_1-AveragePolicyStd | 0.4043844     |
| Step_1-AverageReturn    | 500           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 865           |
| Step_1-MinReturn        | 21.5          |
| Step_1-NumTrajs         | 975           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 169           |
| Time                    | 5.63e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.779         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.006898521   |
| n_timesteps             | 259520000     |
-------------------------------------------

 ---------------- Iteration 811 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 811           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0071064024 |
| LossBefore              | 9.158049e-09  |
| MeanKL                  | 0.0076597393  |
| MeanKLBefore            | 8.938809e-09  |
| Step_0-AverageDiscou... | 183           |
| Step_0-AveragePolicyStd | 0.40428177    |
| Step_0-AverageReturn    | 457           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 888           |
| Step_0-MinReturn        | 20.5          |
| Step_0-NumTrajs         | 994           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 179           |
| Step_1-AveragePolicyStd | 0.40417874    |
| Step_1-AverageReturn    | 444           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 904           |
| Step_1-MinReturn        | 15.9          |
| Step_1-NumTrajs         | 1031          |
| Step_1-PolicyExecTime   | 4.08          |
| Step_1-StdReturn        | 188           |
| Time                    | 5.63e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.794         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0071064117  |
| n_timesteps             | 259840000     |
-------------------------------------------

 ---------------- Iteration 812 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 812           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.0075192032 |
| LossBefore              | 1.6163554e-09 |
| MeanKL                  | 0.00795635    |
| MeanKLBefore            | 4.468762e-09  |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40402976    |
| Step_0-AverageReturn    | 499           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 822           |
| Step_0-MinReturn        | 17.9          |
| Step_0-NumTrajs         | 944           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 175           |
| Step_1-AverageDiscou... | 189           |
| Step_1-AveragePolicyStd | 0.40389162    |
| Step_1-AverageReturn    | 474           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 982           |
| Step_1-MinReturn        | 19.3          |
| Step_1-NumTrajs         | 1004          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 181           |
| Time                    | 5.64e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.796         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.7          |
| dLoss                   | 0.0075192046  |
| n_timesteps             | 260160000     |
-------------------------------------------

 ---------------- Iteration 813 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 813           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007386087  |
| LossBefore              | -5.553026e-09 |
| MeanKL                  | 0.008321355   |
| MeanKLBefore            | 4.470033e-09  |
| Step_0-AverageDiscou... | 189           |
| Step_0-AveragePolicyStd | 0.4046359     |
| Step_0-AverageReturn    | 482           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 840           |
| Step_0-MinReturn        | 29.7          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 180           |
| Step_1-AverageDiscou... | 187           |
| Step_1-AveragePolicyStd | 0.40465686    |
| Step_1-AverageReturn    | 479           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 898           |
| Step_1-MinReturn        | 16.9          |
| Step_1-NumTrajs         | 981           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 183           |
| Time                    | 5.65e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.8           |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0073860814  |
| n_timesteps             | 260480000     |
-------------------------------------------

 ---------------- Iteration 814 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 814          |
| ItrTime                 | 69.7         |
| LossAfter               | -0.007203703 |
| LossBefore              | 2.495654e-09 |
| MeanKL                  | 0.007780613  |
| MeanKLBefore            | 4.471205e-09 |
| Step_0-AverageDiscou... | 187          |
| Step_0-AveragePolicyStd | 0.4045786    |
| Step_0-AverageReturn    | 475          |
| Step_0-EnvExecTime      | 23.1         |
| Step_0-MaxReturn        | 948          |
| Step_0-MinReturn        | 7.43         |
| Step_0-NumTrajs         | 993          |
| Step_0-PolicyExecTime   | 1.32         |
| Step_0-StdReturn        | 187          |
| Step_1-AverageDiscou... | 188          |
| Step_1-AveragePolicyStd | 0.4045332    |
| Step_1-AverageReturn    | 472          |
| Step_1-EnvExecTime      | 23.3         |
| Step_1-MaxReturn        | 884          |
| Step_1-MinReturn        | 19           |
| Step_1-NumTrajs         | 1028         |
| Step_1-PolicyExecTime   | 4.11         |
| Step_1-StdReturn        | 189          |
| Time                    | 5.65e+04     |
| Time-InnerStep          | 0.164        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.8          |
| Time-Sampling           | 55.1         |
| Time-TotalInner         | 56.1         |
| dLoss                   | 0.007203705  |
| n_timesteps             | 260800000    |
------------------------------------------

 ---------------- Iteration 815 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 815            |
| ItrTime                 | 70.8           |
| LossAfter               | -0.007253024   |
| LossBefore              | -1.4268082e-08 |
| MeanKL                  | 0.007922789    |
| MeanKLBefore            | 7.449637e-09   |
| Step_0-AverageDiscou... | 191            |
| Step_0-AveragePolicyStd | 0.40440196     |
| Step_0-AverageReturn    | 492            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 858            |
| Step_0-MinReturn        | 25.5           |
| Step_0-NumTrajs         | 955            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 164            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.4042775      |
| Step_1-AverageReturn    | 476            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 829            |
| Step_1-MinReturn        | 12             |
| Step_1-NumTrajs         | 982            |
| Step_1-PolicyExecTime   | 4.63           |
| Step_1-StdReturn        | 178            |
| Time                    | 5.66e+04       |
| Time-InnerStep          | 0.167          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.805          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0072530094   |
| n_timesteps             | 261120000      |
--------------------------------------------

 ---------------- Iteration 816 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 816           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.0071664415 |
| LossBefore              | -5.661004e-09 |
| MeanKL                  | 0.008336122   |
| MeanKLBefore            | 7.4485187e-09 |
| Step_0-AverageDiscou... | 184           |
| Step_0-AveragePolicyStd | 0.40445653    |
| Step_0-AverageReturn    | 468           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 907           |
| Step_0-MinReturn        | 9.55          |
| Step_0-NumTrajs         | 974           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 182           |
| Step_1-AverageDiscou... | 183           |
| Step_1-AveragePolicyStd | 0.40438017    |
| Step_1-AverageReturn    | 464           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 873           |
| Step_1-MinReturn        | 8.58          |
| Step_1-NumTrajs         | 1000          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 190           |
| Time                    | 5.67e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.793         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.007166436   |
| n_timesteps             | 261440000     |
-------------------------------------------

 ---------------- Iteration 817 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 817            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.007191679   |
| LossBefore              | -1.1143159e-08 |
| MeanKL                  | 0.008425482    |
| MeanKLBefore            | 1.1920579e-08  |
| Step_0-AverageDiscou... | 190            |
| Step_0-AveragePolicyStd | 0.403904       |
| Step_0-AverageReturn    | 492            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 913            |
| Step_0-MinReturn        | 10.1           |
| Step_0-NumTrajs         | 970            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 193            |
| Step_1-AveragePolicyStd | 0.4039032      |
| Step_1-AverageReturn    | 501            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 901            |
| Step_1-MinReturn        | 17.5           |
| Step_1-NumTrajs         | 967            |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 179            |
| Time                    | 5.67e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.81           |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.007191668    |
| n_timesteps             | 261760000      |
--------------------------------------------

 ---------------- Iteration 818 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 818           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.007907221  |
| LossBefore              | -3.827985e-09 |
| MeanKL                  | 0.009207055   |
| MeanKLBefore            | 1.042775e-08  |
| Step_0-AverageDiscou... | 182           |
| Step_0-AveragePolicyStd | 0.40340292    |
| Step_0-AverageReturn    | 466           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 811           |
| Step_0-MinReturn        | 12.5          |
| Step_0-NumTrajs         | 978           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 186           |
| Step_1-AverageDiscou... | 181           |
| Step_1-AveragePolicyStd | 0.40332744    |
| Step_1-AverageReturn    | 451           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 884           |
| Step_1-MinReturn        | 23.1          |
| Step_1-NumTrajs         | 1033          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 187           |
| Time                    | 5.68e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.797         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.7          |
| dLoss                   | 0.007907217   |
| n_timesteps             | 262080000     |
-------------------------------------------

 ---------------- Iteration 819 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 819           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.006585105  |
| LossBefore              | -6.558781e-09 |
| MeanKL                  | 0.0077251596  |
| MeanKLBefore            | -2.98024e-09  |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.40413243    |
| Step_0-AverageReturn    | 486           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 853           |
| Step_0-MinReturn        | 14.3          |
| Step_0-NumTrajs         | 942           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 179           |
| Step_1-AverageDiscou... | 188           |
| Step_1-AveragePolicyStd | 0.40401602    |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 889           |
| Step_1-MinReturn        | 14            |
| Step_1-NumTrajs         | 968           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 185           |
| Time                    | 5.69e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.788         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0065850983  |
| n_timesteps             | 262400000     |
-------------------------------------------

 ---------------- Iteration 820 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 820           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.007672052  |
| LossBefore              | 3.461664e-09  |
| MeanKL                  | 0.008507622   |
| MeanKLBefore            | 7.4505895e-09 |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.40380353    |
| Step_0-AverageReturn    | 492           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 806           |
| Step_0-MinReturn        | 20.5          |
| Step_0-NumTrajs         | 913           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 187           |
| Step_1-AverageDiscou... | 191           |
| Step_1-AveragePolicyStd | 0.40356573    |
| Step_1-AverageReturn    | 504           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 826           |
| Step_1-MinReturn        | 14.7          |
| Step_1-NumTrajs         | 934           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 180           |
| Time                    | 5.7e+04       |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.768         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007672055   |
| n_timesteps             | 262720000     |
-------------------------------------------

 ---------------- Iteration 821 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 821            |
| ItrTime                 | 70.9           |
| LossAfter               | -0.0074996264  |
| LossBefore              | -6.515656e-10  |
| MeanKL                  | 0.008152011    |
| MeanKLBefore            | -1.4887918e-09 |
| Step_0-AverageDiscou... | 199            |
| Step_0-AveragePolicyStd | 0.403796       |
| Step_0-AverageReturn    | 518            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 875            |
| Step_0-MinReturn        | 31.3           |
| Step_0-NumTrajs         | 934            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 159            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.40376884     |
| Step_1-AverageReturn    | 477            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 876            |
| Step_1-MinReturn        | 44.9           |
| Step_1-NumTrajs         | 1008           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 180            |
| Time                    | 5.7e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.795          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.007499626    |
| n_timesteps             | 263040000      |
--------------------------------------------

 ---------------- Iteration 822 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 822            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007170024   |
| LossBefore              | -7.7379685e-09 |
| MeanKL                  | 0.00807124     |
| MeanKLBefore            | 5.9593717e-09  |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.40430427     |
| Step_0-AverageReturn    | 491            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 860            |
| Step_0-MinReturn        | 23.5           |
| Step_0-NumTrajs         | 973            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 178            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.40430883     |
| Step_1-AverageReturn    | 475            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 896            |
| Step_1-MinReturn        | 22.8           |
| Step_1-NumTrajs         | 1008           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 174            |
| Time                    | 5.71e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.789          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007170016    |
| n_timesteps             | 263360000      |
--------------------------------------------

 ---------------- Iteration 823 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 823           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.007200965  |
| LossBefore              | -3.931134e-09 |
| MeanKL                  | 0.007314513   |
| MeanKLBefore            | 8.938778e-09  |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.40436456    |
| Step_0-AverageReturn    | 518           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 846           |
| Step_0-MinReturn        | 13.8          |
| Step_0-NumTrajs         | 941           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 184           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.4043612     |
| Step_1-AverageReturn    | 496           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 834           |
| Step_1-MinReturn        | 6.26          |
| Step_1-NumTrajs         | 971           |
| Step_1-PolicyExecTime   | 4.24          |
| Step_1-StdReturn        | 182           |
| Time                    | 5.72e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.813         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.0072009615  |
| n_timesteps             | 263680000     |
-------------------------------------------

 ---------------- Iteration 824 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 824           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.0075252377 |
| LossBefore              | -9.639931e-10 |
| MeanKL                  | 0.007777019   |
| MeanKLBefore            | -5.959722e-09 |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.40437683    |
| Step_0-AverageReturn    | 525           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 829           |
| Step_0-MinReturn        | -5.29         |
| Step_0-NumTrajs         | 895           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 154           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.4042609     |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 850           |
| Step_1-MinReturn        | 16.7          |
| Step_1-NumTrajs         | 966           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 176           |
| Time                    | 5.72e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.793         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.007525237   |
| n_timesteps             | 264000000     |
-------------------------------------------

 ---------------- Iteration 825 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 825           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007201053  |
| LossBefore              | 2.1042241e-09 |
| MeanKL                  | 0.008174668   |
| MeanKLBefore            | 7.447139e-09  |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40458485    |
| Step_0-AverageReturn    | 498           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 866           |
| Step_0-MinReturn        | 18.9          |
| Step_0-NumTrajs         | 950           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 164           |
| Step_1-AverageDiscou... | 189           |
| Step_1-AveragePolicyStd | 0.40448546    |
| Step_1-AverageReturn    | 482           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 878           |
| Step_1-MinReturn        | 26.8          |
| Step_1-NumTrajs         | 987           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 183           |
| Time                    | 5.73e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.77          |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0072010555  |
| n_timesteps             | 264320000     |
-------------------------------------------

 ---------------- Iteration 826 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 826            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.007728056   |
| LossBefore              | -6.945052e-09  |
| MeanKL                  | 0.00829417     |
| MeanKLBefore            | -1.4896956e-09 |
| Step_0-AverageDiscou... | 195            |
| Step_0-AveragePolicyStd | 0.40482995     |
| Step_0-AverageReturn    | 512            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 883            |
| Step_0-MinReturn        | 16.3           |
| Step_0-NumTrajs         | 941            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.40478167     |
| Step_1-AverageReturn    | 488            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 894            |
| Step_1-MinReturn        | 21             |
| Step_1-NumTrajs         | 1000           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 188            |
| Time                    | 5.74e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007728049    |
| n_timesteps             | 264640000      |
--------------------------------------------

 ---------------- Iteration 827 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 827          |
| ItrTime                 | 70.3         |
| LossAfter               | -0.007324678 |
| LossBefore              | 9.105034e-09 |
| MeanKL                  | 0.0076077506 |
| MeanKLBefore            | 4.46921e-09  |
| Step_0-AverageDiscou... | 188          |
| Step_0-AveragePolicyStd | 0.4044874    |
| Step_0-AverageReturn    | 488          |
| Step_0-EnvExecTime      | 23.3         |
| Step_0-MaxReturn        | 831          |
| Step_0-MinReturn        | 11.7         |
| Step_0-NumTrajs         | 963          |
| Step_0-PolicyExecTime   | 1.33         |
| Step_0-StdReturn        | 179          |
| Step_1-AverageDiscou... | 183          |
| Step_1-AveragePolicyStd | 0.40447974   |
| Step_1-AverageReturn    | 461          |
| Step_1-EnvExecTime      | 23.6         |
| Step_1-MaxReturn        | 880          |
| Step_1-MinReturn        | 10.8         |
| Step_1-NumTrajs         | 1030         |
| Step_1-PolicyExecTime   | 4.16         |
| Step_1-StdReturn        | 196          |
| Time                    | 5.74e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.6         |
| Time-OuterStep          | 13.6         |
| Time-SampleProc         | 0.797        |
| Time-Sampling           | 55.7         |
| Time-TotalInner         | 56.7         |
| dLoss                   | 0.007324687  |
| n_timesteps             | 264960000    |
------------------------------------------

 ---------------- Iteration 828 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 828           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0069725    |
| LossBefore              | -8.305259e-09 |
| MeanKL                  | 0.006851417   |
| MeanKLBefore            | 1.4899498e-09 |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.4049441     |
| Step_0-AverageReturn    | 517           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 856           |
| Step_0-MinReturn        | 7.41          |
| Step_0-NumTrajs         | 920           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 173           |
| Step_1-AverageDiscou... | 193           |
| Step_1-AveragePolicyStd | 0.40489653    |
| Step_1-AverageReturn    | 496           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 909           |
| Step_1-MinReturn        | 11            |
| Step_1-NumTrajs         | 968           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 186           |
| Time                    | 5.75e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.793         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0069724917  |
| n_timesteps             | 265280000     |
-------------------------------------------

 ---------------- Iteration 829 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 829            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.00697646    |
| LossBefore              | -1.0661408e-08 |
| MeanKL                  | 0.0071342513   |
| MeanKLBefore            | -4.4718567e-09 |
| Step_0-AverageDiscou... | 194            |
| Step_0-AveragePolicyStd | 0.40532175     |
| Step_0-AverageReturn    | 503            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 825            |
| Step_0-MinReturn        | 21.5           |
| Step_0-NumTrajs         | 948            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 169            |
| Step_1-AverageDiscou... | 194            |
| Step_1-AveragePolicyStd | 0.40520832     |
| Step_1-AverageReturn    | 497            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 897            |
| Step_1-MinReturn        | 18.6           |
| Step_1-NumTrajs         | 976            |
| Step_1-PolicyExecTime   | 4.48           |
| Step_1-StdReturn        | 177            |
| Time                    | 5.76e+04       |
| Time-InnerStep          | 0.15           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.0069764494   |
| n_timesteps             | 265600000      |
--------------------------------------------

 ---------------- Iteration 830 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 830           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0071733794 |
| LossBefore              | 7.781734e-09  |
| MeanKL                  | 0.008514188   |
| MeanKLBefore            | 7.5193183e-13 |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.40485057    |
| Step_0-AverageReturn    | 518           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 794           |
| Step_0-MinReturn        | 35.5          |
| Step_0-NumTrajs         | 924           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 155           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.40476406    |
| Step_1-AverageReturn    | 498           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 864           |
| Step_1-MinReturn        | 19.1          |
| Step_1-NumTrajs         | 962           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 173           |
| Time                    | 5.77e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.782         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0071733873  |
| n_timesteps             | 265920000     |
-------------------------------------------

 ---------------- Iteration 831 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 831            |
| ItrTime                 | 70             |
| LossAfter               | -0.0071361274  |
| LossBefore              | 4.4537884e-10  |
| MeanKL                  | 0.008742938    |
| MeanKLBefore            | -2.9798766e-09 |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40428498     |
| Step_0-AverageReturn    | 518            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 877            |
| Step_0-MinReturn        | 32.8           |
| Step_0-NumTrajs         | 928            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 169            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.4042614      |
| Step_1-AverageReturn    | 493            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 848            |
| Step_1-MinReturn        | 16.1           |
| Step_1-NumTrajs         | 975            |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 190            |
| Time                    | 5.77e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.775          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007136128    |
| n_timesteps             | 266240000      |
--------------------------------------------

 ---------------- Iteration 832 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 832           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0074238284 |
| LossBefore              | 8.441802e-09  |
| MeanKL                  | 0.007687512   |
| MeanKLBefore            | -2.97991e-09  |
| Step_0-AverageDiscou... | 186           |
| Step_0-AveragePolicyStd | 0.4044443     |
| Step_0-AverageReturn    | 467           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 846           |
| Step_0-MinReturn        | 20.4          |
| Step_0-NumTrajs         | 1007          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 189           |
| Step_1-AverageDiscou... | 180           |
| Step_1-AveragePolicyStd | 0.40442404    |
| Step_1-AverageReturn    | 441           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 859           |
| Step_1-MinReturn        | 17.1          |
| Step_1-NumTrajs         | 1065          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 194           |
| Time                    | 5.78e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.816         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0074238367  |
| n_timesteps             | 266560000     |
-------------------------------------------

 ---------------- Iteration 833 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 833            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.007473967   |
| LossBefore              | -5.109867e-09  |
| MeanKL                  | 0.008122596    |
| MeanKLBefore            | 1.04300035e-08 |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.40462527     |
| Step_0-AverageReturn    | 471            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 874            |
| Step_0-MinReturn        | 14.4           |
| Step_0-NumTrajs         | 995            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 194            |
| Step_1-AverageDiscou... | 186            |
| Step_1-AveragePolicyStd | 0.40456268     |
| Step_1-AverageReturn    | 475            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 843            |
| Step_1-MinReturn        | 10.5           |
| Step_1-NumTrajs         | 1000           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 189            |
| Time                    | 5.79e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.81           |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.007473962    |
| n_timesteps             | 266880000      |
--------------------------------------------

 ---------------- Iteration 834 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 834           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007029674  |
| LossBefore              | 5.8934746e-09 |
| MeanKL                  | 0.007330096   |
| MeanKLBefore            | 5.9594223e-09 |
| Step_0-AverageDiscou... | 202           |
| Step_0-AveragePolicyStd | 0.40460238    |
| Step_0-AverageReturn    | 539           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 864           |
| Step_0-MinReturn        | 19.5          |
| Step_0-NumTrajs         | 898           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 156           |
| Step_1-AverageDiscou... | 191           |
| Step_1-AveragePolicyStd | 0.4045532     |
| Step_1-AverageReturn    | 488           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 882           |
| Step_1-MinReturn        | 21.3          |
| Step_1-NumTrajs         | 985           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 187           |
| Time                    | 5.79e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.795         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.00702968    |
| n_timesteps             | 267200000     |
-------------------------------------------

 ---------------- Iteration 835 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 835            |
| ItrTime                 | 68.5           |
| LossAfter               | -0.007190296   |
| LossBefore              | 3.8848924e-09  |
| MeanKL                  | 0.007538493    |
| MeanKLBefore            | -1.0430043e-08 |
| Step_0-AverageDiscou... | 186            |
| Step_0-AveragePolicyStd | 0.40451887     |
| Step_0-AverageReturn    | 470            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 838            |
| Step_0-MinReturn        | 34.5           |
| Step_0-NumTrajs         | 1002           |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 187            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.40450695     |
| Step_1-AverageReturn    | 471            |
| Step_1-EnvExecTime      | 22.5           |
| Step_1-MaxReturn        | 847            |
| Step_1-MinReturn        | 6.79           |
| Step_1-NumTrajs         | 1017           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 190            |
| Time                    | 5.8e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55             |
| dLoss                   | 0.0071902997   |
| n_timesteps             | 267520000      |
--------------------------------------------

 ---------------- Iteration 836 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 836            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.00696112    |
| LossBefore              | -2.8389526e-09 |
| MeanKL                  | 0.0076382034   |
| MeanKLBefore            | 1.6389116e-08  |
| Step_0-AverageDiscou... | 188            |
| Step_0-AveragePolicyStd | 0.40442723     |
| Step_0-AverageReturn    | 475            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 835            |
| Step_0-MinReturn        | 14.2           |
| Step_0-NumTrajs         | 993            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 187            |
| Step_1-AveragePolicyStd | 0.4044111      |
| Step_1-AverageReturn    | 468            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 852            |
| Step_1-MinReturn        | 15             |
| Step_1-NumTrajs         | 1009           |
| Step_1-PolicyExecTime   | 4.04           |
| Step_1-StdReturn        | 178            |
| Time                    | 5.81e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.006961117    |
| n_timesteps             | 267840000      |
--------------------------------------------

 ---------------- Iteration 837 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 837            |
| ItrTime                 | 67.9           |
| LossAfter               | -0.007954801   |
| LossBefore              | -1.0099937e-08 |
| MeanKL                  | 0.009245317    |
| MeanKLBefore            | 1.4896809e-09  |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.4046818      |
| Step_0-AverageReturn    | 491            |
| Step_0-EnvExecTime      | 22.4           |
| Step_0-MaxReturn        | 828            |
| Step_0-MinReturn        | 20.1           |
| Step_0-NumTrajs         | 982            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 187            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.40459046     |
| Step_1-AverageReturn    | 485            |
| Step_1-EnvExecTime      | 22.4           |
| Step_1-MaxReturn        | 865            |
| Step_1-MinReturn        | 24.4           |
| Step_1-NumTrajs         | 1005           |
| Step_1-PolicyExecTime   | 4              |
| Step_1-StdReturn        | 194            |
| Time                    | 5.81e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.781          |
| Time-Sampling           | 53.3           |
| Time-TotalInner         | 54.3           |
| dLoss                   | 0.007954791    |
| n_timesteps             | 268160000      |
--------------------------------------------

 ---------------- Iteration 838 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 838           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.007324102  |
| LossBefore              | -4.256318e-09 |
| MeanKL                  | 0.0075324057  |
| MeanKLBefore            | 5.9607004e-09 |
| Step_0-AverageDiscou... | 199           |
| Step_0-AveragePolicyStd | 0.4052649     |
| Step_0-AverageReturn    | 516           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 845           |
| Step_0-MinReturn        | 3.36          |
| Step_0-NumTrajs         | 936           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 198           |
| Step_1-AveragePolicyStd | 0.40522724    |
| Step_1-AverageReturn    | 510           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 892           |
| Step_1-MinReturn        | 7.67          |
| Step_1-NumTrajs         | 968           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 174           |
| Time                    | 5.82e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.789         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0073240977  |
| n_timesteps             | 268480000     |
-------------------------------------------

 ---------------- Iteration 839 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 839           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.0070838616 |
| LossBefore              | 5.233265e-09  |
| MeanKL                  | 0.007942012   |
| MeanKLBefore            | 4.470403e-09  |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40537643    |
| Step_0-AverageReturn    | 536           |
| Step_0-EnvExecTime      | 24.3          |
| Step_0-MaxReturn        | 840           |
| Step_0-MinReturn        | 1.37          |
| Step_0-NumTrajs         | 911           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 160           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.4053283     |
| Step_1-AverageReturn    | 524           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 883           |
| Step_1-MinReturn        | 10.8          |
| Step_1-NumTrajs         | 948           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 167           |
| Time                    | 5.83e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.859         |
| Time-Sampling           | 57.2          |
| Time-TotalInner         | 58.2          |
| dLoss                   | 0.0070838667  |
| n_timesteps             | 268800000     |
-------------------------------------------

 ---------------- Iteration 840 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 840           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0072608567 |
| LossBefore              | 9.271636e-09  |
| MeanKL                  | 0.008025905   |
| MeanKLBefore            | 8.93991e-09   |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.40506       |
| Step_0-AverageReturn    | 505           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 885           |
| Step_0-MinReturn        | 23            |
| Step_0-NumTrajs         | 963           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 180           |
| Step_1-AverageDiscou... | 188           |
| Step_1-AveragePolicyStd | 0.4049868     |
| Step_1-AverageReturn    | 471           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 824           |
| Step_1-MinReturn        | 30.5          |
| Step_1-NumTrajs         | 1020          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 185           |
| Time                    | 5.84e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.836         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007260866   |
| n_timesteps             | 269120000     |
-------------------------------------------

 ---------------- Iteration 841 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 841           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007186666  |
| LossBefore              | -1.035775e-08 |
| MeanKL                  | 0.008004123   |
| MeanKLBefore            | 1.4890112e-09 |
| Step_0-AverageDiscou... | 181           |
| Step_0-AveragePolicyStd | 0.40475333    |
| Step_0-AverageReturn    | 462           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 831           |
| Step_0-MinReturn        | 17.1          |
| Step_0-NumTrajs         | 1002          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 203           |
| Step_1-AverageDiscou... | 187           |
| Step_1-AveragePolicyStd | 0.40453383    |
| Step_1-AverageReturn    | 482           |
| Step_1-EnvExecTime      | 24            |
| Step_1-MaxReturn        | 905           |
| Step_1-MinReturn        | 20.3          |
| Step_1-NumTrajs         | 973           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 185           |
| Time                    | 5.84e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.794         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007186656   |
| n_timesteps             | 269440000     |
-------------------------------------------

 ---------------- Iteration 842 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 842           |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0070511205 |
| LossBefore              | 7.020683e-09  |
| MeanKL                  | 0.008114      |
| MeanKLBefore            | 5.958285e-09  |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.4044797     |
| Step_0-AverageReturn    | 514           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 848           |
| Step_0-MinReturn        | 30.9          |
| Step_0-NumTrajs         | 942           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 157           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.4044756     |
| Step_1-AverageReturn    | 527           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 949           |
| Step_1-MinReturn        | 31            |
| Step_1-NumTrajs         | 922           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 160           |
| Time                    | 5.85e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.78          |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.7          |
| dLoss                   | 0.0070511275  |
| n_timesteps             | 269760000     |
-------------------------------------------

 ---------------- Iteration 843 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 843            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0069314362  |
| LossBefore              | -1.1505622e-08 |
| MeanKL                  | 0.0078413505   |
| MeanKLBefore            | 1.259437e-13   |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40424642     |
| Step_0-AverageReturn    | 515            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 887            |
| Step_0-MinReturn        | 14.1           |
| Step_0-NumTrajs         | 930            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.40425953     |
| Step_1-AverageReturn    | 480            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 857            |
| Step_1-MinReturn        | 11.9           |
| Step_1-NumTrajs         | 994            |
| Step_1-PolicyExecTime   | 4.52           |
| Step_1-StdReturn        | 186            |
| Time                    | 5.86e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.805          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.0069314246   |
| n_timesteps             | 270080000      |
--------------------------------------------

 ---------------- Iteration 844 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 844           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.007600627  |
| LossBefore              | -7.915823e-09 |
| MeanKL                  | 0.007727      |
| MeanKLBefore            | -2.978744e-09 |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.4037722     |
| Step_0-AverageReturn    | 472           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 916           |
| Step_0-MinReturn        | 11.4          |
| Step_0-NumTrajs         | 989           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 192           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.4036606     |
| Step_1-AverageReturn    | 487           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 928           |
| Step_1-MinReturn        | 15.9          |
| Step_1-NumTrajs         | 987           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 186           |
| Time                    | 5.86e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.809         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007600619   |
| n_timesteps             | 270400000     |
-------------------------------------------

 ---------------- Iteration 845 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 845           |
| ItrTime                 | 70.8          |
| LossAfter               | -0.0075174123 |
| LossBefore              | 4.885483e-09  |
| MeanKL                  | 0.009168078   |
| MeanKLBefore            | 2.9802005e-09 |
| Step_0-AverageDiscou... | 189           |
| Step_0-AveragePolicyStd | 0.4035492     |
| Step_0-AverageReturn    | 487           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 847           |
| Step_0-MinReturn        | 14.8          |
| Step_0-NumTrajs         | 995           |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 199           |
| Step_1-AverageDiscou... | 189           |
| Step_1-AveragePolicyStd | 0.40344837    |
| Step_1-AverageReturn    | 488           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 929           |
| Step_1-MinReturn        | 17.3          |
| Step_1-NumTrajs         | 991           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 194           |
| Time                    | 5.87e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.881         |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.007517417   |
| n_timesteps             | 270720000     |
-------------------------------------------

 ---------------- Iteration 846 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 846            |
| ItrTime                 | 70             |
| LossAfter               | -0.007279046   |
| LossBefore              | -5.5271503e-09 |
| MeanKL                  | 0.00814567     |
| MeanKLBefore            | -4.468147e-09  |
| Step_0-AverageDiscou... | 188            |
| Step_0-AveragePolicyStd | 0.4033879      |
| Step_0-AverageReturn    | 481            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 951            |
| Step_0-MinReturn        | 12.7           |
| Step_0-NumTrajs         | 991            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 195            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.4033829      |
| Step_1-AverageReturn    | 488            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 889            |
| Step_1-MinReturn        | 13.2           |
| Step_1-NumTrajs         | 988            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 187            |
| Time                    | 5.88e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.8            |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.0072790403   |
| n_timesteps             | 271040000      |
--------------------------------------------

 ---------------- Iteration 847 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 847            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007624286   |
| LossBefore              | -2.6787539e-09 |
| MeanKL                  | 0.008944546    |
| MeanKLBefore            | -8.940633e-09  |
| Step_0-AverageDiscou... | 184            |
| Step_0-AveragePolicyStd | 0.40294373     |
| Step_0-AverageReturn    | 468            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 876            |
| Step_0-MinReturn        | 18.4           |
| Step_0-NumTrajs         | 998            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 188            |
| Step_1-AverageDiscou... | 184            |
| Step_1-AveragePolicyStd | 0.40287983     |
| Step_1-AverageReturn    | 466            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 858            |
| Step_1-MinReturn        | 16.9           |
| Step_1-NumTrajs         | 1015           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 195            |
| Time                    | 5.88e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.796          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0076242834   |
| n_timesteps             | 271360000      |
--------------------------------------------

 ---------------- Iteration 848 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 848            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.006979022   |
| LossBefore              | -2.8378797e-09 |
| MeanKL                  | 0.006991057    |
| MeanKLBefore            | 4.469185e-09   |
| Step_0-AverageDiscou... | 199            |
| Step_0-AveragePolicyStd | 0.403183       |
| Step_0-AverageReturn    | 518            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 886            |
| Step_0-MinReturn        | 30.1           |
| Step_0-NumTrajs         | 955            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 171            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40320688     |
| Step_1-AverageReturn    | 511            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 855            |
| Step_1-MinReturn        | 22.2           |
| Step_1-NumTrajs         | 960            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 185            |
| Time                    | 5.89e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.91           |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.0069790194   |
| n_timesteps             | 271680000      |
--------------------------------------------

 ---------------- Iteration 849 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 849            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.006854888   |
| LossBefore              | -6.9793784e-09 |
| MeanKL                  | 0.0075474666   |
| MeanKLBefore            | 7.4497954e-09  |
| Step_0-AverageDiscou... | 198            |
| Step_0-AveragePolicyStd | 0.40305206     |
| Step_0-AverageReturn    | 512            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 837            |
| Step_0-MinReturn        | 11.5           |
| Step_0-NumTrajs         | 960            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 173            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40306258     |
| Step_1-AverageReturn    | 505            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 913            |
| Step_1-MinReturn        | 15.6           |
| Step_1-NumTrajs         | 984            |
| Step_1-PolicyExecTime   | 4.18           |
| Step_1-StdReturn        | 183            |
| Time                    | 5.9e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.795          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.006854881    |
| n_timesteps             | 272000000      |
--------------------------------------------

 ---------------- Iteration 850 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 850           |
| ItrTime                 | 70.7          |
| LossAfter               | -0.007538469  |
| LossBefore              | 1.755331e-09  |
| MeanKL                  | 0.007972212   |
| MeanKLBefore            | 1.4893441e-09 |
| Step_0-AverageDiscou... | 199           |
| Step_0-AveragePolicyStd | 0.40306005    |
| Step_0-AverageReturn    | 534           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 819           |
| Step_0-MinReturn        | 25            |
| Step_0-NumTrajs         | 908           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.40294433    |
| Step_1-AverageReturn    | 513           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 844           |
| Step_1-MinReturn        | 14.6          |
| Step_1-NumTrajs         | 949           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 179           |
| Time                    | 5.91e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.776         |
| Time-Sampling           | 56.2          |
| Time-TotalInner         | 57.2          |
| dLoss                   | 0.007538471   |
| n_timesteps             | 272320000     |
-------------------------------------------

 ---------------- Iteration 851 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 851           |
| ItrTime                 | 68.8          |
| LossAfter               | -0.0068800547 |
| LossBefore              | -4.125269e-11 |
| MeanKL                  | 0.0071230843  |
| MeanKLBefore            | 1.1918674e-08 |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.403361      |
| Step_0-AverageReturn    | 499           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 875           |
| Step_0-MinReturn        | 22.2          |
| Step_0-NumTrajs         | 976           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 173           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40326262    |
| Step_1-AverageReturn    | 487           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 927           |
| Step_1-MinReturn        | 35.4          |
| Step_1-NumTrajs         | 999           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 179           |
| Time                    | 5.91e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.843         |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.2          |
| dLoss                   | 0.0068800547  |
| n_timesteps             | 272640000     |
-------------------------------------------

 ---------------- Iteration 852 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 852            |
| ItrTime                 | 71.3           |
| LossAfter               | -0.0071272245  |
| LossBefore              | -5.759912e-09  |
| MeanKL                  | 0.007896663    |
| MeanKLBefore            | -4.4695434e-09 |
| Step_0-AverageDiscou... | 194            |
| Step_0-AveragePolicyStd | 0.40307775     |
| Step_0-AverageReturn    | 508            |
| Step_0-EnvExecTime      | 23.8           |
| Step_0-MaxReturn        | 909            |
| Step_0-MinReturn        | 23.7           |
| Step_0-NumTrajs         | 932            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.4029967      |
| Step_1-AverageReturn    | 487            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 878            |
| Step_1-MinReturn        | 32.4           |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 184            |
| Time                    | 5.92e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.787          |
| Time-Sampling           | 56.7           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.007127219    |
| n_timesteps             | 272960000      |
--------------------------------------------

 ---------------- Iteration 853 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 853           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.007453236  |
| LossBefore              | 1.3380197e-09 |
| MeanKL                  | 0.0083118845  |
| MeanKLBefore            | 1.4893656e-09 |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40293282    |
| Step_0-AverageReturn    | 492           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 841           |
| Step_0-MinReturn        | 13.2          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 181           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40287456    |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 916           |
| Step_1-MinReturn        | 19.1          |
| Step_1-NumTrajs         | 1000          |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 197           |
| Time                    | 5.93e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.809         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.007453237   |
| n_timesteps             | 273280000     |
-------------------------------------------

 ---------------- Iteration 854 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 854           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007234753  |
| LossBefore              | 2.6858609e-09 |
| MeanKL                  | 0.008110861   |
| MeanKLBefore            | 1.4885417e-09 |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.4034082     |
| Step_0-AverageReturn    | 509           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 904           |
| Step_0-MinReturn        | 20.2          |
| Step_0-NumTrajs         | 969           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 198           |
| Step_1-AveragePolicyStd | 0.40328562    |
| Step_1-AverageReturn    | 516           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 854           |
| Step_1-MinReturn        | 25            |
| Step_1-NumTrajs         | 957           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 182           |
| Time                    | 5.93e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.791         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007234756   |
| n_timesteps             | 273600000     |
-------------------------------------------

 ---------------- Iteration 855 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 855           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.007216866  |
| LossBefore              | -4.801826e-09 |
| MeanKL                  | 0.009129951   |
| MeanKLBefore            | 8.939397e-09  |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.40381518    |
| Step_0-AverageReturn    | 515           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 827           |
| Step_0-MinReturn        | 17.2          |
| Step_0-NumTrajs         | 951           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 164           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40369335    |
| Step_1-AverageReturn    | 505           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 897           |
| Step_1-MinReturn        | 6.7           |
| Step_1-NumTrajs         | 962           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 185           |
| Time                    | 5.94e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.782         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.0072168615  |
| n_timesteps             | 273920000     |
-------------------------------------------

 ---------------- Iteration 856 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 856           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007126632  |
| LossBefore              | -6.197343e-09 |
| MeanKL                  | 0.008197504   |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 205           |
| Step_0-AveragePolicyStd | 0.4036414     |
| Step_0-AverageReturn    | 533           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 855           |
| Step_0-MinReturn        | 26.4          |
| Step_0-NumTrajs         | 939           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 166           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.40355033    |
| Step_1-AverageReturn    | 507           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 871           |
| Step_1-MinReturn        | 17.6          |
| Step_1-NumTrajs         | 990           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 186           |
| Time                    | 5.95e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.789         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.007126626   |
| n_timesteps             | 274240000     |
-------------------------------------------

 ---------------- Iteration 857 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 857            |
| ItrTime                 | 71.6           |
| LossAfter               | -0.007085572   |
| LossBefore              | -4.153776e-09  |
| MeanKL                  | 0.008268602    |
| MeanKLBefore            | -3.6806113e-13 |
| Step_0-AverageDiscou... | 190            |
| Step_0-AveragePolicyStd | 0.40392035     |
| Step_0-AverageReturn    | 482            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 826            |
| Step_0-MinReturn        | 13.7           |
| Step_0-NumTrajs         | 1004           |
| Step_0-PolicyExecTime   | 1.71           |
| Step_0-StdReturn        | 180            |
| Step_1-AverageDiscou... | 192            |
| Step_1-AveragePolicyStd | 0.4038983      |
| Step_1-AverageReturn    | 493            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 860            |
| Step_1-MinReturn        | 21.3           |
| Step_1-NumTrajs         | 981            |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 176            |
| Time                    | 5.95e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.795          |
| Time-Sampling           | 57             |
| Time-TotalInner         | 58             |
| dLoss                   | 0.007085568    |
| n_timesteps             | 274560000      |
--------------------------------------------

 ---------------- Iteration 858 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 858            |
| ItrTime                 | 71             |
| LossAfter               | -0.007271559   |
| LossBefore              | 7.579224e-09   |
| MeanKL                  | 0.008018268    |
| MeanKLBefore            | -2.9805491e-09 |
| Step_0-AverageDiscou... | 198            |
| Step_0-AveragePolicyStd | 0.40374485     |
| Step_0-AverageReturn    | 516            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 921            |
| Step_0-MinReturn        | 7.2            |
| Step_0-NumTrajs         | 938            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 166            |
| Step_1-AverageDiscou... | 194            |
| Step_1-AveragePolicyStd | 0.4036552      |
| Step_1-AverageReturn    | 503            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 855            |
| Step_1-MinReturn        | 6.54           |
| Step_1-NumTrajs         | 973            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 185            |
| Time                    | 5.96e+04       |
| Time-InnerStep          | 0.169          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.82           |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0072715664   |
| n_timesteps             | 274880000      |
--------------------------------------------

 ---------------- Iteration 859 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 859            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.006909629   |
| LossBefore              | -9.845779e-09  |
| MeanKL                  | 0.007554963    |
| MeanKLBefore            | -2.9802323e-09 |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40398026     |
| Step_0-AverageReturn    | 508            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 873            |
| Step_0-MinReturn        | 9.78           |
| Step_0-NumTrajs         | 957            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 194            |
| Step_1-AveragePolicyStd | 0.4039378      |
| Step_1-AverageReturn    | 494            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 873            |
| Step_1-MinReturn        | 22.7           |
| Step_1-NumTrajs         | 986            |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 185            |
| Time                    | 5.97e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.829          |
| Time-Sampling           | 54.8           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.006909619    |
| n_timesteps             | 275200000      |
--------------------------------------------

 ---------------- Iteration 860 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 860            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0067089526  |
| LossBefore              | 5.284039e-09   |
| MeanKL                  | 0.0072004525   |
| MeanKLBefore            | -3.5269566e-13 |
| Step_0-AverageDiscou... | 195            |
| Step_0-AveragePolicyStd | 0.40357083     |
| Step_0-AverageReturn    | 500            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 840            |
| Step_0-MinReturn        | 25.7           |
| Step_0-NumTrajs         | 979            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 184            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.4034537      |
| Step_1-AverageReturn    | 510            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 824            |
| Step_1-MinReturn        | 13.2           |
| Step_1-NumTrajs         | 962            |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 180            |
| Time                    | 5.98e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.803          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0067089577   |
| n_timesteps             | 275520000      |
--------------------------------------------

 ---------------- Iteration 861 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 861            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.0075601414  |
| LossBefore              | -7.549978e-09  |
| MeanKL                  | 0.008936369    |
| MeanKLBefore            | -1.4897126e-09 |
| Step_0-AverageDiscou... | 194            |
| Step_0-AveragePolicyStd | 0.40298387     |
| Step_0-AverageReturn    | 501            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 851            |
| Step_0-MinReturn        | 17.4           |
| Step_0-NumTrajs         | 964            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 190            |
| Step_1-AveragePolicyStd | 0.4029407      |
| Step_1-AverageReturn    | 485            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 867            |
| Step_1-MinReturn        | 9.67           |
| Step_1-NumTrajs         | 971            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 175            |
| Time                    | 5.98e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.782          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.007560134    |
| n_timesteps             | 275840000      |
--------------------------------------------

 ---------------- Iteration 862 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 862           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0069833845 |
| LossBefore              | 2.4012219e-09 |
| MeanKL                  | 0.0080510825  |
| MeanKLBefore            | 1.4898233e-09 |
| Step_0-AverageDiscou... | 191           |
| Step_0-AveragePolicyStd | 0.40277323    |
| Step_0-AverageReturn    | 490           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 825           |
| Step_0-MinReturn        | 29.7          |
| Step_0-NumTrajs         | 990           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 196           |
| Step_1-AverageDiscou... | 188           |
| Step_1-AveragePolicyStd | 0.40270862    |
| Step_1-AverageReturn    | 471           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 894           |
| Step_1-MinReturn        | 17.9          |
| Step_1-NumTrajs         | 1028          |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 191           |
| Time                    | 5.99e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.809         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.006983387   |
| n_timesteps             | 276160000     |
-------------------------------------------

 ---------------- Iteration 863 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 863            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0071517467  |
| LossBefore              | -1.1291208e-08 |
| MeanKL                  | 0.007647668    |
| MeanKLBefore            | 1.0428965e-08  |
| Step_0-AverageDiscou... | 191            |
| Step_0-AveragePolicyStd | 0.40288618     |
| Step_0-AverageReturn    | 500            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 878            |
| Step_0-MinReturn        | 10.6           |
| Step_0-NumTrajs         | 948            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 183            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.40285844     |
| Step_1-AverageReturn    | 481            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 951            |
| Step_1-MinReturn        | 13.8           |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 194            |
| Time                    | 6e+04          |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.816          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0071517355   |
| n_timesteps             | 276480000      |
--------------------------------------------

 ---------------- Iteration 864 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 864            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.00753704    |
| LossBefore              | -6.3225016e-09 |
| MeanKL                  | 0.008094035    |
| MeanKLBefore            | 1.0429403e-08  |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.4027027      |
| Step_0-AverageReturn    | 487            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 915            |
| Step_0-MinReturn        | 16.2           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40269998     |
| Step_1-AverageReturn    | 509            |
| Step_1-EnvExecTime      | 23.9           |
| Step_1-MaxReturn        | 839            |
| Step_1-MinReturn        | 16.4           |
| Step_1-NumTrajs         | 955            |
| Step_1-PolicyExecTime   | 4.23           |
| Step_1-StdReturn        | 178            |
| Time                    | 6e+04          |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.825          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0075370334   |
| n_timesteps             | 276800000      |
--------------------------------------------

 ---------------- Iteration 865 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
---------------------------------------------
| Itr                     | 865             |
| ItrTime                 | 68.4            |
| LossAfter               | -0.007107141    |
| LossBefore              | -1.34641365e-08 |
| MeanKL                  | 0.007945573     |
| MeanKLBefore            | -1.0430493e-08  |
| Step_0-AverageDiscou... | 186             |
| Step_0-AveragePolicyStd | 0.40261045      |
| Step_0-AverageReturn    | 468             |
| Step_0-EnvExecTime      | 22.4            |
| Step_0-MaxReturn        | 846             |
| Step_0-MinReturn        | 18              |
| Step_0-NumTrajs         | 1007            |
| Step_0-PolicyExecTime   | 1.29            |
| Step_0-StdReturn        | 192             |
| Step_1-AverageDiscou... | 184             |
| Step_1-AveragePolicyStd | 0.4025606       |
| Step_1-AverageReturn    | 459             |
| Step_1-EnvExecTime      | 22.9            |
| Step_1-MaxReturn        | 835             |
| Step_1-MinReturn        | 16.5            |
| Step_1-NumTrajs         | 1022            |
| Step_1-PolicyExecTime   | 4.09            |
| Step_1-StdReturn        | 193             |
| Time                    | 6.01e+04        |
| Time-InnerStep          | 0.161           |
| Time-MAMLSteps          | 13.5            |
| Time-OuterStep          | 13.5            |
| Time-SampleProc         | 0.812           |
| Time-Sampling           | 53.9            |
| Time-TotalInner         | 54.9            |
| dLoss                   | 0.0071071275    |
| n_timesteps             | 277120000       |
---------------------------------------------

 ---------------- Iteration 866 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 866           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007163203  |
| LossBefore              | 2.9958447e-09 |
| MeanKL                  | 0.008299278   |
| MeanKLBefore            | 8.940321e-09  |
| Step_0-AverageDiscou... | 199           |
| Step_0-AveragePolicyStd | 0.4028234     |
| Step_0-AverageReturn    | 524           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 880           |
| Step_0-MinReturn        | 0.242         |
| Step_0-NumTrajs         | 925           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 172           |
| Step_1-AverageDiscou... | 191           |
| Step_1-AveragePolicyStd | 0.4028442     |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 878           |
| Step_1-MinReturn        | 10.8          |
| Step_1-NumTrajs         | 980           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 185           |
| Time                    | 6.02e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.792         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0071632056  |
| n_timesteps             | 277440000     |
-------------------------------------------

 ---------------- Iteration 867 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 867           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0076297396 |
| LossBefore              | 7.1273354e-10 |
| MeanKL                  | 0.008398337   |
| MeanKLBefore            | 7.4484405e-09 |
| Step_0-AverageDiscou... | 203           |
| Step_0-AveragePolicyStd | 0.40288383    |
| Step_0-AverageReturn    | 532           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 859           |
| Step_0-MinReturn        | 7.32          |
| Step_0-NumTrajs         | 933           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 172           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.40285036    |
| Step_1-AverageReturn    | 498           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 919           |
| Step_1-MinReturn        | 21.5          |
| Step_1-NumTrajs         | 992           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 189           |
| Time                    | 6.02e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.786         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0076297405  |
| n_timesteps             | 277760000     |
-------------------------------------------

 ---------------- Iteration 868 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 868           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007005994  |
| LossBefore              | 2.9481986e-09 |
| MeanKL                  | 0.0074765766  |
| MeanKLBefore            | 1.1918622e-08 |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.40221366    |
| Step_0-AverageReturn    | 513           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 847           |
| Step_0-MinReturn        | 13.9          |
| Step_0-NumTrajs         | 951           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 168           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40222546    |
| Step_1-AverageReturn    | 505           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 886           |
| Step_1-MinReturn        | 10.9          |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 188           |
| Time                    | 6.03e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.795         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 57            |
| dLoss                   | 0.007005997   |
| n_timesteps             | 278080000     |
-------------------------------------------

 ---------------- Iteration 869 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 869           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.007248368  |
| LossBefore              | 1.201887e-08  |
| MeanKL                  | 0.008236041   |
| MeanKLBefore            | 1.4899138e-08 |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40213394    |
| Step_0-AverageReturn    | 499           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 839           |
| Step_0-MinReturn        | 11.3          |
| Step_0-NumTrajs         | 960           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.40205264    |
| Step_1-AverageReturn    | 484           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 898           |
| Step_1-MinReturn        | 36.8          |
| Step_1-NumTrajs         | 989           |
| Step_1-PolicyExecTime   | 4.07          |
| Step_1-StdReturn        | 179           |
| Time                    | 6.04e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.7          |
| dLoss                   | 0.00724838    |
| n_timesteps             | 278400000     |
-------------------------------------------

 ---------------- Iteration 870 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 870           |
| ItrTime                 | 68.5          |
| LossAfter               | -0.007320666  |
| LossBefore              | 1.557245e-08  |
| MeanKL                  | 0.0077944985  |
| MeanKLBefore            | 2.9802323e-09 |
| Step_0-AverageDiscou... | 193           |
| Step_0-AveragePolicyStd | 0.4020048     |
| Step_0-AverageReturn    | 487           |
| Step_0-EnvExecTime      | 22.1          |
| Step_0-MaxReturn        | 818           |
| Step_0-MinReturn        | 26.2          |
| Step_0-NumTrajs         | 995           |
| Step_0-PolicyExecTime   | 1.27          |
| Step_0-StdReturn        | 182           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.40200645    |
| Step_1-AverageReturn    | 480           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 895           |
| Step_1-MinReturn        | 19.3          |
| Step_1-NumTrajs         | 998           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 188           |
| Time                    | 6.05e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 53.9          |
| Time-TotalInner         | 54.9          |
| dLoss                   | 0.0073206816  |
| n_timesteps             | 278720000     |
-------------------------------------------

 ---------------- Iteration 871 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 871           |
| ItrTime                 | 70            |
| LossAfter               | -0.00718764   |
| LossBefore              | -8.500509e-09 |
| MeanKL                  | 0.008414433   |
| MeanKLBefore            | 5.9590493e-09 |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40198854    |
| Step_0-AverageReturn    | 503           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 864           |
| Step_0-MinReturn        | 20.2          |
| Step_0-NumTrajs         | 965           |
| Step_0-PolicyExecTime   | 1.71          |
| Step_0-StdReturn        | 192           |
| Step_1-AverageDiscou... | 196           |
| Step_1-AveragePolicyStd | 0.40198755    |
| Step_1-AverageReturn    | 501           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 891           |
| Step_1-MinReturn        | 24            |
| Step_1-NumTrajs         | 988           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 185           |
| Time                    | 6.05e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.81          |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.0071876314  |
| n_timesteps             | 279040000     |
-------------------------------------------

 ---------------- Iteration 872 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 872            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.0075207995  |
| LossBefore              | -1.3515498e-08 |
| MeanKL                  | 0.009416775    |
| MeanKLBefore            | 5.9592495e-09  |
| Step_0-AverageDiscou... | 185            |
| Step_0-AveragePolicyStd | 0.4019346      |
| Step_0-AverageReturn    | 471            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 845            |
| Step_0-MinReturn        | 22.1           |
| Step_0-NumTrajs         | 1004           |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 208            |
| Step_1-AverageDiscou... | 188            |
| Step_1-AveragePolicyStd | 0.40191966     |
| Step_1-AverageReturn    | 478            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 883            |
| Step_1-MinReturn        | 19.9           |
| Step_1-NumTrajs         | 1010           |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 207            |
| Time                    | 6.06e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.847          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 56             |
| dLoss                   | 0.007520786    |
| n_timesteps             | 279360000      |
--------------------------------------------

 ---------------- Iteration 873 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 873           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.006733863  |
| LossBefore              | 2.0541868e-09 |
| MeanKL                  | 0.007748963   |
| MeanKLBefore            | 4.4692277e-09 |
| Step_0-AverageDiscou... | 207           |
| Step_0-AveragePolicyStd | 0.4017605     |
| Step_0-AverageReturn    | 558           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 843           |
| Step_0-MinReturn        | 14.3          |
| Step_0-NumTrajs         | 901           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 161           |
| Step_1-AverageDiscou... | 196           |
| Step_1-AveragePolicyStd | 0.4016937     |
| Step_1-AverageReturn    | 506           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 873           |
| Step_1-MinReturn        | 5.84          |
| Step_1-NumTrajs         | 980           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 193           |
| Time                    | 6.07e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.879         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.006733865   |
| n_timesteps             | 279680000     |
-------------------------------------------

 ---------------- Iteration 874 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 874            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.007483636   |
| LossBefore              | 3.8403534e-09  |
| MeanKL                  | 0.008022106    |
| MeanKLBefore            | -7.5015547e-13 |
| Step_0-AverageDiscou... | 193            |
| Step_0-AveragePolicyStd | 0.40188283     |
| Step_0-AverageReturn    | 499            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 878            |
| Step_0-MinReturn        | 7.75           |
| Step_0-NumTrajs         | 975            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 188            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40181974     |
| Step_1-AverageReturn    | 507            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 928            |
| Step_1-MinReturn        | 8.65           |
| Step_1-NumTrajs         | 972            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 190            |
| Time                    | 6.07e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.786          |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.0074836398   |
| n_timesteps             | 280000000      |
--------------------------------------------

 ---------------- Iteration 875 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 875            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0069517144  |
| LossBefore              | 3.008615e-09   |
| MeanKL                  | 0.0074046226   |
| MeanKLBefore            | -1.4908025e-09 |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.40220097     |
| Step_0-AverageReturn    | 554            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 862            |
| Step_0-MinReturn        | 31.5           |
| Step_0-NumTrajs         | 921            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 163            |
| Step_1-AverageDiscou... | 202            |
| Step_1-AveragePolicyStd | 0.4021627      |
| Step_1-AverageReturn    | 531            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 914            |
| Step_1-MinReturn        | 17             |
| Step_1-NumTrajs         | 943            |
| Step_1-PolicyExecTime   | 4.18           |
| Step_1-StdReturn        | 182            |
| Time                    | 6.08e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.006951717    |
| n_timesteps             | 280320000      |
--------------------------------------------

 ---------------- Iteration 876 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 876            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.007525733   |
| LossBefore              | -1.2680539e-08 |
| MeanKL                  | 0.0085314475   |
| MeanKLBefore            | 1.0429723e-08  |
| Step_0-AverageDiscou... | 193            |
| Step_0-AveragePolicyStd | 0.40230423     |
| Step_0-AverageReturn    | 495            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 886            |
| Step_0-MinReturn        | 20             |
| Step_0-NumTrajs         | 974            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 178            |
| Step_1-AverageDiscou... | 192            |
| Step_1-AveragePolicyStd | 0.40219694     |
| Step_1-AverageReturn    | 489            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 909            |
| Step_1-MinReturn        | 26.7           |
| Step_1-NumTrajs         | 990            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 182            |
| Time                    | 6.09e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.795          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0075257206   |
| n_timesteps             | 280640000      |
--------------------------------------------

 ---------------- Iteration 877 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 877           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0073961606 |
| LossBefore              | 3.2240386e-09 |
| MeanKL                  | 0.007818792   |
| MeanKLBefore            | 5.958779e-09  |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.40220594    |
| Step_0-AverageReturn    | 521           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 853           |
| Step_0-MinReturn        | 20.7          |
| Step_0-NumTrajs         | 955           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.40217063    |
| Step_1-AverageReturn    | 503           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 863           |
| Step_1-MinReturn        | 18            |
| Step_1-NumTrajs         | 978           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 184           |
| Time                    | 6.09e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007396164   |
| n_timesteps             | 280960000     |
-------------------------------------------

 ---------------- Iteration 878 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 878            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.0066379085  |
| LossBefore              | -1.1621176e-08 |
| MeanKL                  | 0.00699271     |
| MeanKLBefore            | 7.4482527e-09  |
| Step_0-AverageDiscou... | 189            |
| Step_0-AveragePolicyStd | 0.40171263     |
| Step_0-AverageReturn    | 485            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 845            |
| Step_0-MinReturn        | 5.74           |
| Step_0-NumTrajs         | 977            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 194            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.40157697     |
| Step_1-AverageReturn    | 487            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 878            |
| Step_1-MinReturn        | 23.5           |
| Step_1-NumTrajs         | 989            |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 196            |
| Time                    | 6.1e+04        |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.006637897    |
| n_timesteps             | 281280000      |
--------------------------------------------

 ---------------- Iteration 879 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 879            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0069482876  |
| LossBefore              | -1.4900248e-09 |
| MeanKL                  | 0.0081127165   |
| MeanKLBefore            | 7.7502447e-13  |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.40108746     |
| Step_0-AverageReturn    | 536            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 845            |
| Step_0-MinReturn        | 25.2           |
| Step_0-NumTrajs         | 937            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 170            |
| Step_1-AverageDiscou... | 205            |
| Step_1-AveragePolicyStd | 0.40109035     |
| Step_1-AverageReturn    | 527            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 859            |
| Step_1-MinReturn        | 30.9           |
| Step_1-NumTrajs         | 967            |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 185            |
| Time                    | 6.11e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.781          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.006948286    |
| n_timesteps             | 281600000      |
--------------------------------------------

 ---------------- Iteration 880 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 880            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.0069859014  |
| LossBefore              | 3.411491e-09   |
| MeanKL                  | 0.007865669    |
| MeanKLBefore            | -1.9349856e-12 |
| Step_0-AverageDiscou... | 193            |
| Step_0-AveragePolicyStd | 0.40057895     |
| Step_0-AverageReturn    | 497            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 813            |
| Step_0-MinReturn        | 7.78           |
| Step_0-NumTrajs         | 954            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 171            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40057692     |
| Step_1-AverageReturn    | 513            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 888            |
| Step_1-MinReturn        | 17.7           |
| Step_1-NumTrajs         | 938            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 177            |
| Time                    | 6.12e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.788          |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.0069859046   |
| n_timesteps             | 281920000      |
--------------------------------------------

 ---------------- Iteration 881 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 881           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007244771  |
| LossBefore              | -1.812625e-09 |
| MeanKL                  | 0.007624312   |
| MeanKLBefore            | 1.491997e-09  |
| Step_0-AverageDiscou... | 195           |
| Step_0-AveragePolicyStd | 0.4003321     |
| Step_0-AverageReturn    | 500           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 880           |
| Step_0-MinReturn        | 26.9          |
| Step_0-NumTrajs         | 987           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 196           |
| Step_1-AverageDiscou... | 189           |
| Step_1-AveragePolicyStd | 0.40032336    |
| Step_1-AverageReturn    | 480           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 886           |
| Step_1-MinReturn        | 24.8          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 191           |
| Time                    | 6.12e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007244769   |
| n_timesteps             | 282240000     |
-------------------------------------------

 ---------------- Iteration 882 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 882           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.0072497665 |
| LossBefore              | -3.099218e-09 |
| MeanKL                  | 0.009439987   |
| MeanKLBefore            | 8.940387e-09  |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.4005313     |
| Step_0-AverageReturn    | 494           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 924           |
| Step_0-MinReturn        | 78.6          |
| Step_0-NumTrajs         | 1007          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 181           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.40045565    |
| Step_1-AverageReturn    | 496           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 867           |
| Step_1-MinReturn        | 35.9          |
| Step_1-NumTrajs         | 991           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 167           |
| Time                    | 6.13e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.802         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.0072497632  |
| n_timesteps             | 282560000     |
-------------------------------------------

 ---------------- Iteration 883 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 883           |
| ItrTime                 | 69.6          |
| LossAfter               | -0.0074210553 |
| LossBefore              | 2.5299667e-09 |
| MeanKL                  | 0.008740248   |
| MeanKLBefore            | 1.0428593e-08 |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.40049198    |
| Step_0-AverageReturn    | 504           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 826           |
| Step_0-MinReturn        | 18.1          |
| Step_0-NumTrajs         | 988           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 200           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.40052035    |
| Step_1-AverageReturn    | 475           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 887           |
| Step_1-MinReturn        | 19.8          |
| Step_1-NumTrajs         | 1044          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.14e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.802         |
| Time-Sampling           | 55.1          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.0074210577  |
| n_timesteps             | 282880000     |
-------------------------------------------

 ---------------- Iteration 884 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 884           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.007317707  |
| LossBefore              | 4.5560764e-09 |
| MeanKL                  | 0.007705872   |
| MeanKLBefore            | 2.9801914e-09 |
| Step_0-AverageDiscou... | 206           |
| Step_0-AveragePolicyStd | 0.4004382     |
| Step_0-AverageReturn    | 535           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 878           |
| Step_0-MinReturn        | 13.3          |
| Step_0-NumTrajs         | 938           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 160           |
| Step_1-AverageDiscou... | 201           |
| Step_1-AveragePolicyStd | 0.4004143     |
| Step_1-AverageReturn    | 522           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 881           |
| Step_1-MinReturn        | 29.7          |
| Step_1-NumTrajs         | 959           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 188           |
| Time                    | 6.14e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.808         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0073177116  |
| n_timesteps             | 283200000     |
-------------------------------------------

 ---------------- Iteration 885 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 885           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.0070774383 |
| LossBefore              | -4.969887e-09 |
| MeanKL                  | 0.0080676265  |
| MeanKLBefore            | 4.4681387e-09 |
| Step_0-AverageDiscou... | 194           |
| Step_0-AveragePolicyStd | 0.40070027    |
| Step_0-AverageReturn    | 496           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 827           |
| Step_0-MinReturn        | 11.2          |
| Step_0-NumTrajs         | 978           |
| Step_0-PolicyExecTime   | 1.69          |
| Step_0-StdReturn        | 180           |
| Step_1-AverageDiscou... | 193           |
| Step_1-AveragePolicyStd | 0.4006232     |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 878           |
| Step_1-MinReturn        | 10.8          |
| Step_1-NumTrajs         | 989           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 185           |
| Time                    | 6.15e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.796         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.007077433   |
| n_timesteps             | 283520000     |
-------------------------------------------

 ---------------- Iteration 886 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 886            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.00752191    |
| LossBefore              | -1.7212255e-09 |
| MeanKL                  | 0.007990086    |
| MeanKLBefore            | 4.471377e-09   |
| Step_0-AverageDiscou... | 192            |
| Step_0-AveragePolicyStd | 0.40076616     |
| Step_0-AverageReturn    | 502            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 867            |
| Step_0-MinReturn        | 15.7           |
| Step_0-NumTrajs         | 949            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 193            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.40076146     |
| Step_1-AverageReturn    | 488            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 881            |
| Step_1-MinReturn        | 11.1           |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 194            |
| Time                    | 6.16e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.789          |
| Time-Sampling           | 54.8           |
| Time-TotalInner         | 55.8           |
| dLoss                   | 0.0075219083   |
| n_timesteps             | 283840000      |
--------------------------------------------

 ---------------- Iteration 887 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 887            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0071247844  |
| LossBefore              | -1.0480955e-08 |
| MeanKL                  | 0.008311669    |
| MeanKLBefore            | 1.4919331e-09  |
| Step_0-AverageDiscou... | 204            |
| Step_0-AveragePolicyStd | 0.40066496     |
| Step_0-AverageReturn    | 536            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 920            |
| Step_0-MinReturn        | 44.4           |
| Step_0-NumTrajs         | 921            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 168            |
| Step_1-AverageDiscou... | 197            |
| Step_1-AveragePolicyStd | 0.40061542     |
| Step_1-AverageReturn    | 511            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 906            |
| Step_1-MinReturn        | 29.6           |
| Step_1-NumTrajs         | 964            |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 195            |
| Time                    | 6.16e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.783          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.0071247737   |
| n_timesteps             | 284160000      |
--------------------------------------------

 ---------------- Iteration 888 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 888           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0069238096 |
| LossBefore              | 3.985881e-10  |
| MeanKL                  | 0.007542306   |
| MeanKLBefore            | -7.451045e-09 |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.40054154    |
| Step_0-AverageReturn    | 509           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 884           |
| Step_0-MinReturn        | 13.9          |
| Step_0-NumTrajs         | 981           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 199           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.40057534    |
| Step_1-AverageReturn    | 478           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 883           |
| Step_1-MinReturn        | 12.1          |
| Step_1-NumTrajs         | 1031          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.17e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.818         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.00692381    |
| n_timesteps             | 284480000     |
-------------------------------------------

 ---------------- Iteration 889 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 889            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0070415623  |
| LossBefore              | -6.8255708e-09 |
| MeanKL                  | 0.0072148563   |
| MeanKLBefore            | 1.4916285e-09  |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40056196     |
| Step_0-AverageReturn    | 520            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 891            |
| Step_0-MinReturn        | 19.8           |
| Step_0-NumTrajs         | 931            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 176            |
| Step_1-AverageDiscou... | 193            |
| Step_1-AveragePolicyStd | 0.40041983     |
| Step_1-AverageReturn    | 500            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 897            |
| Step_1-MinReturn        | 16.4           |
| Step_1-NumTrajs         | 981            |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 192            |
| Time                    | 6.18e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.797          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0070415554   |
| n_timesteps             | 284800000      |
--------------------------------------------

 ---------------- Iteration 890 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 890           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.0072423005 |
| LossBefore              | -9.005417e-09 |
| MeanKL                  | 0.008531702   |
| MeanKLBefore            | 1.9369415e-08 |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.40018293    |
| Step_0-AverageReturn    | 507           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 865           |
| Step_0-MinReturn        | 51.2          |
| Step_0-NumTrajs         | 959           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 187           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40024394    |
| Step_1-AverageReturn    | 489           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 926           |
| Step_1-MinReturn        | 17.8          |
| Step_1-NumTrajs         | 996           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 189           |
| Time                    | 6.19e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.4          |
| Time-OuterStep          | 13.4          |
| Time-SampleProc         | 0.799         |
| Time-Sampling           | 55.3          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.0072422917  |
| n_timesteps             | 285120000     |
-------------------------------------------

 ---------------- Iteration 891 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 891            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0070664375  |
| LossBefore              | -7.1557635e-09 |
| MeanKL                  | 0.0077929944   |
| MeanKLBefore            | 8.939854e-09   |
| Step_0-AverageDiscou... | 198            |
| Step_0-AveragePolicyStd | 0.39993352     |
| Step_0-AverageReturn    | 507            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 881            |
| Step_0-MinReturn        | 13             |
| Step_0-NumTrajs         | 975            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 190            |
| Step_1-AverageDiscou... | 192            |
| Step_1-AveragePolicyStd | 0.39991188     |
| Step_1-AverageReturn    | 491            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 926            |
| Step_1-MinReturn        | 15             |
| Step_1-NumTrajs         | 994            |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 208            |
| Time                    | 6.19e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.816          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0070664305   |
| n_timesteps             | 285440000      |
--------------------------------------------

 ---------------- Iteration 892 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 892           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.007145112  |
| LossBefore              | 1.0868048e-08 |
| MeanKL                  | 0.0081355395  |
| MeanKLBefore            | 5.9599983e-09 |
| Step_0-AverageDiscou... | 205           |
| Step_0-AveragePolicyStd | 0.39996624    |
| Step_0-AverageReturn    | 528           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 866           |
| Step_0-MinReturn        | 40.5          |
| Step_0-NumTrajs         | 967           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 196           |
| Step_1-AveragePolicyStd | 0.39994004    |
| Step_1-AverageReturn    | 498           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 840           |
| Step_1-MinReturn        | 33.4          |
| Step_1-NumTrajs         | 992           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 178           |
| Time                    | 6.2e+04       |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.78          |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0071451226  |
| n_timesteps             | 285760000     |
-------------------------------------------

 ---------------- Iteration 893 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 893            |
| ItrTime                 | 69             |
| LossAfter               | -0.007151605   |
| LossBefore              | -1.9876403e-09 |
| MeanKL                  | 0.007775589    |
| MeanKLBefore            | -7.45087e-09   |
| Step_0-AverageDiscou... | 199            |
| Step_0-AveragePolicyStd | 0.40071443     |
| Step_0-AverageReturn    | 520            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 834            |
| Step_0-MinReturn        | 23             |
| Step_0-NumTrajs         | 949            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 179            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.40068355     |
| Step_1-AverageReturn    | 489            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 875            |
| Step_1-MinReturn        | 19.1           |
| Step_1-NumTrajs         | 1001           |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 201            |
| Time                    | 6.21e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.798          |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.0071516032   |
| n_timesteps             | 286080000      |
--------------------------------------------

 ---------------- Iteration 894 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 894           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.007040255  |
| LossBefore              | 1.1520443e-08 |
| MeanKL                  | 0.00855753    |
| MeanKLBefore            | 2.9805076e-09 |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.4003761     |
| Step_0-AverageReturn    | 522           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 887           |
| Step_0-MinReturn        | 23.9          |
| Step_0-NumTrajs         | 949           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 172           |
| Step_1-AverageDiscou... | 200           |
| Step_1-AveragePolicyStd | 0.40026367    |
| Step_1-AverageReturn    | 519           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 872           |
| Step_1-MinReturn        | 28.8          |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 186           |
| Time                    | 6.21e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.786         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.0070402664  |
| n_timesteps             | 286400000     |
-------------------------------------------

 ---------------- Iteration 895 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 895            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.007089611   |
| LossBefore              | -4.9542086e-09 |
| MeanKL                  | 0.00832173     |
| MeanKLBefore            | 7.448628e-09   |
| Step_0-AverageDiscou... | 213            |
| Step_0-AveragePolicyStd | 0.40030754     |
| Step_0-AverageReturn    | 570            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 837            |
| Step_0-MinReturn        | 25.5           |
| Step_0-NumTrajs         | 901            |
| Step_0-PolicyExecTime   | 1.37           |
| Step_0-StdReturn        | 155            |
| Step_1-AverageDiscou... | 204            |
| Step_1-AveragePolicyStd | 0.40020916     |
| Step_1-AverageReturn    | 531            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 858            |
| Step_1-MinReturn        | 33.4           |
| Step_1-NumTrajs         | 949            |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 172            |
| Time                    | 6.22e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.786          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57.1           |
| dLoss                   | 0.007089606    |
| n_timesteps             | 286720000      |
--------------------------------------------

 ---------------- Iteration 896 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 896           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0071197143 |
| LossBefore              | -6.28021e-09  |
| MeanKL                  | 0.0073583126  |
| MeanKLBefore            | 1.3410802e-08 |
| Step_0-AverageDiscou... | 185           |
| Step_0-AveragePolicyStd | 0.39970794    |
| Step_0-AverageReturn    | 464           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 847           |
| Step_0-MinReturn        | 17.7          |
| Step_0-NumTrajs         | 1034          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 196           |
| Step_1-AverageDiscou... | 184           |
| Step_1-AveragePolicyStd | 0.39972487    |
| Step_1-AverageReturn    | 467           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 894           |
| Step_1-MinReturn        | 14.7          |
| Step_1-NumTrajs         | 1020          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 195           |
| Time                    | 6.23e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.82          |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.007119708   |
| n_timesteps             | 287040000     |
-------------------------------------------

 ---------------- Iteration 897 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 897           |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0073209465 |
| LossBefore              | -6.413504e-10 |
| MeanKL                  | 0.007351662   |
| MeanKLBefore            | 1.3410272e-08 |
| Step_0-AverageDiscou... | 212           |
| Step_0-AveragePolicyStd | 0.39985615    |
| Step_0-AverageReturn    | 563           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 845           |
| Step_0-MinReturn        | 17.2          |
| Step_0-NumTrajs         | 899           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.39982775    |
| Step_1-AverageReturn    | 528           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 896           |
| Step_1-MinReturn        | 29.7          |
| Step_1-NumTrajs         | 959           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 172           |
| Time                    | 6.23e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.789         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.007320946   |
| n_timesteps             | 287360000     |
-------------------------------------------

 ---------------- Iteration 898 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 898           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.006936185  |
| LossBefore              | 1.5262788e-08 |
| MeanKL                  | 0.007935168   |
| MeanKLBefore            | 1.043081e-08  |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.4003653     |
| Step_0-AverageReturn    | 511           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 824           |
| Step_0-MinReturn        | 38.8          |
| Step_0-NumTrajs         | 939           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.4002039     |
| Step_1-AverageReturn    | 503           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 902           |
| Step_1-MinReturn        | 32.3          |
| Step_1-NumTrajs         | 984           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 181           |
| Time                    | 6.24e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0069362004  |
| n_timesteps             | 287680000     |
-------------------------------------------

 ---------------- Iteration 899 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 899           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0076890057 |
| LossBefore              | -2.712273e-09 |
| MeanKL                  | 0.009540041   |
| MeanKLBefore            | -2.979316e-09 |
| Step_0-AverageDiscou... | 206           |
| Step_0-AveragePolicyStd | 0.40017316    |
| Step_0-AverageReturn    | 536           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 901           |
| Step_0-MinReturn        | 15.9          |
| Step_0-NumTrajs         | 936           |
| Step_0-PolicyExecTime   | 1.72          |
| Step_0-StdReturn        | 159           |
| Step_1-AverageDiscou... | 200           |
| Step_1-AveragePolicyStd | 0.40015382    |
| Step_1-AverageReturn    | 518           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 918           |
| Step_1-MinReturn        | 4.56          |
| Step_1-NumTrajs         | 960           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 176           |
| Time                    | 6.25e+04      |
| Time-InnerStep          | 0.169         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.785         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007689003   |
| n_timesteps             | 288000000     |
-------------------------------------------

 ---------------- Iteration 900 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 900            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.007730379   |
| LossBefore              | 1.7348165e-09  |
| MeanKL                  | 0.008489495    |
| MeanKLBefore            | -2.9802323e-09 |
| Step_0-AverageDiscou... | 198            |
| Step_0-AveragePolicyStd | 0.3999096      |
| Step_0-AverageReturn    | 519            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 913            |
| Step_0-MinReturn        | 17.1           |
| Step_0-NumTrajs         | 944            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 195            |
| Step_1-AverageDiscou... | 186            |
| Step_1-AveragePolicyStd | 0.39984974     |
| Step_1-AverageReturn    | 465            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 901            |
| Step_1-MinReturn        | 20.3           |
| Step_1-NumTrajs         | 1053           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 221            |
| Time                    | 6.26e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0077303806   |
| n_timesteps             | 288320000      |
--------------------------------------------

 ---------------- Iteration 901 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 901           |
| ItrTime                 | 71.7          |
| LossAfter               | -0.0073783607 |
| LossBefore              | 9.359183e-10  |
| MeanKL                  | 0.008313407   |
| MeanKLBefore            | 1.1919804e-08 |
| Step_0-AverageDiscou... | 205           |
| Step_0-AveragePolicyStd | 0.39941105    |
| Step_0-AverageReturn    | 544           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 840           |
| Step_0-MinReturn        | 27.9          |
| Step_0-NumTrajs         | 930           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 174           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.39934954    |
| Step_1-AverageReturn    | 533           |
| Step_1-EnvExecTime      | 24.3          |
| Step_1-MaxReturn        | 886           |
| Step_1-MinReturn        | 35.3          |
| Step_1-NumTrajs         | 959           |
| Step_1-PolicyExecTime   | 4.25          |
| Step_1-StdReturn        | 179           |
| Time                    | 6.26e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.784         |
| Time-Sampling           | 57.1          |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.0073783617  |
| n_timesteps             | 288640000     |
-------------------------------------------

 ---------------- Iteration 902 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 902           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0072102756 |
| LossBefore              | 1.4425329e-08 |
| MeanKL                  | 0.007446571   |
| MeanKLBefore            | 2.0858156e-08 |
| Step_0-AverageDiscou... | 196           |
| Step_0-AveragePolicyStd | 0.39950258    |
| Step_0-AverageReturn    | 522           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 923           |
| Step_0-MinReturn        | 9.95          |
| Step_0-NumTrajs         | 935           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 210           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.39937586    |
| Step_1-AverageReturn    | 510           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 920           |
| Step_1-MinReturn        | 11.1          |
| Step_1-NumTrajs         | 973           |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 215           |
| Time                    | 6.27e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.792         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.00721029    |
| n_timesteps             | 288960000     |
-------------------------------------------

 ---------------- Iteration 903 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 903            |
| ItrTime                 | 69             |
| LossAfter               | -0.0072419555  |
| LossBefore              | -1.3345423e-09 |
| MeanKL                  | 0.008132453    |
| MeanKLBefore            | 8.939814e-09   |
| Step_0-AverageDiscou... | 201            |
| Step_0-AveragePolicyStd | 0.3996561      |
| Step_0-AverageReturn    | 517            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 861            |
| Step_0-MinReturn        | 13.1           |
| Step_0-NumTrajs         | 965            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 197            |
| Step_1-AveragePolicyStd | 0.39948586     |
| Step_1-AverageReturn    | 496            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 830            |
| Step_1-MinReturn        | 19.3           |
| Step_1-NumTrajs         | 1006           |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 181            |
| Time                    | 6.28e+04       |
| Time-InnerStep          | 0.16           |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.788          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.007241954    |
| n_timesteps             | 289280000      |
--------------------------------------------

 ---------------- Iteration 904 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 904            |
| ItrTime                 | 68.2           |
| LossAfter               | -0.0073274495  |
| LossBefore              | 1.1104682e-09  |
| MeanKL                  | 0.008456117    |
| MeanKLBefore            | -1.1920192e-08 |
| Step_0-AverageDiscou... | 202            |
| Step_0-AveragePolicyStd | 0.39995298     |
| Step_0-AverageReturn    | 511            |
| Step_0-EnvExecTime      | 22.2           |
| Step_0-MaxReturn        | 872            |
| Step_0-MinReturn        | 53.3           |
| Step_0-NumTrajs         | 981            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 180            |
| Step_1-AverageDiscou... | 200            |
| Step_1-AveragePolicyStd | 0.39986154     |
| Step_1-AverageReturn    | 508            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 907            |
| Step_1-MinReturn        | 38.2           |
| Step_1-NumTrajs         | 988            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 184            |
| Time                    | 6.28e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 53.6           |
| Time-TotalInner         | 54.6           |
| dLoss                   | 0.0073274504   |
| n_timesteps             | 289600000      |
--------------------------------------------

 ---------------- Iteration 905 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 905           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.0073998123 |
| LossBefore              | 8.641747e-09  |
| MeanKL                  | 0.0079422835  |
| MeanKLBefore            | 5.961475e-09  |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40052846    |
| Step_0-AverageReturn    | 520           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 943           |
| Step_0-MinReturn        | 19.1          |
| Step_0-NumTrajs         | 961           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 188           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40051338    |
| Step_1-AverageReturn    | 497           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 946           |
| Step_1-MinReturn        | 3.88          |
| Step_1-NumTrajs         | 992           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.29e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.794         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.007399821   |
| n_timesteps             | 289920000     |
-------------------------------------------

 ---------------- Iteration 906 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 906           |
| ItrTime                 | 68.6          |
| LossAfter               | -0.0072809653 |
| LossBefore              | 1.1691875e-08 |
| MeanKL                  | 0.008830669   |
| MeanKLBefore            | 4.4695616e-09 |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.40040797    |
| Step_0-AverageReturn    | 505           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 887           |
| Step_0-MinReturn        | 11            |
| Step_0-NumTrajs         | 958           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 189           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.40040177    |
| Step_1-AverageReturn    | 504           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 851           |
| Step_1-MinReturn        | 13.2          |
| Step_1-NumTrajs         | 970           |
| Step_1-PolicyExecTime   | 4.09          |
| Step_1-StdReturn        | 186           |
| Time                    | 6.3e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.787         |
| Time-Sampling           | 54.1          |
| Time-TotalInner         | 55.1          |
| dLoss                   | 0.007280977   |
| n_timesteps             | 290240000     |
-------------------------------------------

 ---------------- Iteration 907 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 907            |
| ItrTime                 | 71.2           |
| LossAfter               | -0.0069293864  |
| LossBefore              | 2.095622e-09   |
| MeanKL                  | 0.0077465745   |
| MeanKLBefore            | -2.9813165e-09 |
| Step_0-AverageDiscou... | 194            |
| Step_0-AveragePolicyStd | 0.3991924      |
| Step_0-AverageReturn    | 501            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 890            |
| Step_0-MinReturn        | 16.8           |
| Step_0-NumTrajs         | 988            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 208            |
| Step_1-AverageDiscou... | 195            |
| Step_1-AveragePolicyStd | 0.3991677      |
| Step_1-AverageReturn    | 509            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 881            |
| Step_1-MinReturn        | 19             |
| Step_1-NumTrajs         | 966            |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 195            |
| Time                    | 6.3e+04        |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.802          |
| Time-Sampling           | 56.6           |
| Time-TotalInner         | 57.6           |
| dLoss                   | 0.0069293887   |
| n_timesteps             | 290560000      |
--------------------------------------------

 ---------------- Iteration 908 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 908            |
| ItrTime                 | 68.6           |
| LossAfter               | -0.007557377   |
| LossBefore              | -2.3969324e-09 |
| MeanKL                  | 0.008556667    |
| MeanKLBefore            | 7.450361e-09   |
| Step_0-AverageDiscou... | 199            |
| Step_0-AveragePolicyStd | 0.39939925     |
| Step_0-AverageReturn    | 515            |
| Step_0-EnvExecTime      | 22.5           |
| Step_0-MaxReturn        | 853            |
| Step_0-MinReturn        | 19.6           |
| Step_0-NumTrajs         | 968            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 186            |
| Step_1-AverageDiscou... | 189            |
| Step_1-AveragePolicyStd | 0.3992763      |
| Step_1-AverageReturn    | 474            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 928            |
| Step_1-MinReturn        | 20.4           |
| Step_1-NumTrajs         | 1034           |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 197            |
| Time                    | 6.31e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.801          |
| Time-Sampling           | 54             |
| Time-TotalInner         | 55             |
| dLoss                   | 0.007557375    |
| n_timesteps             | 290880000      |
--------------------------------------------

 ---------------- Iteration 909 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 909            |
| ItrTime                 | 69.4           |
| LossAfter               | -0.0072399154  |
| LossBefore              | -1.3030366e-08 |
| MeanKL                  | 0.008921768    |
| MeanKLBefore            | 6.377121e-14   |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.39993486     |
| Step_0-AverageReturn    | 508            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 851            |
| Step_0-MinReturn        | 24.3           |
| Step_0-NumTrajs         | 957            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 184            |
| Step_1-AverageDiscou... | 191            |
| Step_1-AveragePolicyStd | 0.3999163      |
| Step_1-AverageReturn    | 489            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 858            |
| Step_1-MinReturn        | 18.4           |
| Step_1-NumTrajs         | 987            |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 196            |
| Time                    | 6.32e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.877          |
| Time-Sampling           | 54.8           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.0072399024   |
| n_timesteps             | 291200000      |
--------------------------------------------

 ---------------- Iteration 910 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 910            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.0072435588  |
| LossBefore              | -2.4368194e-09 |
| MeanKL                  | 0.0083176885   |
| MeanKLBefore            | 7.54774e-13    |
| Step_0-AverageDiscou... | 212            |
| Step_0-AveragePolicyStd | 0.40034088     |
| Step_0-AverageReturn    | 574            |
| Step_0-EnvExecTime      | 24.2           |
| Step_0-MaxReturn        | 905            |
| Step_0-MinReturn        | 38.4           |
| Step_0-NumTrajs         | 883            |
| Step_0-PolicyExecTime   | 1.4            |
| Step_0-StdReturn        | 146            |
| Step_1-AverageDiscou... | 203            |
| Step_1-AveragePolicyStd | 0.4002941      |
| Step_1-AverageReturn    | 536            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 883            |
| Step_1-MinReturn        | 15.8           |
| Step_1-NumTrajs         | 935            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 185            |
| Time                    | 6.32e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.797          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.0072435564   |
| n_timesteps             | 291520000      |
--------------------------------------------

 ---------------- Iteration 911 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 911            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0070129046  |
| LossBefore              | 3.9657224e-09  |
| MeanKL                  | 0.008245158    |
| MeanKLBefore            | -2.9787017e-09 |
| Step_0-AverageDiscou... | 201            |
| Step_0-AveragePolicyStd | 0.40007886     |
| Step_0-AverageReturn    | 524            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 849            |
| Step_0-MinReturn        | 19.7           |
| Step_0-NumTrajs         | 961            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 192            |
| Step_1-AverageDiscou... | 202            |
| Step_1-AveragePolicyStd | 0.3999691      |
| Step_1-AverageReturn    | 522            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 896            |
| Step_1-MinReturn        | 23             |
| Step_1-NumTrajs         | 966            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 183            |
| Time                    | 6.33e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.793          |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007012909    |
| n_timesteps             | 291840000      |
--------------------------------------------

 ---------------- Iteration 912 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 912           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.0070764227 |
| LossBefore              | 4.0302366e-09 |
| MeanKL                  | 0.008951224   |
| MeanKLBefore            | 8.939906e-09  |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.39977315    |
| Step_0-AverageReturn    | 506           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 973           |
| Step_0-MinReturn        | 15.3          |
| Step_0-NumTrajs         | 1000          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 193           |
| Step_1-AverageDiscou... | 190           |
| Step_1-AveragePolicyStd | 0.3996414     |
| Step_1-AverageReturn    | 473           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 856           |
| Step_1-MinReturn        | 32.1          |
| Step_1-NumTrajs         | 1034          |
| Step_1-PolicyExecTime   | 4.28          |
| Step_1-StdReturn        | 200           |
| Time                    | 6.34e+04      |
| Time-InnerStep          | 0.165         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.871         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.6          |
| dLoss                   | 0.007076427   |
| n_timesteps             | 292160000     |
-------------------------------------------

 ---------------- Iteration 913 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 913            |
| ItrTime                 | 71             |
| LossAfter               | -0.007849278   |
| LossBefore              | -1.5238356e-09 |
| MeanKL                  | 0.008899031    |
| MeanKLBefore            | -4.2419402e-13 |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.3996637      |
| Step_0-AverageReturn    | 546            |
| Step_0-EnvExecTime      | 24             |
| Step_0-MaxReturn        | 876            |
| Step_0-MinReturn        | 13.9           |
| Step_0-NumTrajs         | 941            |
| Step_0-PolicyExecTime   | 1.41           |
| Step_0-StdReturn        | 160            |
| Step_1-AverageDiscou... | 204            |
| Step_1-AveragePolicyStd | 0.39957112     |
| Step_1-AverageReturn    | 529            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 935            |
| Step_1-MinReturn        | 17.4           |
| Step_1-NumTrajs         | 955            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 175            |
| Time                    | 6.35e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.007849276    |
| n_timesteps             | 292480000      |
--------------------------------------------

 ---------------- Iteration 914 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 914           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007373451  |
| LossBefore              | -4.216182e-09 |
| MeanKL                  | 0.008575894   |
| MeanKLBefore            | 1.4898442e-08 |
| Step_0-AverageDiscou... | 204           |
| Step_0-AveragePolicyStd | 0.39909014    |
| Step_0-AverageReturn    | 533           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 951           |
| Step_0-MinReturn        | 44.1          |
| Step_0-NumTrajs         | 963           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 181           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.39895698    |
| Step_1-AverageReturn    | 520           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 851           |
| Step_1-MinReturn        | 26.9          |
| Step_1-NumTrajs         | 968           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 182           |
| Time                    | 6.35e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.791         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0073734466  |
| n_timesteps             | 292800000     |
-------------------------------------------

 ---------------- Iteration 915 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 915           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.007155308  |
| LossBefore              | 5.223007e-09  |
| MeanKL                  | 0.007921381   |
| MeanKLBefore            | 1.4904027e-09 |
| Step_0-AverageDiscou... | 195           |
| Step_0-AveragePolicyStd | 0.39990097    |
| Step_0-AverageReturn    | 496           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 850           |
| Step_0-MinReturn        | 24            |
| Step_0-NumTrajs         | 1005          |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 191           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.39992633    |
| Step_1-AverageReturn    | 513           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 886           |
| Step_1-MinReturn        | 31            |
| Step_1-NumTrajs         | 971           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 189           |
| Time                    | 6.36e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.789         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.1          |
| dLoss                   | 0.007155313   |
| n_timesteps             | 293120000     |
-------------------------------------------

 ---------------- Iteration 916 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 916           |
| ItrTime                 | 70            |
| LossAfter               | -0.0074694576 |
| LossBefore              | 4.8177347e-09 |
| MeanKL                  | 0.008654579   |
| MeanKLBefore            | 5.9600884e-09 |
| Step_0-AverageDiscou... | 210           |
| Step_0-AveragePolicyStd | 0.39968133    |
| Step_0-AverageReturn    | 568           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 882           |
| Step_0-MinReturn        | 7.33          |
| Step_0-NumTrajs         | 889           |
| Step_0-PolicyExecTime   | 1.39          |
| Step_0-StdReturn        | 173           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.3995769     |
| Step_1-AverageReturn    | 500           |
| Step_1-EnvExecTime      | 22.8          |
| Step_1-MaxReturn        | 899           |
| Step_1-MinReturn        | 11.9          |
| Step_1-NumTrajs         | 1010          |
| Step_1-PolicyExecTime   | 4.04          |
| Step_1-StdReturn        | 191           |
| Time                    | 6.37e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.78          |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007469462   |
| n_timesteps             | 293440000     |
-------------------------------------------

 ---------------- Iteration 917 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 917           |
| ItrTime                 | 69            |
| LossAfter               | -0.007581015  |
| LossBefore              | -4.175728e-09 |
| MeanKL                  | 0.009051651   |
| MeanKLBefore            | 1.3410045e-08 |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.39984763    |
| Step_0-AverageReturn    | 524           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 883           |
| Step_0-MinReturn        | 15.1          |
| Step_0-NumTrajs         | 957           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 193           |
| Step_1-AverageDiscou... | 200           |
| Step_1-AveragePolicyStd | 0.39975506    |
| Step_1-AverageReturn    | 512           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 882           |
| Step_1-MinReturn        | 29.3          |
| Step_1-NumTrajs         | 992           |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 190           |
| Time                    | 6.37e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.798         |
| Time-Sampling           | 54.5          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.007581011   |
| n_timesteps             | 293760000     |
-------------------------------------------

 ---------------- Iteration 918 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 918           |
| ItrTime                 | 69.7          |
| LossAfter               | -0.0073914556 |
| LossBefore              | 5.1135394e-09 |
| MeanKL                  | 0.007899219   |
| MeanKLBefore            | 4.4695847e-09 |
| Step_0-AverageDiscou... | 192           |
| Step_0-AveragePolicyStd | 0.39973763    |
| Step_0-AverageReturn    | 491           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 989           |
| Step_0-MinReturn        | 7.89          |
| Step_0-NumTrajs         | 996           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 197           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.39969584    |
| Step_1-AverageReturn    | 489           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 876           |
| Step_1-MinReturn        | 5.39          |
| Step_1-NumTrajs         | 1001          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 195           |
| Time                    | 6.38e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.805         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.0073914607  |
| n_timesteps             | 294080000     |
-------------------------------------------

 ---------------- Iteration 919 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 919           |
| ItrTime                 | 68.7          |
| LossAfter               | -0.0074188733 |
| LossBefore              | 3.7919254e-09 |
| MeanKL                  | 0.009222959   |
| MeanKLBefore            | 2.980278e-09  |
| Step_0-AverageDiscou... | 189           |
| Step_0-AveragePolicyStd | 0.39988682    |
| Step_0-AverageReturn    | 469           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 874           |
| Step_0-MinReturn        | 32.4          |
| Step_0-NumTrajs         | 1054          |
| Step_0-PolicyExecTime   | 1.29          |
| Step_0-StdReturn        | 209           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.39980313    |
| Step_1-AverageReturn    | 491           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 864           |
| Step_1-MinReturn        | 23.6          |
| Step_1-NumTrajs         | 1007          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 198           |
| Time                    | 6.39e+04      |
| Time-InnerStep          | 0.18          |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.838         |
| Time-Sampling           | 54.1          |
| Time-TotalInner         | 55.1          |
| dLoss                   | 0.007418877   |
| n_timesteps             | 294400000     |
-------------------------------------------

 ---------------- Iteration 920 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 920           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.0076034376 |
| LossBefore              | 3.656649e-09  |
| MeanKL                  | 0.007858632   |
| MeanKLBefore            | 8.93958e-09   |
| Step_0-AverageDiscou... | 197           |
| Step_0-AveragePolicyStd | 0.39979967    |
| Step_0-AverageReturn    | 508           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 847           |
| Step_0-MinReturn        | 17.5          |
| Step_0-NumTrajs         | 972           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 195           |
| Step_1-AverageDiscou... | 188           |
| Step_1-AveragePolicyStd | 0.39975867    |
| Step_1-AverageReturn    | 476           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 890           |
| Step_1-MinReturn        | 13.4          |
| Step_1-NumTrajs         | 1017          |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 202           |
| Time                    | 6.39e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.833         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0076034414  |
| n_timesteps             | 294720000     |
-------------------------------------------

 ---------------- Iteration 921 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 921            |
| ItrTime                 | 69             |
| LossAfter               | -0.006772467   |
| LossBefore              | -8.979001e-09  |
| MeanKL                  | 0.007675004    |
| MeanKLBefore            | -7.4502315e-09 |
| Step_0-AverageDiscou... | 200            |
| Step_0-AveragePolicyStd | 0.40000403     |
| Step_0-AverageReturn    | 513            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 858            |
| Step_0-MinReturn        | 33.4           |
| Step_0-NumTrajs         | 966            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 180            |
| Step_1-AverageDiscou... | 195            |
| Step_1-AveragePolicyStd | 0.39992076     |
| Step_1-AverageReturn    | 499            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 922            |
| Step_1-MinReturn        | 23.5           |
| Step_1-NumTrajs         | 980            |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.4e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.796          |
| Time-Sampling           | 54.4           |
| Time-TotalInner         | 55.4           |
| dLoss                   | 0.006772458    |
| n_timesteps             | 295040000      |
--------------------------------------------

 ---------------- Iteration 922 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 922           |
| ItrTime                 | 71.9          |
| LossAfter               | -0.00726671   |
| LossBefore              | 4.673855e-09  |
| MeanKL                  | 0.008990474   |
| MeanKLBefore            | 4.4693156e-09 |
| Step_0-AverageDiscou... | 204           |
| Step_0-AveragePolicyStd | 0.40029106    |
| Step_0-AverageReturn    | 543           |
| Step_0-EnvExecTime      | 24.2          |
| Step_0-MaxReturn        | 883           |
| Step_0-MinReturn        | 20.9          |
| Step_0-NumTrajs         | 926           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 200           |
| Step_1-AveragePolicyStd | 0.40026763    |
| Step_1-AverageReturn    | 527           |
| Step_1-EnvExecTime      | 24.2          |
| Step_1-MaxReturn        | 880           |
| Step_1-MinReturn        | 19            |
| Step_1-NumTrajs         | 964           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 199           |
| Time                    | 6.41e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.801         |
| Time-Sampling           | 57.3          |
| Time-TotalInner         | 58.3          |
| dLoss                   | 0.0072667147  |
| n_timesteps             | 295360000     |
-------------------------------------------

 ---------------- Iteration 923 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 923           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007624121  |
| LossBefore              | -9.391734e-09 |
| MeanKL                  | 0.008363361   |
| MeanKLBefore            | 2.0860847e-08 |
| Step_0-AverageDiscou... | 209           |
| Step_0-AveragePolicyStd | 0.4000807     |
| Step_0-AverageReturn    | 546           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 927           |
| Step_0-MinReturn        | 34.2          |
| Step_0-NumTrajs         | 929           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 210           |
| Step_1-AveragePolicyStd | 0.4000155     |
| Step_1-AverageReturn    | 550           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 904           |
| Step_1-MinReturn        | 30.5          |
| Step_1-NumTrajs         | 940           |
| Step_1-PolicyExecTime   | 4.25          |
| Step_1-StdReturn        | 170           |
| Time                    | 6.42e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.789         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 55.9          |
| dLoss                   | 0.0076241116  |
| n_timesteps             | 295680000     |
-------------------------------------------

 ---------------- Iteration 924 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 924           |
| ItrTime                 | 71.1          |
| LossAfter               | -0.0073807416 |
| LossBefore              | -1.132706e-08 |
| MeanKL                  | 0.007682373   |
| MeanKLBefore            | 5.959374e-09  |
| Step_0-AverageDiscou... | 214           |
| Step_0-AveragePolicyStd | 0.40000972    |
| Step_0-AverageReturn    | 583           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 944           |
| Step_0-MinReturn        | 38.3          |
| Step_0-NumTrajs         | 867           |
| Step_0-PolicyExecTime   | 1.37          |
| Step_0-StdReturn        | 140           |
| Step_1-AverageDiscou... | 208           |
| Step_1-AveragePolicyStd | 0.4000024     |
| Step_1-AverageReturn    | 558           |
| Step_1-EnvExecTime      | 23.9          |
| Step_1-MaxReturn        | 932           |
| Step_1-MinReturn        | 27.4          |
| Step_1-NumTrajs         | 926           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 189           |
| Time                    | 6.42e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.766         |
| Time-Sampling           | 56.6          |
| Time-TotalInner         | 57.6          |
| dLoss                   | 0.0073807305  |
| n_timesteps             | 296000000     |
-------------------------------------------

 ---------------- Iteration 925 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 925           |
| ItrTime                 | 68.3          |
| LossAfter               | -0.0071200235 |
| LossBefore              | 3.1439327e-09 |
| MeanKL                  | 0.0076608146  |
| MeanKLBefore            | -5.958915e-09 |
| Step_0-AverageDiscou... | 205           |
| Step_0-AveragePolicyStd | 0.39976916    |
| Step_0-AverageReturn    | 525           |
| Step_0-EnvExecTime      | 22            |
| Step_0-MaxReturn        | 903           |
| Step_0-MinReturn        | 55.3          |
| Step_0-NumTrajs         | 973           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 203           |
| Step_1-AveragePolicyStd | 0.39962864    |
| Step_1-AverageReturn    | 516           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 902           |
| Step_1-MinReturn        | 58.1          |
| Step_1-NumTrajs         | 988           |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 188           |
| Time                    | 6.43e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.809         |
| Time-Sampling           | 53.8          |
| Time-TotalInner         | 54.8          |
| dLoss                   | 0.0071200267  |
| n_timesteps             | 296320000     |
-------------------------------------------

 ---------------- Iteration 926 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 926            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0067508044  |
| LossBefore              | -8.51171e-09   |
| MeanKL                  | 0.00795472     |
| MeanKLBefore            | -5.9581353e-09 |
| Step_0-AverageDiscou... | 205            |
| Step_0-AveragePolicyStd | 0.39945257     |
| Step_0-AverageReturn    | 535            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 856            |
| Step_0-MinReturn        | 12.8           |
| Step_0-NumTrajs         | 952            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 207            |
| Step_1-AveragePolicyStd | 0.39936665     |
| Step_1-AverageReturn    | 545            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 936            |
| Step_1-MinReturn        | 9.73           |
| Step_1-NumTrajs         | 941            |
| Step_1-PolicyExecTime   | 4.56           |
| Step_1-StdReturn        | 189            |
| Time                    | 6.44e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.787          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.006750796    |
| n_timesteps             | 296640000      |
--------------------------------------------

 ---------------- Iteration 927 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 927           |
| ItrTime                 | 71.5          |
| LossAfter               | -0.007677628  |
| LossBefore              | 4.836522e-11  |
| MeanKL                  | 0.008431818   |
| MeanKLBefore            | 2.0860496e-08 |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.39970782    |
| Step_0-AverageReturn    | 516           |
| Step_0-EnvExecTime      | 23.8          |
| Step_0-MaxReturn        | 954           |
| Step_0-MinReturn        | 20            |
| Step_0-NumTrajs         | 965           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 204           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.39968562    |
| Step_1-AverageReturn    | 508           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 907           |
| Step_1-MinReturn        | 17.8          |
| Step_1-NumTrajs         | 970           |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 197           |
| Time                    | 6.44e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.84          |
| Time-Sampling           | 56.7          |
| Time-TotalInner         | 57.8          |
| dLoss                   | 0.007677628   |
| n_timesteps             | 296960000     |
-------------------------------------------

 ---------------- Iteration 928 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 928           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.007254121  |
| LossBefore              | 7.0324035e-09 |
| MeanKL                  | 0.007929293   |
| MeanKLBefore            | 5.9580927e-09 |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.4003016     |
| Step_0-AverageReturn    | 511           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 867           |
| Step_0-MinReturn        | 24            |
| Step_0-NumTrajs         | 978           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 200           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.4001939     |
| Step_1-AverageReturn    | 493           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 907           |
| Step_1-MinReturn        | 4.1           |
| Step_1-NumTrajs         | 1015          |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 200           |
| Time                    | 6.45e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.836         |
| Time-Sampling           | 54.5          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.007254128   |
| n_timesteps             | 297280000     |
-------------------------------------------

 ---------------- Iteration 929 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 929            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007703026   |
| LossBefore              | -7.4068005e-09 |
| MeanKL                  | 0.007952148    |
| MeanKLBefore            | 5.960121e-09   |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.40071476     |
| Step_0-AverageReturn    | 520            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 917            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 963            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 209            |
| Step_1-AverageDiscou... | 198            |
| Step_1-AveragePolicyStd | 0.40056536     |
| Step_1-AverageReturn    | 509            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 946            |
| Step_1-MinReturn        | 11.5           |
| Step_1-NumTrajs         | 1009           |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 209            |
| Time                    | 6.46e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.7           |
| Time-OuterStep          | 13.7           |
| Time-SampleProc         | 0.82           |
| Time-Sampling           | 55.5           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.0077030184   |
| n_timesteps             | 297600000      |
--------------------------------------------

 ---------------- Iteration 930 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 930           |
| ItrTime                 | 70.6          |
| LossAfter               | -0.0072423657 |
| LossBefore              | 1.6575911e-09 |
| MeanKL                  | 0.008377995   |
| MeanKLBefore            | 4.4693165e-09 |
| Step_0-AverageDiscou... | 206           |
| Step_0-AveragePolicyStd | 0.40071094    |
| Step_0-AverageReturn    | 533           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 875           |
| Step_0-MinReturn        | 29.9          |
| Step_0-NumTrajs         | 970           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 184           |
| Step_1-AverageDiscou... | 205           |
| Step_1-AveragePolicyStd | 0.400684      |
| Step_1-AverageReturn    | 528           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 941           |
| Step_1-MinReturn        | 36.9          |
| Step_1-NumTrajs         | 978           |
| Step_1-PolicyExecTime   | 4.33          |
| Step_1-StdReturn        | 187           |
| Time                    | 6.46e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.81          |
| Time-Sampling           | 56.1          |
| Time-TotalInner         | 57.1          |
| dLoss                   | 0.0072423676  |
| n_timesteps             | 297920000     |
-------------------------------------------

 ---------------- Iteration 931 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 931            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0072492994  |
| LossBefore              | -7.5300814e-09 |
| MeanKL                  | 0.008344138    |
| MeanKLBefore            | -8.939714e-09  |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.40159744     |
| Step_0-AverageReturn    | 535            |
| Step_0-EnvExecTime      | 22.7           |
| Step_0-MaxReturn        | 859            |
| Step_0-MinReturn        | 34.9           |
| Step_0-NumTrajs         | 954            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 169            |
| Step_1-AverageDiscou... | 204            |
| Step_1-AveragePolicyStd | 0.40155473     |
| Step_1-AverageReturn    | 525            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 927            |
| Step_1-MinReturn        | 26.8           |
| Step_1-NumTrajs         | 980            |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 196            |
| Time                    | 6.47e+04       |
| Time-InnerStep          | 0.165          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.804          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.007249292    |
| n_timesteps             | 298240000      |
--------------------------------------------

 ---------------- Iteration 932 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 932           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007259713  |
| LossBefore              | 1.1079109e-08 |
| MeanKL                  | 0.00848913    |
| MeanKLBefore            | -5.960127e-09 |
| Step_0-AverageDiscou... | 207           |
| Step_0-AveragePolicyStd | 0.40139762    |
| Step_0-AverageReturn    | 537           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 893           |
| Step_0-MinReturn        | 36.7          |
| Step_0-NumTrajs         | 957           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 182           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.40139738    |
| Step_1-AverageReturn    | 527           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 924           |
| Step_1-MinReturn        | 14.8          |
| Step_1-NumTrajs         | 963           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 179           |
| Time                    | 6.48e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.767         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007259724   |
| n_timesteps             | 298560000     |
-------------------------------------------

 ---------------- Iteration 933 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 933           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007306707  |
| LossBefore              | 5.3314286e-09 |
| MeanKL                  | 0.008871739   |
| MeanKLBefore            | -2.979872e-09 |
| Step_0-AverageDiscou... | 205           |
| Step_0-AveragePolicyStd | 0.4012902     |
| Step_0-AverageReturn    | 543           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 876           |
| Step_0-MinReturn        | 30.3          |
| Step_0-NumTrajs         | 915           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 171           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.40113813    |
| Step_1-AverageReturn    | 531           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 969           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.49e+04      |
| Time-InnerStep          | 0.169         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.805         |
| Time-Sampling           | 55.9          |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.007306712   |
| n_timesteps             | 298880000     |
-------------------------------------------

 ---------------- Iteration 934 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 934           |
| ItrTime                 | 70            |
| LossAfter               | -0.0075143874 |
| LossBefore              | -5.545274e-09 |
| MeanKL                  | 0.008202696   |
| MeanKLBefore            | 2.979521e-09  |
| Step_0-AverageDiscou... | 195           |
| Step_0-AveragePolicyStd | 0.4002406     |
| Step_0-AverageReturn    | 494           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 919           |
| Step_0-MinReturn        | 23.9          |
| Step_0-NumTrajs         | 1007          |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 191           |
| Step_1-AverageDiscou... | 198           |
| Step_1-AveragePolicyStd | 0.40009433    |
| Step_1-AverageReturn    | 505           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 925           |
| Step_1-MinReturn        | 17.7          |
| Step_1-NumTrajs         | 1000          |
| Step_1-PolicyExecTime   | 4.29          |
| Step_1-StdReturn        | 195           |
| Time                    | 6.49e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.835         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.3          |
| dLoss                   | 0.007514382   |
| n_timesteps             | 299200000     |
-------------------------------------------

 ---------------- Iteration 935 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 935            |
| ItrTime                 | 71             |
| LossAfter               | -0.0072333603  |
| LossBefore              | -2.3949451e-09 |
| MeanKL                  | 0.007795901    |
| MeanKLBefore            | 1.0431165e-08  |
| Step_0-AverageDiscou... | 197            |
| Step_0-AveragePolicyStd | 0.4006008      |
| Step_0-AverageReturn    | 512            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 833            |
| Step_0-MinReturn        | 21.9           |
| Step_0-NumTrajs         | 972            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 190            |
| Step_1-AverageDiscou... | 195            |
| Step_1-AveragePolicyStd | 0.40050176     |
| Step_1-AverageReturn    | 505            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 846            |
| Step_1-MinReturn        | 31.4           |
| Step_1-NumTrajs         | 994            |
| Step_1-PolicyExecTime   | 4.33           |
| Step_1-StdReturn        | 195            |
| Time                    | 6.5e+04        |
| Time-InnerStep          | 0.166          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.826          |
| Time-Sampling           | 56.3           |
| Time-TotalInner         | 57.3           |
| dLoss                   | 0.007233358    |
| n_timesteps             | 299520000      |
--------------------------------------------

 ---------------- Iteration 936 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 936            |
| ItrTime                 | 69.9           |
| LossAfter               | -0.0074430555  |
| LossBefore              | 1.3853413e-09  |
| MeanKL                  | 0.008362082    |
| MeanKLBefore            | -1.0429622e-08 |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.40032408     |
| Step_0-AverageReturn    | 559            |
| Step_0-EnvExecTime      | 23.3           |
| Step_0-MaxReturn        | 977            |
| Step_0-MinReturn        | 13.4           |
| Step_0-NumTrajs         | 917            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 172            |
| Step_1-AverageDiscou... | 206            |
| Step_1-AveragePolicyStd | 0.40030566     |
| Step_1-AverageReturn    | 544            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 884            |
| Step_1-MinReturn        | 35             |
| Step_1-NumTrajs         | 941            |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 182            |
| Time                    | 6.51e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.3           |
| dLoss                   | 0.007443057    |
| n_timesteps             | 299840000      |
--------------------------------------------

 ---------------- Iteration 937 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 937            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.008463351   |
| LossBefore              | -4.6798694e-09 |
| MeanKL                  | 0.009834454    |
| MeanKLBefore            | 1.1290524e-12  |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.4002163      |
| Step_0-AverageReturn    | 542            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 904            |
| Step_0-MinReturn        | 34.8           |
| Step_0-NumTrajs         | 944            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 202            |
| Step_1-AveragePolicyStd | 0.40012896     |
| Step_1-AverageReturn    | 519            |
| Step_1-EnvExecTime      | 23.7           |
| Step_1-MaxReturn        | 933            |
| Step_1-MinReturn        | 32.4           |
| Step_1-NumTrajs         | 974            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.51e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.777          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.008463346    |
| n_timesteps             | 300160000      |
--------------------------------------------

 ---------------- Iteration 938 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 938           |
| ItrTime                 | 71            |
| LossAfter               | -0.006791074  |
| LossBefore              | 3.5958152e-09 |
| MeanKL                  | 0.007888101   |
| MeanKLBefore            | 4.4710804e-09 |
| Step_0-AverageDiscou... | 208           |
| Step_0-AveragePolicyStd | 0.40038934    |
| Step_0-AverageReturn    | 543           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 898           |
| Step_0-MinReturn        | 10.3          |
| Step_0-NumTrajs         | 936           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 170           |
| Step_1-AverageDiscou... | 205           |
| Step_1-AveragePolicyStd | 0.40046778    |
| Step_1-AverageReturn    | 537           |
| Step_1-EnvExecTime      | 24.1          |
| Step_1-MaxReturn        | 893           |
| Step_1-MinReturn        | 12.2          |
| Step_1-NumTrajs         | 930           |
| Step_1-PolicyExecTime   | 4.3           |
| Step_1-StdReturn        | 176           |
| Time                    | 6.52e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.867         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.0067910776  |
| n_timesteps             | 300480000     |
-------------------------------------------

 ---------------- Iteration 939 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 939          |
| ItrTime                 | 69.2         |
| LossAfter               | -0.007473967 |
| LossBefore              | 2.269439e-09 |
| MeanKL                  | 0.007651335  |
| MeanKLBefore            | 4.46878e-09  |
| Step_0-AverageDiscou... | 201          |
| Step_0-AveragePolicyStd | 0.40018728   |
| Step_0-AverageReturn    | 524          |
| Step_0-EnvExecTime      | 22.8         |
| Step_0-MaxReturn        | 853          |
| Step_0-MinReturn        | 17.1         |
| Step_0-NumTrajs         | 943          |
| Step_0-PolicyExecTime   | 1.3          |
| Step_0-StdReturn        | 181          |
| Step_1-AverageDiscou... | 201          |
| Step_1-AveragePolicyStd | 0.40016323   |
| Step_1-AverageReturn    | 519          |
| Step_1-EnvExecTime      | 23.2         |
| Step_1-MaxReturn        | 951          |
| Step_1-MinReturn        | 14.1         |
| Step_1-NumTrajs         | 979          |
| Step_1-PolicyExecTime   | 4.12         |
| Step_1-StdReturn        | 188          |
| Time                    | 6.53e+04     |
| Time-InnerStep          | 0.163        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.795        |
| Time-Sampling           | 54.7         |
| Time-TotalInner         | 55.7         |
| dLoss                   | 0.0074739694 |
| n_timesteps             | 300800000    |
------------------------------------------

 ---------------- Iteration 940 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 940           |
| ItrTime                 | 70.5          |
| LossAfter               | -0.007316838  |
| LossBefore              | -9.773469e-09 |
| MeanKL                  | 0.0079392     |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | 212           |
| Step_0-AveragePolicyStd | 0.4001694     |
| Step_0-AverageReturn    | 565           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 897           |
| Step_0-MinReturn        | 57.6          |
| Step_0-NumTrajs         | 912           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 207           |
| Step_1-AveragePolicyStd | 0.40013683    |
| Step_1-AverageReturn    | 548           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 902           |
| Step_1-MinReturn        | 31.1          |
| Step_1-NumTrajs         | 951           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 188           |
| Time                    | 6.54e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.767         |
| Time-Sampling           | 56            |
| Time-TotalInner         | 56.9          |
| dLoss                   | 0.0073168282  |
| n_timesteps             | 301120000     |
-------------------------------------------

 ---------------- Iteration 941 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 941           |
| ItrTime                 | 70.4          |
| LossAfter               | -0.007118106  |
| LossBefore              | -1.073345e-08 |
| MeanKL                  | 0.00848642    |
| MeanKLBefore            | 1.0430032e-08 |
| Step_0-AverageDiscou... | 206           |
| Step_0-AveragePolicyStd | 0.40017447    |
| Step_0-AverageReturn    | 539           |
| Step_0-EnvExecTime      | 23.6          |
| Step_0-MaxReturn        | 895           |
| Step_0-MinReturn        | 22.7          |
| Step_0-NumTrajs         | 948           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 172           |
| Step_1-AverageDiscou... | 202           |
| Step_1-AveragePolicyStd | 0.39998582    |
| Step_1-AverageReturn    | 531           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 842           |
| Step_1-MinReturn        | 16.4          |
| Step_1-NumTrajs         | 950           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 181           |
| Time                    | 6.54e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.793         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.007118095   |
| n_timesteps             | 301440000     |
-------------------------------------------

 ---------------- Iteration 942 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 942           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.0072869943 |
| LossBefore              | 5.120616e-09  |
| MeanKL                  | 0.008231928   |
| MeanKLBefore            | 4.470376e-09  |
| Step_0-AverageDiscou... | 211           |
| Step_0-AveragePolicyStd | 0.4000223     |
| Step_0-AverageReturn    | 557           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 840           |
| Step_0-MinReturn        | 25.7          |
| Step_0-NumTrajs         | 934           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 207           |
| Step_1-AveragePolicyStd | 0.39996406    |
| Step_1-AverageReturn    | 543           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 883           |
| Step_1-MinReturn        | 18.4          |
| Step_1-NumTrajs         | 941           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 182           |
| Time                    | 6.55e+04      |
| Time-InnerStep          | 0.176         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.797         |
| Time-Sampling           | 55.6          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0072869994  |
| n_timesteps             | 301760000     |
-------------------------------------------

 ---------------- Iteration 943 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 943            |
| ItrTime                 | 68.8           |
| LossAfter               | -0.00745401    |
| LossBefore              | -2.9127096e-09 |
| MeanKL                  | 0.008139521    |
| MeanKLBefore            | -2.9797387e-09 |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.39962643     |
| Step_0-AverageReturn    | 545            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 886            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 942            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 201            |
| Step_1-AveragePolicyStd | 0.39955634     |
| Step_1-AverageReturn    | 510            |
| Step_1-EnvExecTime      | 22.9           |
| Step_1-MaxReturn        | 882            |
| Step_1-MinReturn        | 11.2           |
| Step_1-NumTrajs         | 1003           |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 193            |
| Time                    | 6.56e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.785          |
| Time-Sampling           | 54.2           |
| Time-TotalInner         | 55.2           |
| dLoss                   | 0.0074540074   |
| n_timesteps             | 302080000      |
--------------------------------------------

 ---------------- Iteration 944 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 944           |
| ItrTime                 | 69.1          |
| LossAfter               | -0.007482373  |
| LossBefore              | 1.2673143e-09 |
| MeanKL                  | 0.007706468   |
| MeanKLBefore            | 7.450158e-09  |
| Step_0-AverageDiscou... | 189           |
| Step_0-AveragePolicyStd | 0.39994133    |
| Step_0-AverageReturn    | 475           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 838           |
| Step_0-MinReturn        | 22.3          |
| Step_0-NumTrajs         | 1040          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 208           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.39997652    |
| Step_1-AverageReturn    | 497           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 883           |
| Step_1-MinReturn        | 13.8          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 208           |
| Time                    | 6.56e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.844         |
| Time-Sampling           | 54.5          |
| Time-TotalInner         | 55.5          |
| dLoss                   | 0.0074823746  |
| n_timesteps             | 302400000     |
-------------------------------------------

 ---------------- Iteration 945 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 945            |
| ItrTime                 | 70.1           |
| LossAfter               | -0.007487604   |
| LossBefore              | -7.7619395e-09 |
| MeanKL                  | 0.00841242     |
| MeanKLBefore            | 1.0428982e-08  |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.4005397      |
| Step_0-AverageReturn    | 542            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 862            |
| Step_0-MinReturn        | 4.78           |
| Step_0-NumTrajs         | 940            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 202            |
| Step_1-AveragePolicyStd | 0.40038532     |
| Step_1-AverageReturn    | 526            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 874            |
| Step_1-MinReturn        | 24.4           |
| Step_1-NumTrajs         | 952            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.57e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.824          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.4           |
| dLoss                   | 0.007487596    |
| n_timesteps             | 302720000      |
--------------------------------------------

 ---------------- Iteration 946 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 946            |
| ItrTime                 | 69.7           |
| LossAfter               | -0.007397771   |
| LossBefore              | 2.5651132e-09  |
| MeanKL                  | 0.008685604    |
| MeanKLBefore            | -1.0430645e-08 |
| Step_0-AverageDiscou... | 204            |
| Step_0-AveragePolicyStd | 0.40103644     |
| Step_0-AverageReturn    | 539            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 857            |
| Step_0-MinReturn        | 11.5           |
| Step_0-NumTrajs         | 938            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 193            |
| Step_1-AveragePolicyStd | 0.4010103      |
| Step_1-AverageReturn    | 493            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 928            |
| Step_1-MinReturn        | 14.8           |
| Step_1-NumTrajs         | 993            |
| Step_1-PolicyExecTime   | 4.09           |
| Step_1-StdReturn        | 198            |
| Time                    | 6.58e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.786          |
| Time-Sampling           | 55.1           |
| Time-TotalInner         | 56.1           |
| dLoss                   | 0.0073977737   |
| n_timesteps             | 303040000      |
--------------------------------------------

 ---------------- Iteration 947 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 947           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007405787  |
| LossBefore              | 1.8171373e-09 |
| MeanKL                  | 0.008179878   |
| MeanKLBefore            | 1.3407929e-08 |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.4009997     |
| Step_0-AverageReturn    | 524           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 991           |
| Step_0-MinReturn        | 26.3          |
| Step_0-NumTrajs         | 953           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 191           |
| Step_1-AverageDiscou... | 193           |
| Step_1-AveragePolicyStd | 0.40103546    |
| Step_1-AverageReturn    | 495           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 879           |
| Step_1-MinReturn        | 21.8          |
| Step_1-NumTrajs         | 1001          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 204           |
| Time                    | 6.58e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.7          |
| Time-OuterStep          | 13.7          |
| Time-SampleProc         | 0.827         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.0074057886  |
| n_timesteps             | 303360000     |
-------------------------------------------

 ---------------- Iteration 948 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 948            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.007190441   |
| LossBefore              | 7.2901982e-09  |
| MeanKL                  | 0.0075954273   |
| MeanKLBefore            | 1.19216335e-08 |
| Step_0-AverageDiscou... | 204            |
| Step_0-AveragePolicyStd | 0.40122858     |
| Step_0-AverageReturn    | 536            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 914            |
| Step_0-MinReturn        | 41.8           |
| Step_0-NumTrajs         | 923            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 176            |
| Step_1-AverageDiscou... | 203            |
| Step_1-AveragePolicyStd | 0.40119857     |
| Step_1-AverageReturn    | 531            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 844            |
| Step_1-MinReturn        | 10.1           |
| Step_1-NumTrajs         | 953            |
| Step_1-PolicyExecTime   | 4.22           |
| Step_1-StdReturn        | 191            |
| Time                    | 6.59e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.81           |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007190448    |
| n_timesteps             | 303680000      |
--------------------------------------------

 ---------------- Iteration 949 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 949           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.0075906203 |
| LossBefore              | 6.5004144e-09 |
| MeanKL                  | 0.008158295   |
| MeanKLBefore            | 2.5460523e-12 |
| Step_0-AverageDiscou... | 203           |
| Step_0-AveragePolicyStd | 0.4018203     |
| Step_0-AverageReturn    | 522           |
| Step_0-EnvExecTime      | 22.4          |
| Step_0-MaxReturn        | 846           |
| Step_0-MinReturn        | 12.8          |
| Step_0-NumTrajs         | 962           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 199           |
| Step_1-AveragePolicyStd | 0.40167502    |
| Step_1-AverageReturn    | 512           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 907           |
| Step_1-MinReturn        | 12.4          |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 191           |
| Time                    | 6.6e+04       |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.796         |
| Time-Sampling           | 54.4          |
| Time-TotalInner         | 55.4          |
| dLoss                   | 0.007590627   |
| n_timesteps             | 304000000     |
-------------------------------------------

 ---------------- Iteration 950 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 950            |
| ItrTime                 | 70.4           |
| LossAfter               | -0.0071724704  |
| LossBefore              | 1.3458708e-08  |
| MeanKL                  | 0.0071579916   |
| MeanKLBefore            | -2.9794958e-09 |
| Step_0-AverageDiscou... | 201            |
| Step_0-AveragePolicyStd | 0.40170127     |
| Step_0-AverageReturn    | 526            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 891            |
| Step_0-MinReturn        | 13.2           |
| Step_0-NumTrajs         | 942            |
| Step_0-PolicyExecTime   | 1.39           |
| Step_0-StdReturn        | 184            |
| Step_1-AverageDiscou... | 198            |
| Step_1-AveragePolicyStd | 0.40162203     |
| Step_1-AverageReturn    | 507            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 871            |
| Step_1-MinReturn        | 18             |
| Step_1-NumTrajs         | 982            |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 192            |
| Time                    | 6.6e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.798          |
| Time-Sampling           | 55.8           |
| Time-TotalInner         | 56.8           |
| dLoss                   | 0.007172484    |
| n_timesteps             | 304320000      |
--------------------------------------------

 ---------------- Iteration 951 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 951           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.007322525  |
| LossBefore              | 9.848139e-09  |
| MeanKL                  | 0.008426883   |
| MeanKLBefore            | -1.490988e-09 |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40171823    |
| Step_0-AverageReturn    | 508           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 890           |
| Step_0-MinReturn        | 15.2          |
| Step_0-NumTrajs         | 1002          |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 201           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40174538    |
| Step_1-AverageReturn    | 491           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 873           |
| Step_1-MinReturn        | 19            |
| Step_1-NumTrajs         | 1011          |
| Step_1-PolicyExecTime   | 4.13          |
| Step_1-StdReturn        | 204           |
| Time                    | 6.61e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.803         |
| Time-Sampling           | 54.3          |
| Time-TotalInner         | 55.3          |
| dLoss                   | 0.007322535   |
| n_timesteps             | 304640000     |
-------------------------------------------

 ---------------- Iteration 952 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 952          |
| ItrTime                 | 70.4         |
| LossAfter               | -0.007754497 |
| LossBefore              | 2.73682e-09  |
| MeanKL                  | 0.008727098  |
| MeanKLBefore            | 8.939155e-09 |
| Step_0-AverageDiscou... | 186          |
| Step_0-AveragePolicyStd | 0.40205643   |
| Step_0-AverageReturn    | 471          |
| Step_0-EnvExecTime      | 23.3         |
| Step_0-MaxReturn        | 934          |
| Step_0-MinReturn        | 17           |
| Step_0-NumTrajs         | 1036         |
| Step_0-PolicyExecTime   | 1.32         |
| Step_0-StdReturn        | 218          |
| Step_1-AverageDiscou... | 192          |
| Step_1-AveragePolicyStd | 0.40201515   |
| Step_1-AverageReturn    | 494          |
| Step_1-EnvExecTime      | 23.6         |
| Step_1-MaxReturn        | 887          |
| Step_1-MinReturn        | 21           |
| Step_1-NumTrajs         | 1004         |
| Step_1-PolicyExecTime   | 4.18         |
| Step_1-StdReturn        | 212          |
| Time                    | 6.62e+04     |
| Time-InnerStep          | 0.162        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.818        |
| Time-Sampling           | 55.8         |
| Time-TotalInner         | 56.8         |
| dLoss                   | 0.0077545    |
| n_timesteps             | 304960000    |
------------------------------------------

 ---------------- Iteration 953 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 953           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.0074179485 |
| LossBefore              | -6.110267e-09 |
| MeanKL                  | 0.0087276995  |
| MeanKLBefore            | 2.9810194e-09 |
| Step_0-AverageDiscou... | 214           |
| Step_0-AveragePolicyStd | 0.40270996    |
| Step_0-AverageReturn    | 565           |
| Step_0-EnvExecTime      | 22.5          |
| Step_0-MaxReturn        | 867           |
| Step_0-MinReturn        | 33.4          |
| Step_0-NumTrajs         | 909           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 163           |
| Step_1-AverageDiscou... | 209           |
| Step_1-AveragePolicyStd | 0.40255386    |
| Step_1-AverageReturn    | 540           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 900           |
| Step_1-MinReturn        | 29.8          |
| Step_1-NumTrajs         | 956           |
| Step_1-PolicyExecTime   | 4.14          |
| Step_1-StdReturn        | 178           |
| Time                    | 6.63e+04      |
| Time-InnerStep          | 0.185         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.785         |
| Time-Sampling           | 54.3          |
| Time-TotalInner         | 55.3          |
| dLoss                   | 0.0074179424  |
| n_timesteps             | 305280000     |
-------------------------------------------

 ---------------- Iteration 954 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 954            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.007918086   |
| LossBefore              | -2.0738433e-10 |
| MeanKL                  | 0.008127332    |
| MeanKLBefore            | 1.4908459e-09  |
| Step_0-AverageDiscou... | 199            |
| Step_0-AveragePolicyStd | 0.40294266     |
| Step_0-AverageReturn    | 531            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 875            |
| Step_0-MinReturn        | 8.82           |
| Step_0-NumTrajs         | 916            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 201            |
| Step_1-AverageDiscou... | 200            |
| Step_1-AveragePolicyStd | 0.402826       |
| Step_1-AverageReturn    | 526            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 857            |
| Step_1-MinReturn        | 18.9           |
| Step_1-NumTrajs         | 945            |
| Step_1-PolicyExecTime   | 4.6            |
| Step_1-StdReturn        | 197            |
| Time                    | 6.63e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.79           |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 57.9           |
| dLoss                   | 0.007918086    |
| n_timesteps             | 305600000      |
--------------------------------------------

 ---------------- Iteration 955 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 955           |
| ItrTime                 | 69.8          |
| LossAfter               | -0.007183696  |
| LossBefore              | 6.002426e-09  |
| MeanKL                  | 0.007645309   |
| MeanKLBefore            | 1.1921086e-08 |
| Step_0-AverageDiscou... | 201           |
| Step_0-AveragePolicyStd | 0.40323552    |
| Step_0-AverageReturn    | 525           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 894           |
| Step_0-MinReturn        | 16            |
| Step_0-NumTrajs         | 944           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 176           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.4032267     |
| Step_1-AverageReturn    | 506           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 919           |
| Step_1-MinReturn        | 21.4          |
| Step_1-NumTrajs         | 982           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 192           |
| Time                    | 6.64e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.808         |
| Time-Sampling           | 55.2          |
| Time-TotalInner         | 56.2          |
| dLoss                   | 0.007183702   |
| n_timesteps             | 305920000     |
-------------------------------------------

 ---------------- Iteration 956 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 956           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0076567903 |
| LossBefore              | -5.620133e-09 |
| MeanKL                  | 0.008189807   |
| MeanKLBefore            | 7.4475297e-09 |
| Step_0-AverageDiscou... | 204           |
| Step_0-AveragePolicyStd | 0.40364343    |
| Step_0-AverageReturn    | 541           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 882           |
| Step_0-MinReturn        | 4.33          |
| Step_0-NumTrajs         | 925           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 187           |
| Step_1-AverageDiscou... | 193           |
| Step_1-AveragePolicyStd | 0.4035235     |
| Step_1-AverageReturn    | 501           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 1e+03         |
| Step_1-MinReturn        | 5.3           |
| Step_1-NumTrajs         | 996           |
| Step_1-PolicyExecTime   | 4.18          |
| Step_1-StdReturn        | 219           |
| Time                    | 6.65e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.79          |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.8          |
| dLoss                   | 0.0076567847  |
| n_timesteps             | 306240000     |
-------------------------------------------

 ---------------- Iteration 957 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 957            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0066578267  |
| LossBefore              | -4.0988524e-09 |
| MeanKL                  | 0.007260178    |
| MeanKLBefore            | 8.939148e-09   |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.40324408     |
| Step_0-AverageReturn    | 549            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 895            |
| Step_0-MinReturn        | 12.2           |
| Step_0-NumTrajs         | 917            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 180            |
| Step_1-AverageDiscou... | 199            |
| Step_1-AveragePolicyStd | 0.4031753      |
| Step_1-AverageReturn    | 512            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 897            |
| Step_1-MinReturn        | 0.485          |
| Step_1-NumTrajs         | 981            |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 208            |
| Time                    | 6.65e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.786          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0066578225   |
| n_timesteps             | 306560000      |
--------------------------------------------

 ---------------- Iteration 958 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 958            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.007547119   |
| LossBefore              | -6.3034036e-09 |
| MeanKL                  | 0.008064024    |
| MeanKLBefore            | 5.9615433e-09  |
| Step_0-AverageDiscou... | 194            |
| Step_0-AveragePolicyStd | 0.40349317     |
| Step_0-AverageReturn    | 500            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 866            |
| Step_0-MinReturn        | 15.9           |
| Step_0-NumTrajs         | 996            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 214            |
| Step_1-AverageDiscou... | 197            |
| Step_1-AveragePolicyStd | 0.40352318     |
| Step_1-AverageReturn    | 503            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 940            |
| Step_1-MinReturn        | 22.7           |
| Step_1-NumTrajs         | 1006           |
| Step_1-PolicyExecTime   | 4.14           |
| Step_1-StdReturn        | 207            |
| Time                    | 6.66e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.808          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.0075471126   |
| n_timesteps             | 306880000      |
--------------------------------------------

 ---------------- Iteration 959 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 959           |
| ItrTime                 | 71            |
| LossAfter               | -0.007270166  |
| LossBefore              | 2.356803e-08  |
| MeanKL                  | 0.007719231   |
| MeanKLBefore            | 1.3408777e-08 |
| Step_0-AverageDiscou... | 209           |
| Step_0-AveragePolicyStd | 0.40355298    |
| Step_0-AverageReturn    | 550           |
| Step_0-EnvExecTime      | 23.7          |
| Step_0-MaxReturn        | 878           |
| Step_0-MinReturn        | 21            |
| Step_0-NumTrajs         | 928           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.40352702    |
| Step_1-AverageReturn    | 537           |
| Step_1-EnvExecTime      | 23.8          |
| Step_1-MaxReturn        | 886           |
| Step_1-MinReturn        | 18            |
| Step_1-NumTrajs         | 936           |
| Step_1-PolicyExecTime   | 4.23          |
| Step_1-StdReturn        | 179           |
| Time                    | 6.67e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.778         |
| Time-Sampling           | 56.5          |
| Time-TotalInner         | 57.5          |
| dLoss                   | 0.00727019    |
| n_timesteps             | 307200000     |
-------------------------------------------

 ---------------- Iteration 960 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 960            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.007252398   |
| LossBefore              | 6.0484675e-09  |
| MeanKL                  | 0.008261599    |
| MeanKLBefore            | -2.9808618e-09 |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.40384287     |
| Step_0-AverageReturn    | 542            |
| Step_0-EnvExecTime      | 23             |
| Step_0-MaxReturn        | 878            |
| Step_0-MinReturn        | 9.39           |
| Step_0-NumTrajs         | 929            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 178            |
| Step_1-AverageDiscou... | 201            |
| Step_1-AveragePolicyStd | 0.40376648     |
| Step_1-AverageReturn    | 515            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 931            |
| Step_1-MinReturn        | 5.44           |
| Step_1-NumTrajs         | 975            |
| Step_1-PolicyExecTime   | 4.06           |
| Step_1-StdReturn        | 193            |
| Time                    | 6.67e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.78           |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.007252404    |
| n_timesteps             | 307520000      |
--------------------------------------------

 ---------------- Iteration 961 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 961           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0070544207 |
| LossBefore              | 3.9946055e-09 |
| MeanKL                  | 0.007783351   |
| MeanKLBefore            | 5.9593406e-09 |
| Step_0-AverageDiscou... | 209           |
| Step_0-AveragePolicyStd | 0.40349108    |
| Step_0-AverageReturn    | 552           |
| Step_0-EnvExecTime      | 23.1          |
| Step_0-MaxReturn        | 927           |
| Step_0-MinReturn        | 26.2          |
| Step_0-NumTrajs         | 917           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 169           |
| Step_1-AverageDiscou... | 202           |
| Step_1-AveragePolicyStd | 0.4033903     |
| Step_1-AverageReturn    | 524           |
| Step_1-EnvExecTime      | 22.9          |
| Step_1-MaxReturn        | 967           |
| Step_1-MinReturn        | 18.7          |
| Step_1-NumTrajs         | 967           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.68e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.777         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.7          |
| dLoss                   | 0.007054425   |
| n_timesteps             | 307840000     |
-------------------------------------------

 ---------------- Iteration 962 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 962            |
| ItrTime                 | 70.6           |
| LossAfter               | -0.0077847973  |
| LossBefore              | -1.9960926e-09 |
| MeanKL                  | 0.008391505    |
| MeanKLBefore            | 4.4702566e-09  |
| Step_0-AverageDiscou... | 208            |
| Step_0-AveragePolicyStd | 0.40359703     |
| Step_0-AverageReturn    | 556            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 891            |
| Step_0-MinReturn        | 27.8           |
| Step_0-NumTrajs         | 925            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 182            |
| Step_1-AverageDiscou... | 208            |
| Step_1-AveragePolicyStd | 0.40352133     |
| Step_1-AverageReturn    | 550            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 884            |
| Step_1-MinReturn        | 30.1           |
| Step_1-NumTrajs         | 951            |
| Step_1-PolicyExecTime   | 4.21           |
| Step_1-StdReturn        | 181            |
| Time                    | 6.69e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.777          |
| Time-Sampling           | 56.1           |
| Time-TotalInner         | 57             |
| dLoss                   | 0.0077847955   |
| n_timesteps             | 308160000      |
--------------------------------------------

 ---------------- Iteration 963 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 963            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.0071168556  |
| LossBefore              | -1.1642178e-09 |
| MeanKL                  | 0.0077749514   |
| MeanKLBefore            | -1.0428941e-08 |
| Step_0-AverageDiscou... | 203            |
| Step_0-AveragePolicyStd | 0.40374774     |
| Step_0-AverageReturn    | 537            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 934            |
| Step_0-MinReturn        | 18.7           |
| Step_0-NumTrajs         | 949            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 194            |
| Step_1-AverageDiscou... | 201            |
| Step_1-AveragePolicyStd | 0.40368602     |
| Step_1-AverageReturn    | 521            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 951            |
| Step_1-MinReturn        | 10.5           |
| Step_1-NumTrajs         | 996            |
| Step_1-PolicyExecTime   | 4.12           |
| Step_1-StdReturn        | 204            |
| Time                    | 6.7e+04        |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.789          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007116854    |
| n_timesteps             | 308480000      |
--------------------------------------------

 ---------------- Iteration 964 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 964           |
| ItrTime                 | 71.8          |
| LossAfter               | -0.00728254   |
| LossBefore              | 1.3351663e-09 |
| MeanKL                  | 0.007950252   |
| MeanKLBefore            | 1.4886087e-09 |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40335768    |
| Step_0-AverageReturn    | 516           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 922           |
| Step_0-MinReturn        | 21.1          |
| Step_0-NumTrajs         | 979           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 190           |
| Step_1-AverageDiscou... | 205           |
| Step_1-AveragePolicyStd | 0.40337315    |
| Step_1-AverageReturn    | 541           |
| Step_1-EnvExecTime      | 24.3          |
| Step_1-MaxReturn        | 959           |
| Step_1-MinReturn        | 16.2          |
| Step_1-NumTrajs         | 949           |
| Step_1-PolicyExecTime   | 4.24          |
| Step_1-StdReturn        | 194           |
| Time                    | 6.7e+04       |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.798         |
| Time-Sampling           | 57.1          |
| Time-TotalInner         | 58.1          |
| dLoss                   | 0.0072825416  |
| n_timesteps             | 308800000     |
-------------------------------------------

 ---------------- Iteration 965 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 965           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.0072489246 |
| LossBefore              | -7.889788e-10 |
| MeanKL                  | 0.007981648   |
| MeanKLBefore            | 2.9807203e-09 |
| Step_0-AverageDiscou... | 206           |
| Step_0-AveragePolicyStd | 0.4034282     |
| Step_0-AverageReturn    | 530           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 846           |
| Step_0-MinReturn        | 13.9          |
| Step_0-NumTrajs         | 953           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 177           |
| Step_1-AverageDiscou... | 205           |
| Step_1-AveragePolicyStd | 0.4033481     |
| Step_1-AverageReturn    | 522           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 894           |
| Step_1-MinReturn        | 17.7          |
| Step_1-NumTrajs         | 982           |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 180           |
| Time                    | 6.71e+04      |
| Time-InnerStep          | 0.161         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.798         |
| Time-Sampling           | 54.6          |
| Time-TotalInner         | 55.6          |
| dLoss                   | 0.0072489236  |
| n_timesteps             | 309120000     |
-------------------------------------------

 ---------------- Iteration 966 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 966            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0069778003  |
| LossBefore              | -6.0071825e-10 |
| MeanKL                  | 0.007917037    |
| MeanKLBefore            | 1.0431234e-08  |
| Step_0-AverageDiscou... | 206            |
| Step_0-AveragePolicyStd | 0.4033422      |
| Step_0-AverageReturn    | 544            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 859            |
| Step_0-MinReturn        | 28.8           |
| Step_0-NumTrajs         | 943            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 177            |
| Step_1-AverageDiscou... | 199            |
| Step_1-AveragePolicyStd | 0.4033466      |
| Step_1-AverageReturn    | 509            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 876            |
| Step_1-MinReturn        | 24.6           |
| Step_1-NumTrajs         | 1024           |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 209            |
| Time                    | 6.72e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.801          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.0069778      |
| n_timesteps             | 309440000      |
--------------------------------------------

 ---------------- Iteration 967 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 967           |
| ItrTime                 | 71            |
| LossAfter               | -0.0072722333 |
| LossBefore              | 3.5446697e-09 |
| MeanKL                  | 0.00810922    |
| MeanKLBefore            | 1.4919432e-09 |
| Step_0-AverageDiscou... | 207           |
| Step_0-AveragePolicyStd | 0.40308276    |
| Step_0-AverageReturn    | 559           |
| Step_0-EnvExecTime      | 23.9          |
| Step_0-MaxReturn        | 926           |
| Step_0-MinReturn        | 12.3          |
| Step_0-NumTrajs         | 889           |
| Step_0-PolicyExecTime   | 1.38          |
| Step_0-StdReturn        | 172           |
| Step_1-AverageDiscou... | 207           |
| Step_1-AveragePolicyStd | 0.40300596    |
| Step_1-AverageReturn    | 549           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 943           |
| Step_1-MinReturn        | 8.52          |
| Step_1-NumTrajs         | 950           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 194           |
| Time                    | 6.72e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.812         |
| Time-Sampling           | 56.4          |
| Time-TotalInner         | 57.4          |
| dLoss                   | 0.007272237   |
| n_timesteps             | 309760000     |
-------------------------------------------

 ---------------- Iteration 968 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 968           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.0068042367 |
| LossBefore              | 5.674372e-09  |
| MeanKL                  | 0.0073870467  |
| MeanKLBefore            | 1.3409524e-08 |
| Step_0-AverageDiscou... | 198           |
| Step_0-AveragePolicyStd | 0.4034941     |
| Step_0-AverageReturn    | 513           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 888           |
| Step_0-MinReturn        | 15.1          |
| Step_0-NumTrajs         | 971           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 199           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.40346414    |
| Step_1-AverageReturn    | 495           |
| Step_1-EnvExecTime      | 23.2          |
| Step_1-MaxReturn        | 909           |
| Step_1-MinReturn        | 21.6          |
| Step_1-NumTrajs         | 1008          |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 200           |
| Time                    | 6.73e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.825         |
| Time-Sampling           | 54.9          |
| Time-TotalInner         | 56            |
| dLoss                   | 0.0068042423  |
| n_timesteps             | 310080000     |
-------------------------------------------

 ---------------- Iteration 969 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 969            |
| ItrTime                 | 69.5           |
| LossAfter               | -0.007374231   |
| LossBefore              | -1.8828565e-10 |
| MeanKL                  | 0.008061382    |
| MeanKLBefore            | 5.959624e-09   |
| Step_0-AverageDiscou... | 200            |
| Step_0-AveragePolicyStd | 0.40374666     |
| Step_0-AverageReturn    | 516            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 868            |
| Step_0-MinReturn        | 30.1           |
| Step_0-NumTrajs         | 972            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 190            |
| Step_1-AverageDiscou... | 197            |
| Step_1-AveragePolicyStd | 0.40369552     |
| Step_1-AverageReturn    | 499            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 876            |
| Step_1-MinReturn        | 6.88           |
| Step_1-NumTrajs         | 1008           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 194            |
| Time                    | 6.74e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.801          |
| Time-Sampling           | 54.9           |
| Time-TotalInner         | 55.9           |
| dLoss                   | 0.007374231    |
| n_timesteps             | 310400000      |
--------------------------------------------

 ---------------- Iteration 970 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 970            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.007816576   |
| LossBefore              | -5.8912817e-09 |
| MeanKL                  | 0.009860036    |
| MeanKLBefore            | 1.3409161e-08  |
| Step_0-AverageDiscou... | 212            |
| Step_0-AveragePolicyStd | 0.40396336     |
| Step_0-AverageReturn    | 567            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 876            |
| Step_0-MinReturn        | 7.08           |
| Step_0-NumTrajs         | 885            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 160            |
| Step_1-AverageDiscou... | 203            |
| Step_1-AveragePolicyStd | 0.4039613      |
| Step_1-AverageReturn    | 528            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 942            |
| Step_1-MinReturn        | 21.5           |
| Step_1-NumTrajs         | 952            |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 179            |
| Time                    | 6.74e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.767          |
| Time-Sampling           | 55.3           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.007816571    |
| n_timesteps             | 310720000      |
--------------------------------------------

 ---------------- Iteration 971 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 971            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.0070278496  |
| LossBefore              | -7.186429e-10  |
| MeanKL                  | 0.007523513    |
| MeanKLBefore            | -8.0859765e-13 |
| Step_0-AverageDiscou... | 207            |
| Step_0-AveragePolicyStd | 0.40389475     |
| Step_0-AverageReturn    | 542            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 910            |
| Step_0-MinReturn        | 17.4           |
| Step_0-NumTrajs         | 944            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 185            |
| Step_1-AverageDiscou... | 206            |
| Step_1-AveragePolicyStd | 0.4038433      |
| Step_1-AverageReturn    | 534            |
| Step_1-EnvExecTime      | 23.3           |
| Step_1-MaxReturn        | 871            |
| Step_1-MinReturn        | 35.2           |
| Step_1-NumTrajs         | 967            |
| Step_1-PolicyExecTime   | 4.15           |
| Step_1-StdReturn        | 187            |
| Time                    | 6.75e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.6           |
| Time-OuterStep          | 13.6           |
| Time-SampleProc         | 0.78           |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.0070278486   |
| n_timesteps             | 311040000      |
--------------------------------------------

 ---------------- Iteration 972 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 972           |
| ItrTime                 | 69.5          |
| LossAfter               | -0.007827778  |
| LossBefore              | 5.7496132e-09 |
| MeanKL                  | 0.008597685   |
| MeanKLBefore            | 2.981262e-09  |
| Step_0-AverageDiscou... | 195           |
| Step_0-AveragePolicyStd | 0.40450975    |
| Step_0-AverageReturn    | 511           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 900           |
| Step_0-MinReturn        | -1.69         |
| Step_0-NumTrajs         | 942           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 189           |
| Step_1-AverageDiscou... | 192           |
| Step_1-AveragePolicyStd | 0.4044458     |
| Step_1-AverageReturn    | 491           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 889           |
| Step_1-MinReturn        | 9.17          |
| Step_1-NumTrajs         | 1002          |
| Step_1-PolicyExecTime   | 4.12          |
| Step_1-StdReturn        | 203           |
| Time                    | 6.76e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.8           |
| Time-Sampling           | 55            |
| Time-TotalInner         | 56            |
| dLoss                   | 0.007827784   |
| n_timesteps             | 311360000     |
-------------------------------------------

 ---------------- Iteration 973 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 973            |
| ItrTime                 | 69.2           |
| LossAfter               | -0.0072868816  |
| LossBefore              | 5.2939124e-09  |
| MeanKL                  | 0.008301118    |
| MeanKLBefore            | -2.9800953e-09 |
| Step_0-AverageDiscou... | 215            |
| Step_0-AveragePolicyStd | 0.40443805     |
| Step_0-AverageReturn    | 576            |
| Step_0-EnvExecTime      | 23.2           |
| Step_0-MaxReturn        | 812            |
| Step_0-MinReturn        | 8.99           |
| Step_0-NumTrajs         | 897            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 207            |
| Step_1-AveragePolicyStd | 0.4043496      |
| Step_1-AverageReturn    | 541            |
| Step_1-EnvExecTime      | 22.8           |
| Step_1-MaxReturn        | 892            |
| Step_1-MinReturn        | 19             |
| Step_1-NumTrajs         | 977            |
| Step_1-PolicyExecTime   | 4.1            |
| Step_1-StdReturn        | 202            |
| Time                    | 6.77e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.787          |
| Time-Sampling           | 54.7           |
| Time-TotalInner         | 55.7           |
| dLoss                   | 0.0072868867   |
| n_timesteps             | 311680000      |
--------------------------------------------

 ---------------- Iteration 974 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 974            |
| ItrTime                 | 71.7           |
| LossAfter               | -0.007499162   |
| LossBefore              | -5.4488103e-09 |
| MeanKL                  | 0.007924353    |
| MeanKLBefore            | -8.941258e-09  |
| Step_0-AverageDiscou... | 211            |
| Step_0-AveragePolicyStd | 0.40435702     |
| Step_0-AverageReturn    | 569            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 891            |
| Step_0-MinReturn        | 23.5           |
| Step_0-NumTrajs         | 904            |
| Step_0-PolicyExecTime   | 1.38           |
| Step_0-StdReturn        | 162            |
| Step_1-AverageDiscou... | 202            |
| Step_1-AveragePolicyStd | 0.40426412     |
| Step_1-AverageReturn    | 535            |
| Step_1-EnvExecTime      | 24.1           |
| Step_1-MaxReturn        | 954            |
| Step_1-MinReturn        | 20.7           |
| Step_1-NumTrajs         | 954            |
| Step_1-PolicyExecTime   | 4.24           |
| Step_1-StdReturn        | 193            |
| Time                    | 6.77e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.779          |
| Time-Sampling           | 57.2           |
| Time-TotalInner         | 58.2           |
| dLoss                   | 0.0074991565   |
| n_timesteps             | 312000000      |
--------------------------------------------

 ---------------- Iteration 975 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 975            |
| ItrTime                 | 69.8           |
| LossAfter               | -0.0074588666  |
| LossBefore              | -3.0500114e-09 |
| MeanKL                  | 0.008826811    |
| MeanKLBefore            | 1.0429638e-08  |
| Step_0-AverageDiscou... | 206            |
| Step_0-AveragePolicyStd | 0.40429604     |
| Step_0-AverageReturn    | 548            |
| Step_0-EnvExecTime      | 23.1           |
| Step_0-MaxReturn        | 867            |
| Step_0-MinReturn        | 20.3           |
| Step_0-NumTrajs         | 910            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 173            |
| Step_1-AverageDiscou... | 200            |
| Step_1-AveragePolicyStd | 0.4041399      |
| Step_1-AverageReturn    | 525            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 867            |
| Step_1-MinReturn        | 7.26           |
| Step_1-NumTrajs         | 945            |
| Step_1-PolicyExecTime   | 4.13           |
| Step_1-StdReturn        | 191            |
| Time                    | 6.78e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.78           |
| Time-Sampling           | 55.2           |
| Time-TotalInner         | 56.2           |
| dLoss                   | 0.0074588633   |
| n_timesteps             | 312320000      |
--------------------------------------------

 ---------------- Iteration 976 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 976           |
| ItrTime                 | 68.9          |
| LossAfter               | -0.007549307  |
| LossBefore              | -6.753996e-10 |
| MeanKL                  | 0.008524215   |
| MeanKLBefore            | 7.4502218e-09 |
| Step_0-AverageDiscou... | 195           |
| Step_0-AveragePolicyStd | 0.4046152     |
| Step_0-AverageReturn    | 499           |
| Step_0-EnvExecTime      | 22.6          |
| Step_0-MaxReturn        | 877           |
| Step_0-MinReturn        | 12.1          |
| Step_0-NumTrajs         | 996           |
| Step_0-PolicyExecTime   | 1.3           |
| Step_0-StdReturn        | 210           |
| Step_1-AverageDiscou... | 194           |
| Step_1-AveragePolicyStd | 0.40463978    |
| Step_1-AverageReturn    | 492           |
| Step_1-EnvExecTime      | 23            |
| Step_1-MaxReturn        | 918           |
| Step_1-MinReturn        | 17.4          |
| Step_1-NumTrajs         | 1019          |
| Step_1-PolicyExecTime   | 4.1           |
| Step_1-StdReturn        | 213           |
| Time                    | 6.79e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.822         |
| Time-Sampling           | 54.2          |
| Time-TotalInner         | 55.2          |
| dLoss                   | 0.0075493064  |
| n_timesteps             | 312640000     |
-------------------------------------------

 ---------------- Iteration 977 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 977          |
| ItrTime                 | 69.8         |
| LossAfter               | -0.007618585 |
| LossBefore              | 7.578904e-09 |
| MeanKL                  | 0.007969521  |
| MeanKLBefore            | 1.340926e-08 |
| Step_0-AverageDiscou... | 199          |
| Step_0-AveragePolicyStd | 0.40520027   |
| Step_0-AverageReturn    | 525          |
| Step_0-EnvExecTime      | 23.2         |
| Step_0-MaxReturn        | 879          |
| Step_0-MinReturn        | 26.8         |
| Step_0-NumTrajs         | 965          |
| Step_0-PolicyExecTime   | 1.34         |
| Step_0-StdReturn        | 209          |
| Step_1-AverageDiscou... | 193          |
| Step_1-AveragePolicyStd | 0.40504733   |
| Step_1-AverageReturn    | 495          |
| Step_1-EnvExecTime      | 23.2         |
| Step_1-MaxReturn        | 950          |
| Step_1-MinReturn        | 14.1         |
| Step_1-NumTrajs         | 1020         |
| Step_1-PolicyExecTime   | 4.1          |
| Step_1-StdReturn        | 209          |
| Time                    | 6.79e+04     |
| Time-InnerStep          | 0.162        |
| Time-MAMLSteps          | 13.5         |
| Time-OuterStep          | 13.5         |
| Time-SampleProc         | 0.82         |
| Time-Sampling           | 55.2         |
| Time-TotalInner         | 56.2         |
| dLoss                   | 0.0076185926 |
| n_timesteps             | 312960000    |
------------------------------------------

 ---------------- Iteration 978 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 978            |
| ItrTime                 | 70             |
| LossAfter               | -0.0074689584  |
| LossBefore              | -3.3144212e-09 |
| MeanKL                  | 0.0089466665   |
| MeanKLBefore            | 2.9795815e-09  |
| Step_0-AverageDiscou... | 208            |
| Step_0-AveragePolicyStd | 0.40507057     |
| Step_0-AverageReturn    | 549            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 947            |
| Step_0-MinReturn        | 15.2           |
| Step_0-NumTrajs         | 928            |
| Step_0-PolicyExecTime   | 1.35           |
| Step_0-StdReturn        | 174            |
| Step_1-AverageDiscou... | 204            |
| Step_1-AveragePolicyStd | 0.405028       |
| Step_1-AverageReturn    | 530            |
| Step_1-EnvExecTime      | 23.2           |
| Step_1-MaxReturn        | 894            |
| Step_1-MinReturn        | 24.5           |
| Step_1-NumTrajs         | 956            |
| Step_1-PolicyExecTime   | 4.17           |
| Step_1-StdReturn        | 184            |
| Time                    | 6.8e+04        |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.839          |
| Time-Sampling           | 55.4           |
| Time-TotalInner         | 56.5           |
| dLoss                   | 0.007468955    |
| n_timesteps             | 313280000      |
--------------------------------------------

 ---------------- Iteration 979 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 979           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0075087696 |
| LossBefore              | 2.645681e-09  |
| MeanKL                  | 0.008362981   |
| MeanKLBefore            | 8.939562e-09  |
| Step_0-AverageDiscou... | 215           |
| Step_0-AveragePolicyStd | 0.40525672    |
| Step_0-AverageReturn    | 572           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 912           |
| Step_0-MinReturn        | 16.3          |
| Step_0-NumTrajs         | 902           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 153           |
| Step_1-AverageDiscou... | 209           |
| Step_1-AveragePolicyStd | 0.40517285    |
| Step_1-AverageReturn    | 549           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 927           |
| Step_1-MinReturn        | 18.7          |
| Step_1-NumTrajs         | 940           |
| Step_1-PolicyExecTime   | 4.17          |
| Step_1-StdReturn        | 182           |
| Time                    | 6.81e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.762         |
| Time-Sampling           | 55.8          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0075087724  |
| n_timesteps             | 313600000     |
-------------------------------------------

 ---------------- Iteration 980 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 980           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.0074248    |
| LossBefore              | 4.9942352e-09 |
| MeanKL                  | 0.008283598   |
| MeanKLBefore            | -4.468909e-09 |
| Step_0-AverageDiscou... | 213           |
| Step_0-AveragePolicyStd | 0.4053717     |
| Step_0-AverageReturn    | 572           |
| Step_0-EnvExecTime      | 23.4          |
| Step_0-MaxReturn        | 913           |
| Step_0-MinReturn        | 31.5          |
| Step_0-NumTrajs         | 901           |
| Step_0-PolicyExecTime   | 1.36          |
| Step_0-StdReturn        | 165           |
| Step_1-AverageDiscou... | 210           |
| Step_1-AveragePolicyStd | 0.40526325    |
| Step_1-AverageReturn    | 560           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 984           |
| Step_1-MinReturn        | 16.3          |
| Step_1-NumTrajs         | 925           |
| Step_1-PolicyExecTime   | 4.21          |
| Step_1-StdReturn        | 181           |
| Time                    | 6.81e+04      |
| Time-InnerStep          | 0.166         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.806         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.0074248053  |
| n_timesteps             | 313920000     |
-------------------------------------------

 ---------------- Iteration 981 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 981           |
| ItrTime                 | 69.2          |
| LossAfter               | -0.006797486  |
| LossBefore              | -7.948101e-09 |
| MeanKL                  | 0.0070527047  |
| MeanKLBefore            | 1.6387776e-08 |
| Step_0-AverageDiscou... | 192           |
| Step_0-AveragePolicyStd | 0.40489116    |
| Step_0-AverageReturn    | 493           |
| Step_0-EnvExecTime      | 22.3          |
| Step_0-MaxReturn        | 877           |
| Step_0-MinReturn        | 14.5          |
| Step_0-NumTrajs         | 999           |
| Step_0-PolicyExecTime   | 1.26          |
| Step_0-StdReturn        | 212           |
| Step_1-AverageDiscou... | 195           |
| Step_1-AveragePolicyStd | 0.40481132    |
| Step_1-AverageReturn    | 498           |
| Step_1-EnvExecTime      | 23.4          |
| Step_1-MaxReturn        | 931           |
| Step_1-MinReturn        | 11.1          |
| Step_1-NumTrajs         | 1011          |
| Step_1-PolicyExecTime   | 4.51          |
| Step_1-StdReturn        | 219           |
| Time                    | 6.82e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.813         |
| Time-Sampling           | 54.7          |
| Time-TotalInner         | 55.7          |
| dLoss                   | 0.006797478   |
| n_timesteps             | 314240000     |
-------------------------------------------

 ---------------- Iteration 982 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 982           |
| ItrTime                 | 69.3          |
| LossAfter               | -0.0071110604 |
| LossBefore              | 1.8421023e-08 |
| MeanKL                  | 0.0077001536  |
| MeanKLBefore            | -4.469597e-09 |
| Step_0-AverageDiscou... | 209           |
| Step_0-AveragePolicyStd | 0.40460086    |
| Step_0-AverageReturn    | 555           |
| Step_0-EnvExecTime      | 22.9          |
| Step_0-MaxReturn        | 919           |
| Step_0-MinReturn        | 24.4          |
| Step_0-NumTrajs         | 917           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 175           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.40445292    |
| Step_1-AverageReturn    | 535           |
| Step_1-EnvExecTime      | 23.1          |
| Step_1-MaxReturn        | 864           |
| Step_1-MinReturn        | 27.9          |
| Step_1-NumTrajs         | 956           |
| Step_1-PolicyExecTime   | 4.16          |
| Step_1-StdReturn        | 187           |
| Time                    | 6.83e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.799         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.007111079   |
| n_timesteps             | 314560000     |
-------------------------------------------

 ---------------- Iteration 983 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 983           |
| ItrTime                 | 69.9          |
| LossAfter               | -0.0074600326 |
| LossBefore              | 6.389071e-09  |
| MeanKL                  | 0.008352505   |
| MeanKLBefore            | 1.4912571e-09 |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40394774    |
| Step_0-AverageReturn    | 526           |
| Step_0-EnvExecTime      | 23.5          |
| Step_0-MaxReturn        | 885           |
| Step_0-MinReturn        | -0.119        |
| Step_0-NumTrajs         | 943           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 191           |
| Step_1-AverageDiscou... | 200           |
| Step_1-AveragePolicyStd | 0.4039037     |
| Step_1-AverageReturn    | 520           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 931           |
| Step_1-MinReturn        | 15.8          |
| Step_1-NumTrajs         | 962           |
| Step_1-PolicyExecTime   | 4.11          |
| Step_1-StdReturn        | 182           |
| Time                    | 6.84e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.783         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007460039   |
| n_timesteps             | 314880000     |
-------------------------------------------

 ---------------- Iteration 984 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 984           |
| ItrTime                 | 70.2          |
| LossAfter               | -0.008132368  |
| LossBefore              | 2.5480644e-09 |
| MeanKL                  | 0.008219347   |
| MeanKLBefore            | 8.940705e-09  |
| Step_0-AverageDiscou... | 211           |
| Step_0-AveragePolicyStd | 0.40408587    |
| Step_0-AverageReturn    | 559           |
| Step_0-EnvExecTime      | 23.3          |
| Step_0-MaxReturn        | 859           |
| Step_0-MinReturn        | 74.2          |
| Step_0-NumTrajs         | 917           |
| Step_0-PolicyExecTime   | 1.35          |
| Step_0-StdReturn        | 162           |
| Step_1-AverageDiscou... | 208           |
| Step_1-AveragePolicyStd | 0.4039703     |
| Step_1-AverageReturn    | 547           |
| Step_1-EnvExecTime      | 23.5          |
| Step_1-MaxReturn        | 966           |
| Step_1-MinReturn        | 20.6          |
| Step_1-NumTrajs         | 940           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 172           |
| Time                    | 6.84e+04      |
| Time-InnerStep          | 0.164         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.776         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.008132371   |
| n_timesteps             | 315200000     |
-------------------------------------------

 ---------------- Iteration 985 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 985            |
| ItrTime                 | 69.6           |
| LossAfter               | -0.0074259914  |
| LossBefore              | -2.3456654e-09 |
| MeanKL                  | 0.008054261    |
| MeanKLBefore            | -7.675638e-13  |
| Step_0-AverageDiscou... | 202            |
| Step_0-AveragePolicyStd | 0.40416887     |
| Step_0-AverageReturn    | 529            |
| Step_0-EnvExecTime      | 22.8           |
| Step_0-MaxReturn        | 891            |
| Step_0-MinReturn        | 37.2           |
| Step_0-NumTrajs         | 957            |
| Step_0-PolicyExecTime   | 1.3            |
| Step_0-StdReturn        | 189            |
| Step_1-AverageDiscou... | 203            |
| Step_1-AveragePolicyStd | 0.40416452     |
| Step_1-AverageReturn    | 532            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 898            |
| Step_1-MinReturn        | 24             |
| Step_1-NumTrajs         | 955            |
| Step_1-PolicyExecTime   | 4.18           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.85e+04       |
| Time-InnerStep          | 0.161          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.78           |
| Time-Sampling           | 55             |
| Time-TotalInner         | 56             |
| dLoss                   | 0.007425989    |
| n_timesteps             | 315520000      |
--------------------------------------------

 ---------------- Iteration 986 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 986            |
| ItrTime                 | 70.3           |
| LossAfter               | -0.007820336   |
| LossBefore              | 1.09776845e-08 |
| MeanKL                  | 0.008776556    |
| MeanKLBefore            | 1.34096085e-08 |
| Step_0-AverageDiscou... | 208            |
| Step_0-AveragePolicyStd | 0.40382886     |
| Step_0-AverageReturn    | 552            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 915            |
| Step_0-MinReturn        | 17.3           |
| Step_0-NumTrajs         | 916            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 181            |
| Step_1-AverageDiscou... | 205            |
| Step_1-AveragePolicyStd | 0.40390614     |
| Step_1-AverageReturn    | 542            |
| Step_1-EnvExecTime      | 23.5           |
| Step_1-MaxReturn        | 907            |
| Step_1-MinReturn        | 29.4           |
| Step_1-NumTrajs         | 933            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 192            |
| Time                    | 6.86e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.768          |
| Time-Sampling           | 55.7           |
| Time-TotalInner         | 56.7           |
| dLoss                   | 0.007820347    |
| n_timesteps             | 315840000      |
--------------------------------------------

 ---------------- Iteration 987 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 987           |
| ItrTime                 | 70            |
| LossAfter               | -0.007374416  |
| LossBefore              | 2.1346522e-09 |
| MeanKL                  | 0.008316537   |
| MeanKLBefore            | 1.1919463e-08 |
| Step_0-AverageDiscou... | 200           |
| Step_0-AveragePolicyStd | 0.40390894    |
| Step_0-AverageReturn    | 525           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 864           |
| Step_0-MinReturn        | 8.89          |
| Step_0-NumTrajs         | 942           |
| Step_0-PolicyExecTime   | 1.31          |
| Step_0-StdReturn        | 190           |
| Step_1-AverageDiscou... | 197           |
| Step_1-AveragePolicyStd | 0.40380692    |
| Step_1-AverageReturn    | 517           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 890           |
| Step_1-MinReturn        | 8.43          |
| Step_1-NumTrajs         | 955           |
| Step_1-PolicyExecTime   | 4.22          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.86e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.808         |
| Time-Sampling           | 55.5          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.0073744184  |
| n_timesteps             | 316160000     |
-------------------------------------------

 ---------------- Iteration 988 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 988            |
| ItrTime                 | 71.4           |
| LossAfter               | -0.0072524236  |
| LossBefore              | -9.2162544e-10 |
| MeanKL                  | 0.008129173    |
| MeanKLBefore            | -3.8795632e-13 |
| Step_0-AverageDiscou... | 220            |
| Step_0-AveragePolicyStd | 0.40364274     |
| Step_0-AverageReturn    | 602            |
| Step_0-EnvExecTime      | 23.9           |
| Step_0-MaxReturn        | 843            |
| Step_0-MinReturn        | 21             |
| Step_0-NumTrajs         | 859            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 150            |
| Step_1-AverageDiscou... | 217            |
| Step_1-AveragePolicyStd | 0.40355784     |
| Step_1-AverageReturn    | 583            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 910            |
| Step_1-MinReturn        | 22.5           |
| Step_1-NumTrajs         | 908            |
| Step_1-PolicyExecTime   | 4.26           |
| Step_1-StdReturn        | 176            |
| Time                    | 6.87e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.758          |
| Time-Sampling           | 56.9           |
| Time-TotalInner         | 57.8           |
| dLoss                   | 0.0072524226   |
| n_timesteps             | 316480000      |
--------------------------------------------

 ---------------- Iteration 989 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 989            |
| ItrTime                 | 70.2           |
| LossAfter               | -0.0070566284  |
| LossBefore              | -1.3194958e-08 |
| MeanKL                  | 0.007661155    |
| MeanKLBefore            | 7.4501814e-09  |
| Step_0-AverageDiscou... | 213            |
| Step_0-AveragePolicyStd | 0.40382296     |
| Step_0-AverageReturn    | 562            |
| Step_0-EnvExecTime      | 23.4           |
| Step_0-MaxReturn        | 929            |
| Step_0-MinReturn        | 32.4           |
| Step_0-NumTrajs         | 920            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 179            |
| Step_1-AverageDiscou... | 208            |
| Step_1-AveragePolicyStd | 0.40378964     |
| Step_1-AverageReturn    | 540            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 920            |
| Step_1-MinReturn        | 14.4           |
| Step_1-NumTrajs         | 977            |
| Step_1-PolicyExecTime   | 4.16           |
| Step_1-StdReturn        | 204            |
| Time                    | 6.88e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.794          |
| Time-Sampling           | 55.6           |
| Time-TotalInner         | 56.6           |
| dLoss                   | 0.0070566153   |
| n_timesteps             | 316800000      |
--------------------------------------------

 ---------------- Iteration 990 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 990           |
| ItrTime                 | 69.4          |
| LossAfter               | -0.0066646175 |
| LossBefore              | 6.606167e-09  |
| MeanKL                  | 0.0070465915  |
| MeanKLBefore            | 5.958987e-09  |
| Step_0-AverageDiscou... | 207           |
| Step_0-AveragePolicyStd | 0.40419853    |
| Step_0-AverageReturn    | 534           |
| Step_0-EnvExecTime      | 22.7          |
| Step_0-MaxReturn        | 859           |
| Step_0-MinReturn        | 15.4          |
| Step_0-NumTrajs         | 972           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 186           |
| Step_1-AverageDiscou... | 208           |
| Step_1-AveragePolicyStd | 0.40413904    |
| Step_1-AverageReturn    | 544           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 887           |
| Step_1-MinReturn        | 23.8          |
| Step_1-NumTrajs         | 956           |
| Step_1-PolicyExecTime   | 4.15          |
| Step_1-StdReturn        | 189           |
| Time                    | 6.88e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.794         |
| Time-Sampling           | 54.8          |
| Time-TotalInner         | 55.8          |
| dLoss                   | 0.006664624   |
| n_timesteps             | 317120000     |
-------------------------------------------

 ---------------- Iteration 991 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 991            |
| ItrTime                 | 71.1           |
| LossAfter               | -0.007215844   |
| LossBefore              | -1.5713109e-08 |
| MeanKL                  | 0.007906823    |
| MeanKLBefore            | 1.0430062e-08  |
| Step_0-AverageDiscou... | 206            |
| Step_0-AveragePolicyStd | 0.4043701      |
| Step_0-AverageReturn    | 543            |
| Step_0-EnvExecTime      | 23.7           |
| Step_0-MaxReturn        | 886            |
| Step_0-MinReturn        | 12.2           |
| Step_0-NumTrajs         | 949            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 202            |
| Step_1-AverageDiscou... | 208            |
| Step_1-AveragePolicyStd | 0.4042002      |
| Step_1-AverageReturn    | 551            |
| Step_1-EnvExecTime      | 23.8           |
| Step_1-MaxReturn        | 902            |
| Step_1-MinReturn        | 17.2           |
| Step_1-NumTrajs         | 939            |
| Step_1-PolicyExecTime   | 4.25           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.89e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.818          |
| Time-Sampling           | 56.5           |
| Time-TotalInner         | 57.5           |
| dLoss                   | 0.007215828    |
| n_timesteps             | 317440000      |
--------------------------------------------

 ---------------- Iteration 992 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 992            |
| ItrTime                 | 70.7           |
| LossAfter               | -0.006777799   |
| LossBefore              | -1.8556268e-09 |
| MeanKL                  | 0.007637988    |
| MeanKLBefore            | -5.9626792e-09 |
| Step_0-AverageDiscou... | 195            |
| Step_0-AveragePolicyStd | 0.40454686     |
| Step_0-AverageReturn    | 515            |
| Step_0-EnvExecTime      | 24.1           |
| Step_0-MaxReturn        | 866            |
| Step_0-MinReturn        | 15.3           |
| Step_0-NumTrajs         | 958            |
| Step_0-PolicyExecTime   | 1.36           |
| Step_0-StdReturn        | 215            |
| Step_1-AverageDiscou... | 196            |
| Step_1-AveragePolicyStd | 0.40443456     |
| Step_1-AverageReturn    | 516            |
| Step_1-EnvExecTime      | 23.4           |
| Step_1-MaxReturn        | 872            |
| Step_1-MinReturn        | 16.7           |
| Step_1-NumTrajs         | 968            |
| Step_1-PolicyExecTime   | 4.07           |
| Step_1-StdReturn        | 216            |
| Time                    | 6.9e+04        |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.79           |
| Time-Sampling           | 56.2           |
| Time-TotalInner         | 57.2           |
| dLoss                   | 0.0067777974   |
| n_timesteps             | 317760000      |
--------------------------------------------

 ---------------- Iteration 993 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 993           |
| ItrTime                 | 70.3          |
| LossAfter               | -0.007365438  |
| LossBefore              | 9.058882e-09  |
| MeanKL                  | 0.008047436   |
| MeanKLBefore            | 1.4895495e-09 |
| Step_0-AverageDiscou... | 210           |
| Step_0-AveragePolicyStd | 0.40477335    |
| Step_0-AverageReturn    | 569           |
| Step_0-EnvExecTime      | 23.2          |
| Step_0-MaxReturn        | 930           |
| Step_0-MinReturn        | 19.8          |
| Step_0-NumTrajs         | 902           |
| Step_0-PolicyExecTime   | 1.34          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 204           |
| Step_1-AveragePolicyStd | 0.4047033     |
| Step_1-AverageReturn    | 542           |
| Step_1-EnvExecTime      | 23.6          |
| Step_1-MaxReturn        | 939           |
| Step_1-MinReturn        | 10.9          |
| Step_1-NumTrajs         | 939           |
| Step_1-PolicyExecTime   | 4.19          |
| Step_1-StdReturn        | 196           |
| Time                    | 6.91e+04      |
| Time-InnerStep          | 0.162         |
| Time-MAMLSteps          | 13.6          |
| Time-OuterStep          | 13.6          |
| Time-SampleProc         | 0.799         |
| Time-Sampling           | 55.7          |
| Time-TotalInner         | 56.7          |
| dLoss                   | 0.007365447   |
| n_timesteps             | 318080000     |
-------------------------------------------

 ---------------- Iteration 994 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 994            |
| ItrTime                 | 71             |
| LossAfter               | -0.006904731   |
| LossBefore              | 7.572018e-09   |
| MeanKL                  | 0.007270423    |
| MeanKLBefore            | -1.1917875e-08 |
| Step_0-AverageDiscou... | 210            |
| Step_0-AveragePolicyStd | 0.40457043     |
| Step_0-AverageReturn    | 558            |
| Step_0-EnvExecTime      | 23.6           |
| Step_0-MaxReturn        | 971            |
| Step_0-MinReturn        | 23.7           |
| Step_0-NumTrajs         | 933            |
| Step_0-PolicyExecTime   | 1.33           |
| Step_0-StdReturn        | 191            |
| Step_1-AverageDiscou... | 207            |
| Step_1-AveragePolicyStd | 0.4045117      |
| Step_1-AverageReturn    | 543            |
| Step_1-EnvExecTime      | 24             |
| Step_1-MaxReturn        | 905            |
| Step_1-MinReturn        | 22.9           |
| Step_1-NumTrajs         | 954            |
| Step_1-PolicyExecTime   | 4.19           |
| Step_1-StdReturn        | 186            |
| Time                    | 6.91e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.781          |
| Time-Sampling           | 56.4           |
| Time-TotalInner         | 57.4           |
| dLoss                   | 0.0069047385   |
| n_timesteps             | 318400000      |
--------------------------------------------

 ---------------- Iteration 995 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 995           |
| ItrTime                 | 70            |
| LossAfter               | -0.007012449  |
| LossBefore              | 1.2496971e-08 |
| MeanKL                  | 0.007477101   |
| MeanKLBefore            | 5.95922e-09   |
| Step_0-AverageDiscou... | 207           |
| Step_0-AveragePolicyStd | 0.40458137    |
| Step_0-AverageReturn    | 552           |
| Step_0-EnvExecTime      | 23            |
| Step_0-MaxReturn        | 882           |
| Step_0-MinReturn        | 21.8          |
| Step_0-NumTrajs         | 930           |
| Step_0-PolicyExecTime   | 1.33          |
| Step_0-StdReturn        | 207           |
| Step_1-AverageDiscou... | 208           |
| Step_1-AveragePolicyStd | 0.40455672    |
| Step_1-AverageReturn    | 547           |
| Step_1-EnvExecTime      | 23.3          |
| Step_1-MaxReturn        | 913           |
| Step_1-MinReturn        | 12.1          |
| Step_1-NumTrajs         | 954           |
| Step_1-PolicyExecTime   | 4.52          |
| Step_1-StdReturn        | 197           |
| Time                    | 6.92e+04      |
| Time-InnerStep          | 0.163         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.804         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.5          |
| dLoss                   | 0.007012462   |
| n_timesteps             | 318720000     |
-------------------------------------------

 ---------------- Iteration 996 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 996            |
| ItrTime                 | 68.7           |
| LossAfter               | -0.006920074   |
| LossBefore              | -2.9047094e-09 |
| MeanKL                  | 0.007595292    |
| MeanKLBefore            | 2.9798417e-09  |
| Step_0-AverageDiscou... | 211            |
| Step_0-AveragePolicyStd | 0.4044148      |
| Step_0-AverageReturn    | 545            |
| Step_0-EnvExecTime      | 22.6           |
| Step_0-MaxReturn        | 875            |
| Step_0-MinReturn        | 22.9           |
| Step_0-NumTrajs         | 966            |
| Step_0-PolicyExecTime   | 1.32           |
| Step_0-StdReturn        | 189            |
| Step_1-AverageDiscou... | 205            |
| Step_1-AveragePolicyStd | 0.40431648     |
| Step_1-AverageReturn    | 528            |
| Step_1-EnvExecTime      | 22.7           |
| Step_1-MaxReturn        | 893            |
| Step_1-MinReturn        | 35.8           |
| Step_1-NumTrajs         | 979            |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 189            |
| Time                    | 6.93e+04       |
| Time-InnerStep          | 0.162          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.801          |
| Time-Sampling           | 54.1           |
| Time-TotalInner         | 55.1           |
| dLoss                   | 0.0069200713   |
| n_timesteps             | 319040000      |
--------------------------------------------

 ---------------- Iteration 997 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 997            |
| ItrTime                 | 70.5           |
| LossAfter               | -0.00736742    |
| LossBefore              | 1.3838464e-08  |
| MeanKL                  | 0.008545835    |
| MeanKLBefore            | -1.4911716e-09 |
| Step_0-AverageDiscou... | 209            |
| Step_0-AveragePolicyStd | 0.404574       |
| Step_0-AverageReturn    | 561            |
| Step_0-EnvExecTime      | 23.5           |
| Step_0-MaxReturn        | 905            |
| Step_0-MinReturn        | 35.5           |
| Step_0-NumTrajs         | 909            |
| Step_0-PolicyExecTime   | 1.34           |
| Step_0-StdReturn        | 175            |
| Step_1-AverageDiscou... | 213            |
| Step_1-AveragePolicyStd | 0.40442216     |
| Step_1-AverageReturn    | 571            |
| Step_1-EnvExecTime      | 23.6           |
| Step_1-MaxReturn        | 910            |
| Step_1-MinReturn        | 18.7           |
| Step_1-NumTrajs         | 921            |
| Step_1-PolicyExecTime   | 4.2            |
| Step_1-StdReturn        | 174            |
| Time                    | 6.93e+04       |
| Time-InnerStep          | 0.163          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.771          |
| Time-Sampling           | 56             |
| Time-TotalInner         | 56.9           |
| dLoss                   | 0.007367434    |
| n_timesteps             | 319360000      |
--------------------------------------------

 ---------------- Iteration 998 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 998            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0076534017  |
| LossBefore              | 2.5922409e-09  |
| MeanKL                  | 0.008441057    |
| MeanKLBefore            | -2.9792575e-09 |
| Step_0-AverageDiscou... | 206            |
| Step_0-AveragePolicyStd | 0.40487492     |
| Step_0-AverageReturn    | 538            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 890            |
| Step_0-MinReturn        | 22.3           |
| Step_0-NumTrajs         | 946            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 192            |
| Step_1-AverageDiscou... | 204            |
| Step_1-AveragePolicyStd | 0.40481135     |
| Step_1-AverageReturn    | 528            |
| Step_1-EnvExecTime      | 23             |
| Step_1-MaxReturn        | 900            |
| Step_1-MinReturn        | 28.1           |
| Step_1-NumTrajs         | 966            |
| Step_1-PolicyExecTime   | 4.11           |
| Step_1-StdReturn        | 196            |
| Time                    | 6.94e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.786          |
| Time-Sampling           | 54.5           |
| Time-TotalInner         | 55.5           |
| dLoss                   | 0.0076534045   |
| n_timesteps             | 319680000      |
--------------------------------------------

 ---------------- Iteration 999 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 999            |
| ItrTime                 | 69.1           |
| LossAfter               | -0.0073282733  |
| LossBefore              | -2.6645532e-09 |
| MeanKL                  | 0.007912705    |
| MeanKLBefore            | 1.4895256e-09  |
| Step_0-AverageDiscou... | 206            |
| Step_0-AveragePolicyStd | 0.40512297     |
| Step_0-AverageReturn    | 533            |
| Step_0-EnvExecTime      | 22.9           |
| Step_0-MaxReturn        | 825            |
| Step_0-MinReturn        | 51.1           |
| Step_0-NumTrajs         | 970            |
| Step_0-PolicyExecTime   | 1.31           |
| Step_0-StdReturn        | 192            |
| Step_1-AverageDiscou... | 194            |
| Step_1-AveragePolicyStd | 0.4051625      |
| Step_1-AverageReturn    | 484            |
| Step_1-EnvExecTime      | 23.1           |
| Step_1-MaxReturn        | 860            |
| Step_1-MinReturn        | 20.9           |
| Step_1-NumTrajs         | 1048           |
| Step_1-PolicyExecTime   | 4.08           |
| Step_1-StdReturn        | 208            |
| Time                    | 6.95e+04       |
| Time-InnerStep          | 0.164          |
| Time-MAMLSteps          | 13.5           |
| Time-OuterStep          | 13.5           |
| Time-SampleProc         | 0.796          |
| Time-Sampling           | 54.6           |
| Time-TotalInner         | 55.6           |
| dLoss                   | 0.0073282705   |
| n_timesteps             | 320000000      |
--------------------------------------------

 ---------------- Iteration 1000 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 1000          |
| ItrTime                 | 70            |
| LossAfter               | -0.0072253    |
| LossBefore              | 1.1564128e-08 |
| MeanKL                  | 0.008113457   |
| MeanKLBefore            | 1.4901161e-09 |
| Step_0-AverageDiscou... | 208           |
| Step_0-AveragePolicyStd | 0.4050086     |
| Step_0-AverageReturn    | 537           |
| Step_0-EnvExecTime      | 22.8          |
| Step_0-MaxReturn        | 854           |
| Step_0-MinReturn        | 31.5          |
| Step_0-NumTrajs         | 980           |
| Step_0-PolicyExecTime   | 1.32          |
| Step_0-StdReturn        | 185           |
| Step_1-AverageDiscou... | 209           |
| Step_1-AveragePolicyStd | 0.404909      |
| Step_1-AverageReturn    | 550           |
| Step_1-EnvExecTime      | 23.7          |
| Step_1-MaxReturn        | 879           |
| Step_1-MinReturn        | 14            |
| Step_1-NumTrajs         | 950           |
| Step_1-PolicyExecTime   | 4.2           |
| Step_1-StdReturn        | 176           |
| Time                    | 6.95e+04      |
| Time-InnerStep          | 0.186         |
| Time-MAMLSteps          | 13.5          |
| Time-OuterStep          | 13.5          |
| Time-SampleProc         | 0.814         |
| Time-Sampling           | 55.4          |
| Time-TotalInner         | 56.4          |
| dLoss                   | 0.007225312   |
| n_timesteps             | 320320000     |
-------------------------------------------
Training finished
